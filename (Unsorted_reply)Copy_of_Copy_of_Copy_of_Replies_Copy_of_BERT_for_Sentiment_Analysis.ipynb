{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(Unsorted reply)Copy of Copy_of_Copy_of_Replies_Copy_of_BERT_for_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "adfd875dc62242e48df319736b0a3c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_758d96e8ee3446578532ce4078b587db",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5d256c42c1cc438d91d9aa837676408f",
              "IPY_MODEL_85a4c7b239f44b62994c2442e09f2aad"
            ]
          }
        },
        "758d96e8ee3446578532ce4078b587db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5d256c42c1cc438d91d9aa837676408f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_070ab29288cf4dda84776a775111f22e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_945a5b5969b146c1a361f9b164428bd1"
          }
        },
        "85a4c7b239f44b62994c2442e09f2aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_efa411894ca84a18bb16547db0486b41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 621kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bdd72cb812d9413baee81500c84413c3"
          }
        },
        "070ab29288cf4dda84776a775111f22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "945a5b5969b146c1a361f9b164428bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efa411894ca84a18bb16547db0486b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bdd72cb812d9413baee81500c84413c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebada6c0013945e4a1d6e87c7cb3b97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_392c0425861540ab8377a587b39aac30",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1e93f835346f46f9adbed8fb9dba1167",
              "IPY_MODEL_d669efb902554dc79b753cfe2b9629e0"
            ]
          }
        },
        "392c0425861540ab8377a587b39aac30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e93f835346f46f9adbed8fb9dba1167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22a5022a14cb4bcb8151c20ec1078fd0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b8f12ad9d044f2c98412a78c32c81d2"
          }
        },
        "d669efb902554dc79b753cfe2b9629e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8a76f98286a243bdbe4ca33c2a283a37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 515B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57b7b0bd21db4fa9917e882633d8c000"
          }
        },
        "22a5022a14cb4bcb8151c20ec1078fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b8f12ad9d044f2c98412a78c32c81d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a76f98286a243bdbe4ca33c2a283a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57b7b0bd21db4fa9917e882633d8c000": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a70383b0dc94ac0a81591fe100b9ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c21ba027254a4c80a87c99947679d058",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2041b68cb63848f6940275cc4539621b",
              "IPY_MODEL_3b0db88d7e95410ca48aa076ece413f8"
            ]
          }
        },
        "c21ba027254a4c80a87c99947679d058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2041b68cb63848f6940275cc4539621b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76f350f9e89c4ac1adac955871f9bd45",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8d40cca2329c42b98e31d6412140cd14"
          }
        },
        "3b0db88d7e95410ca48aa076ece413f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_24efb8330d294b189ad5a655ec33ea13",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:52&lt;00:00, 8.38MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96c9bb74699e49bb9d1c17147014814c"
          }
        },
        "76f350f9e89c4ac1adac955871f9bd45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8d40cca2329c42b98e31d6412140cd14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "24efb8330d294b189ad5a655ec33ea13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96c9bb74699e49bb9d1c17147014814c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiran98780/NLP_Assignment2/blob/main/(Unsorted_reply)Copy_of_Copy_of_Copy_of_Replies_Copy_of_BERT_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MstNuBjkCGsG"
      },
      "source": [
        "# Step 1 - Mount drive\n",
        "Mount your google drive which has the training, test and dev jsonlfiles and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lTXsMK3sNYr"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import time\n",
        "import collections\n",
        "import json\n",
        "from sklearn import preprocessing\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXYMwHV6iDeY",
        "outputId": "52c0bc37-61c0-4b2d-91d0-e8d98f4c3f9c"
      },
      "source": [
        "#function to return the replies as a list for a source tweet \n",
        "def get_replies(df):\n",
        "    dict_time = {}\n",
        "    # rows = df.loc[\"Utah Jazz\"]\n",
        "    for index, row in df.iterrows():\n",
        "        #check if the in_reply_to_status_id_str string matches a number\n",
        "        x = re.search(r'\\d+', str(row['in_reply_to_status_id_str']))  \n",
        "        if x:\n",
        "            \n",
        "            #append the reply tweets in a big_list and return the list\n",
        "            big_list.append(row[\"tweet\"]) \n",
        "            #if the tweets are created at the same time - append it to existing list\n",
        "            if (row[\"created_at\"] in dict_time):\n",
        "                # print(\"True\")\n",
        "                value = dict_time.get(row[\"created_at\"])\n",
        "                value.append(row[\"tweet\"])\n",
        "                dict_time[row[\"created_at\"]] = value\n",
        "                # print(replyTw_list)\n",
        "            else:\n",
        "                replyTw_list = []\n",
        "                replyTw_list.append(row[\"tweet\"])\n",
        "                # print(replyTw_list) \n",
        "                dict_time[row[\"created_at\"]] = replyTw_list\n",
        "            # print(dict_time)\n",
        "        else:\n",
        "            big_list = []\n",
        "    #returns the list of replies for a source tweet, length of the reply list and dict of {created_at_Time:reply_tweet}\n",
        "    print(big_list, dict_time)\n",
        "    return big_list,len(big_list),dict_time \n",
        "  \n",
        "data = [['time1', \"source1\",\"None\"],['time1', \"reply1\",\"123\"], ['time2', \"reply2\",\"234\"], ['time1', \"reply3\",\"345\"]]\n",
        "# Create the pandas DataFrame\n",
        "df = pd.DataFrame(data, columns = ['created_at', 'tweet', 'in_reply_to_status_id_str'])\n",
        "big_list,len_list,dict_time = get_replies(df)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['reply1', 'reply2', 'reply3'] {'time1': ['reply1', 'reply3'], 'time2': ['reply2']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HWT3KaUwcdA"
      },
      "source": [
        "replies_list_2d = []\n",
        "replies_list_1d = []\n",
        "list_replies = []\n",
        "dict_replies_list = []"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu0cQ8rTjG5O",
        "outputId": "83122179-1beb-45f2-95eb-6091981a8cd7"
      },
      "source": [
        "replies_list_2d = list(dict_time.values()) #get all the replies for a single source tweet\n",
        "# print(len(replies_list_2d))\n",
        "replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "# print(len(replies_list_1d))\n",
        "reply_length = len(replies_list_1d)\n",
        "list_replies.append(reply_length) \n",
        "dict_replies_list.append(dict_time)\n",
        "print(replies_list_2d)\n",
        "print(replies_list_1d)\n",
        "print(reply_length)\n",
        "print(list_replies)\n",
        "print(dict_replies_list)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['reply1', 'reply3'], ['reply2']]\n",
            "['reply1', 'reply3', 'reply2']\n",
            "3\n",
            "[3]\n",
            "[{'time1': ['reply1', 'reply3'], 'time2': ['reply2']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouuno_uYZkxa",
        "outputId": "3eb2f18e-0202-4ce5-821c-0534a5b45d5c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHZ0vJLqZwKH",
        "outputId": "c68ef5d0-e976-41a5-a9e7-7f65477756b4"
      },
      "source": [
        "%cd /content/drive/My Drive/NLP_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxMCKPVZZ2Gp",
        "outputId": "cd823d53-df02-4df6-a206-bf0f01eb252e"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copy_of_10_bert (1).ipynb'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe1.csv'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe2.csv'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe3.csv'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe3.gsheet'\n",
            " dev.baseline_03_BERT.json\n",
            " dev.baseline_bert5.json\n",
            " dev.baseline_bert6.json\n",
            " dev.data.jsonl\n",
            " dev.label.json\n",
            " export_dataframe1.csv\n",
            " export_dataframe2.csv\n",
            " export_dataframe3.csv\n",
            " \u001b[0m\u001b[01;34mmodel_save\u001b[0m/\n",
            " sstcls_0.dat\n",
            " sstcls_1.dat\n",
            " sstcls_2.dat\n",
            " sstcls_4.dat\n",
            " test.baseline_03_BERT.json\n",
            " test.data.jsonl\n",
            " train.data.jsonl\n",
            " train.label.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2MGn7cpVp83",
        "outputId": "32080ebf-ef3a-4a12-dad2-d5a7564332a0"
      },
      "source": [
        "%cd /content/drive/My Drive/NLP_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqt0gJncVpxk",
        "outputId": "bc36fdeb-3e57-41eb-bbc4-a014acad2d8b"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copy_of_10_bert (1).ipynb'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe1.csv'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe2.csv'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe3.csv'\n",
            "'C:\\Users\\Kiran\\Downloads\\NLP Project\\NLP_Assignment2\\project-data\\export_dataframe3.gsheet'\n",
            " dev.baseline_03_BERT.json\n",
            " dev.baseline_bert5.json\n",
            " dev.baseline_bert6.json\n",
            " dev.data.jsonl\n",
            " dev.label.json\n",
            " export_dataframe1.csv\n",
            " export_dataframe2.csv\n",
            " export_dataframe3.csv\n",
            " \u001b[0m\u001b[01;34mmodel_save\u001b[0m/\n",
            " sstcls_0.dat\n",
            " sstcls_1.dat\n",
            " sstcls_2.dat\n",
            " sstcls_4.dat\n",
            " test.baseline_03_BERT.json\n",
            " test.data.jsonl\n",
            " train.data.jsonl\n",
            " train.label.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slO_rmYgwmmE"
      },
      "source": [
        "# Step 2 - Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31OW0dhozvli"
      },
      "source": [
        "## 1. Load Essential Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u07WRKnxsX96"
      },
      "source": [
        "## 2. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK07OJ21IS9Y"
      },
      "source": [
        "# #function to return the replies as a list for a source tweet \n",
        "# def get_replies(df):\n",
        "#     dict_time = {}\n",
        "#     for index, row in df.iterrows():\n",
        "#         replyTw_list = []\n",
        "#         #check if the in_reply_to_status_id_str string matches a number\n",
        "#         x = re.search(r'\\d+', str(row['in_reply_to_status_id_str']))  \n",
        "#         if x:\n",
        "#             #append the reply tweets in a big_list and return the list\n",
        "#             big_list.append(row[\"tweet\"]) \n",
        "#             replyTw_list.append(row[\"tweet\"])\n",
        "#             #if the tweets are created at the same time - append it to existing list\n",
        "#             if (row[\"created_at\"] in dict_time):\n",
        "#                 replyTw_list.append(row[\"tweet\"])\n",
        "#             dict_time[row[\"created_at\"]] = replyTw_list\n",
        "#         else:\n",
        "#             big_list = []\n",
        "#     #returns the list of replies for a source tweet, length of the reply list and dict of {created_at_Time:reply_tweet}\n",
        "#     return big_list,len(big_list),dict_time \n",
        "#function to return the replies as a list for a source tweet \n",
        "def get_replies(df):\n",
        "    dict_time = {}\n",
        "    # rows = df.loc[\"Utah Jazz\"]\n",
        "    for index, row in df.iterrows():\n",
        "        #check if the in_reply_to_status_id_str string matches a number\n",
        "        x = re.search(r'\\d+', str(row['in_reply_to_status_id_str']))  \n",
        "        if x:\n",
        "            \n",
        "            #append the reply tweets in a big_list and return the list\n",
        "            big_list.append(row[\"tweet\"]) \n",
        "            #if the tweets are created at the same time - append it to existing list\n",
        "            if (row[\"created_at\"] in dict_time):\n",
        "                # print(\"True\")\n",
        "                value = dict_time.get(row[\"created_at\"])\n",
        "                value.append(row[\"tweet\"])\n",
        "                dict_time[row[\"created_at\"]] = value\n",
        "                # print(replyTw_list)\n",
        "            else:\n",
        "                replyTw_list = []\n",
        "                replyTw_list.append(row[\"tweet\"])\n",
        "                # print(replyTw_list) \n",
        "                dict_time[row[\"created_at\"]] = replyTw_list\n",
        "            # print(dict_time)\n",
        "        else:\n",
        "            big_list = []\n",
        "    #returns the list of replies for a source tweet, length of the reply list and dict of {created_at_Time:reply_tweet}\n",
        "    # print(big_list, dict_time)\n",
        "    return big_list,len(big_list),dict_time \n",
        "#function to return the list of reply tweets sorted by time for all source tweets\n",
        "#takes the list of dictionaries as a parameter of the form [{created_At_time1:[tweet1, tweet2]},{created_At_time2:[tweet3]}]\n",
        "def get_sorted_by_time(dict_replies_list):\n",
        "    sorted_list = []\n",
        "    for i in range(len(dict_replies_list)):\n",
        "        # dtime = 'Mon Dec 15 21:50:30 +0000 2014'\n",
        "        newkeys_list = []\n",
        "        for k, v in dict_replies_list[i].items():  \n",
        "            new_datetime = datetime.strftime(datetime.strptime(k,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S') \n",
        "            date_to_float = datetime.fromisoformat(new_datetime).timestamp()#1420646211.0\n",
        "            newkeys_list.append(date_to_float) #list of keys for the new dictionary \n",
        "            vals = list(dict_replies_list[i].values()) #list of values for the new dictionary \n",
        "        newdictionary = {k: v for k, v in zip(newkeys_list, vals)} #zip the keys and values to form a new dictionary \n",
        "        sorted_list.append(newdictionary)\n",
        "    print(\"The length of sorted list is = \", len(sorted_list))\n",
        "    return sorted_list"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVkXcFzrtREn"
      },
      "source": [
        "### 2.1. Load Train Data\n",
        "The train.data.jsonl has about 4k source tweets and about 81k source + reply tweets.\n",
        "\n",
        "The label files ([train,dev].label.json), are standard JSON files:\n",
        "{\n",
        "\"552800070199148544\": \"non-rumour\",\n",
        "\"544388259359387648\": \"non-rumour\",\n",
        "\"552805970536333314\": \"non-rumour\",\n",
        "\"525071376084791297\": \"rumour\"\n",
        "} \n",
        "\n",
        "All data files ([train,dev,test,covid].data.jsonl) are JSONL files, where each line is a JSON string. For these files, each line is an event: a list of tweets where the first tweet is a source tweet and the rest are reply tweets.\n",
        "\n",
        "Here the train data is loaded and merged with the labels and the resulting columns are \"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\", \"label\"]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-ltEHJIZ9Bi"
      },
      "source": [
        "# pd.set_option('display.width', 1000)\n",
        "# pd.set_option('display.max_columns', 500)\n",
        "# pd.set_option('display.min_rows', 100)\n",
        "# pd.set_option('display.max_rows', 500)\n",
        "# def load_data(data_filename, label_filename):\n",
        "#     with open(data_filename, 'r', encoding='utf-8') as train_data_file:\n",
        "#       list_of_rlists = []\n",
        "#       chunks = [] #list of all the dataframes for all source and reply tweets\n",
        "#       num_replies = [] #list of number of replies for every source tweet \n",
        "#       dict_replies= {} #dict of {created_At_time:[reply tweet]} for every source tweet\n",
        "#       list_replies = []\n",
        "#       dict_replies_list = []\n",
        "#       for line in train_data_file:\n",
        "#         train_Data_list = json.loads(line)\n",
        "#         df = pd.DataFrame.from_dict(train_Data_list)\n",
        "#         df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "#         df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "#         df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "\n",
        "#         replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "#         replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "#         # print(len(replies_list_2d))\n",
        "#         replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "#         # print(len(replies_list_1d))\n",
        "#         reply_length = len(replies_list_1d)\n",
        "#         list_replies.append(reply_length) \n",
        "#         dict_replies_list.append(dict_replies)\n",
        "\n",
        "#         chunks.append(df)\n",
        "#         list_of_rlists.append(replies_list)\n",
        "#         num_replies.append(len_reply_list)\n",
        "#         # print(\"The output of list_of_rlists in every iteration=\",list_of_rlists,\"\\n\",len(list_of_rlists))\n",
        "#       dfs1 = pd.concat(chunks)\n",
        "#     train_data_file.close()\n",
        "#     #print(dfs1)\n",
        "#     print(\"The len of list of list of replies=\", len(list_of_rlists))\n",
        "\n",
        "\n",
        "#     with open(label_filename) as train_label_file:\n",
        "#         train_label_json_file = json.load(train_label_file)\n",
        "#         df_train_label = pd.DataFrame(list(train_label_json_file.items()),columns = ['id', 'label'])\n",
        "#     train_label_file.close()\n",
        "\n",
        "#     #check for class imbalance\n",
        "#     print(df_train_label)\n",
        "#     print(df_train_label.groupby('label').size())\n",
        "#     # non - rumour\n",
        "#     # 3058\n",
        "#     # rumour\n",
        "#     # 1583a\n",
        "\n",
        "#     #check the id and if it is present in rumours/non-rumours labels, then add it to the row of that dataframe (check from the labels dataframe)\n",
        "#     df_final1 = pd.merge(dfs1, df_train_label, how='left', left_on='id', right_on='id')\n",
        "#     df_final2 = pd.merge(dfs1, df_train_label, how='inner', left_on='id', right_on='id')\n",
        "\n",
        "#     # print(df_final1)\n",
        "#     print(df_final1.shape) #(81120, 7)\n",
        "\n",
        "#     # print(df_final2)\n",
        "#     print(df_final2.shape) #(4641, 7)\n",
        "\n",
        "\n",
        "#     #only text and label - drop columns from old one and store in new one \n",
        "#     Corpus = df_final2.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "#     le = preprocessing.LabelEncoder()\n",
        "\n",
        "#     reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "#     sorted_replies = []\n",
        "#     for i in range(len(reply_dict_list)):\n",
        "#       values_2d = reply_dict_list[i].values()\n",
        "#       values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "#       sorted_replies.append(values_list_1d)\n",
        "\n",
        "#     Corpus['label'] = le.fit_transform(Corpus['label'])\n",
        "#     Corpus[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "#     Corpus[\"num_replies\"] = num_replies\n",
        "#     Corpus[\"dict_replies\"] = list_replies\n",
        "#     Corpus[\"all_replies_by_time\"] = sorted_replies\n",
        "#     print(Corpus.head(10))\n",
        "#     print(Corpus.dtypes)\n",
        "#     print(Corpus.shape)\n",
        "\n",
        "#     return Corpus\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDtDlbkgHA-c"
      },
      "source": [
        "# Check for class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMEN93mUMt8I"
      },
      "source": [
        "# Corpus = load_data(\"train.data.jsonl\",\"train.label.json\")\n",
        "# Corpus_dev = load_data(\"dev.data.jsonl\",\"dev.label.json\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7lA1-RtI3i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5815f48a-3b4e-4a07-bb32-d8391ca9434d"
      },
      "source": [
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.min_rows', 100)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "with open(\"train.data.jsonl\", 'r', encoding='utf-8') as train_data_file:\n",
        "  list_of_rlists = []\n",
        "  chunks = [] #list of all the dataframes for all source and reply tweets\n",
        "  num_replies = [] #list of number of replies for every source tweet \n",
        "  dict_replies= {} #dict of {created_At_time:[reply tweet]} for every source tweet\n",
        "  list_replies = []\n",
        "  dict_replies_list = []\n",
        "  for line in train_data_file:\n",
        "    train_Data_list = json.loads(line)\n",
        "    df = pd.DataFrame.from_dict(train_Data_list)\n",
        "    df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "    df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "    df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "\n",
        "    replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "    replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "    # print(len(replies_list_2d))\n",
        "    replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "    # print(len(replies_list_1d))\n",
        "    reply_length = len(replies_list_1d)\n",
        "    list_replies.append(reply_length) \n",
        "    dict_replies_list.append(dict_replies)\n",
        "\n",
        "    chunks.append(df)\n",
        "    list_of_rlists.append(replies_list)\n",
        "    num_replies.append(len_reply_list)\n",
        "    # print(\"The output of list_of_rlists in every iteration=\",list_of_rlists,\"\\n\",len(list_of_rlists))\n",
        "  dfs1 = pd.concat(chunks)\n",
        "train_data_file.close()\n",
        "#print(dfs1)\n",
        "print(\"The len of list of list of replies=\", len(list_of_rlists))\n",
        "\n",
        "\n",
        "with open(\"train.label.json\") as train_label_file:\n",
        "    train_label_json_file = json.load(train_label_file)\n",
        "    df_train_label = pd.DataFrame(list(train_label_json_file.items()),columns = ['id', 'label'])\n",
        "train_label_file.close()\n",
        "\n",
        "#check for class imbalance\n",
        "print(df_train_label)\n",
        "print(df_train_label.groupby('label').size())\n",
        "# non - rumour\n",
        "# 3058\n",
        "# rumour\n",
        "# 1583a\n",
        "\n",
        "#check the id and if it is present in rumours/non-rumours labels, then add it to the row of that dataframe (check from the labels dataframe)\n",
        "df_final1 = pd.merge(dfs1, df_train_label, how='left', left_on='id', right_on='id')\n",
        "df_final2 = pd.merge(dfs1, df_train_label, how='inner', left_on='id', right_on='id')\n",
        "\n",
        "# print(df_final1)\n",
        "print(df_final1.shape) #(81120, 7)\n",
        "\n",
        "# print(df_final2)\n",
        "print(df_final2.shape) #(4641, 7)\n",
        "\n",
        "\n",
        "#only text and label - drop columns from old one and store in new one \n",
        "Corpus = df_final2.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "sorted_replies = []\n",
        "for i in range(len(reply_dict_list)):\n",
        "  values_2d = reply_dict_list[i].values()\n",
        "  values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "  sorted_replies.append(values_list_1d)\n",
        "\n",
        "Corpus['label'] = le.fit_transform(Corpus['label'])\n",
        "Corpus[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "Corpus[\"num_replies\"] = num_replies\n",
        "Corpus[\"dict_replies\"] = list_replies\n",
        "Corpus[\"all_replies_by_time\"] = sorted_replies\n",
        "print(Corpus.head(10))\n",
        "print(Corpus.dtypes)\n",
        "print(Corpus.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The len of list of list of replies= 4641\n",
            "                      id       label\n",
            "0     552800070199148544  non-rumour\n",
            "1     544388259359387648  non-rumour\n",
            "2     552805970536333314  non-rumour\n",
            "3     525071376084791297      rumour\n",
            "4     498355319979143168  non-rumour\n",
            "5     553591259672379392      rumour\n",
            "6     580342401392312321  non-rumour\n",
            "7     498609222020780033  non-rumour\n",
            "8     544518044643770368  non-rumour\n",
            "9     553566877956734976      rumour\n",
            "10    524946336190586880      rumour\n",
            "11    553233414145310720  non-rumour\n",
            "12    544391176137089024      rumour\n",
            "13    525003253185277952  non-rumour\n",
            "14    553100922268942336  non-rumour\n",
            "15    498259432237969408  non-rumour\n",
            "16    552851546384646145  non-rumour\n",
            "17    500322242296041473  non-rumour\n",
            "18    525059348574130176      rumour\n",
            "19    544279945128591360      rumour\n",
            "20    525033278555033601      rumour\n",
            "21    553160244847984640  non-rumour\n",
            "22    544425494763302913  non-rumour\n",
            "23    544286688436969472  non-rumour\n",
            "24    544483913830531073  non-rumour\n",
            "25    498500813845966848  non-rumour\n",
            "26    544285296565948416      rumour\n",
            "27    524980744658382848      rumour\n",
            "28    500284871541940225      rumour\n",
            "29    544312118497914880  non-rumour\n",
            "30    552804454773567488  non-rumour\n",
            "31    553538523266318336  non-rumour\n",
            "32    500317687239966722  non-rumour\n",
            "33    544280617160949760  non-rumour\n",
            "34    580337873691533313  non-rumour\n",
            "35    552812572731527168  non-rumour\n",
            "36    552817101057318913      rumour\n",
            "37    544444661859749890  non-rumour\n",
            "38    544428008258297856  non-rumour\n",
            "39    553133340795363328  non-rumour\n",
            "40    524974779540197376      rumour\n",
            "41    552797058990870528  non-rumour\n",
            "42    552813155572006912  non-rumour\n",
            "43    552807874192482305  non-rumour\n",
            "44    552808282398924800      rumour\n",
            "45    552808487924011008  non-rumour\n",
            "46    544307589245587457  non-rumour\n",
            "47    544351832483450880      rumour\n",
            "48    553490160344502273  non-rumour\n",
            "49    524957429889777665      rumour\n",
            "...                  ...         ...\n",
            "4591  500163501961142275  non-rumour\n",
            "4592  552826238407294977  non-rumour\n",
            "4593  498256407980826625  non-rumour\n",
            "4594  525012738280792065  non-rumour\n",
            "4595  524952437137092608      rumour\n",
            "4596  553127632947396609  non-rumour\n",
            "4597  580321948070240256      rumour\n",
            "4598  500285282650832896  non-rumour\n",
            "4599  500285919539511296      rumour\n",
            "4600  500305514447003649      rumour\n",
            "4601  544306616137293824  non-rumour\n",
            "4602  544271284796784640      rumour\n",
            "4603  553591548613386241  non-rumour\n",
            "4604  544294692456628224      rumour\n",
            "4605  524930678983753728  non-rumour\n",
            "4606  524956731638841344  non-rumour\n",
            "4607  500359006331092994      rumour\n",
            "4608  553477554041257985      rumour\n",
            "4609  544271815007145984      rumour\n",
            "4610  552834754056556544  non-rumour\n",
            "4611  553532980871053312      rumour\n",
            "4612  544277728930062336      rumour\n",
            "4613  553164722380742656  non-rumour\n",
            "4614  524942687003484161  non-rumour\n",
            "4615  544514474963922944      rumour\n",
            "4616  544399927045283840      rumour\n",
            "4617  524932327907270656      rumour\n",
            "4618  500393152444772352      rumour\n",
            "4619  499662393627410432  non-rumour\n",
            "4620  553585447235223552  non-rumour\n",
            "4621  525021697003782145      rumour\n",
            "4622  524934827909210112      rumour\n",
            "4623  580339662096605184  non-rumour\n",
            "4624  524943615437848576  non-rumour\n",
            "4625  499455799723184130  non-rumour\n",
            "4626  524949443607412737      rumour\n",
            "4627  553578566563426305  non-rumour\n",
            "4628  525012912562536448  non-rumour\n",
            "4629  580336766663540736      rumour\n",
            "4630  544291619084959744      rumour\n",
            "4631  580336528955691008  non-rumour\n",
            "4632  500344846612447233      rumour\n",
            "4633  524949315245322241      rumour\n",
            "4634  552807783385800705  non-rumour\n",
            "4635  552805596198871040      rumour\n",
            "4636  524959027516932096      rumour\n",
            "4637  524940940721418240  non-rumour\n",
            "4638  580331453889708032      rumour\n",
            "4639  552820384039706624  non-rumour\n",
            "4640  499426471300329472  non-rumour\n",
            "\n",
            "[4641 rows x 2 columns]\n",
            "label\n",
            "non-rumour    3058\n",
            "rumour        1583\n",
            "dtype: int64\n",
            "(81120, 8)\n",
            "(4641, 8)\n",
            "The length of sorted list is =  4641\n",
            "                   id                                              tweet                      created_at  label                                        all_replies  num_replies  dict_replies                                all_replies_by_time\n",
            "0  552800070199148544  How to respond to the murderous attack on Char...  Wed Jan 07 12:13:01 +0000 2015      0  [@Heresy_Corner @KrustyAllslopp \\nJews label a...           28            28  [@Heresy_Corner @KrustyAllslopp \\nJews label a...\n",
            "1  544388259359387648  You can't condemn an entire race, nation or re...  Mon Dec 15 07:07:29 +0000 2014      0  [@LozzaCopland How many times are we going to ...           29            29  [@LozzaCopland How many times are we going to ...\n",
            "2  552805970536333314  Attempts to extend blame for this to all Musli...  Wed Jan 07 12:36:28 +0000 2015      0  [@iyad_elbaghdadi @Axxeen when will we see mas...           15            15  [@iyad_elbaghdadi @Axxeen when will we see mas...\n",
            "3  525071376084791297  Rest in Peace, Cpl. Nathan Cirillo. Killed tod...  Wed Oct 22 23:49:06 +0000 2014      1  [@GigiGraciette hot guy, @Corey_Frizzell hey f...           13            13  [@GigiGraciette hot guy, @Corey_Frizzell hey f...\n",
            "4  498355319979143168  People DEBATING whether #MikeBrown shoplifted ...  Sun Aug 10 06:29:01 +0000 2014      0  [@kurbster3 @VABVOX @commonman80 yeah and shoo...           12            12  [@kurbster3 @VABVOX @commonman80 yeah and shoo...\n",
            "5  553591259672379392  Update - PA: gunman holding hostages in #Paris...  Fri Jan 09 16:36:56 +0000 2015      1  [@SkyNewsBreak Wish we'd been as decisive with...           20            20  [@SkyNewsBreak Wish we'd been as decisive with...\n",
            "6  580342401392312321  Here's a recap of the key points so far in the...  Tue Mar 24 12:16:25 +0000 2015      0  [.@SkyNews Correction:- \"Germanwings\" is the a...            4             4  [.@SkyNews Correction:- \"Germanwings\" is the a...\n",
            "7  498609222020780033  #Ferguson protestors have shut down a major in...  Sun Aug 10 23:17:56 +0000 2014      0  [“@Dreamdefenders: #Ferguson protestors have s...           11            11  [“@Dreamdefenders: #Ferguson protestors have s...\n",
            "8  544518044643770368  #BREAKING: Police have confirmed Sydney hostag...  Mon Dec 15 15:43:13 +0000 2014      0  [@CTVNews @JenniDavis91 It's not over.  As lon...            4             4  [@CTVNews @JenniDavis91 It's not over.  As lon...\n",
            "9  553566877956734976  France faces 2 hostage-taking attacks; Paris k...  Fri Jan 09 15:00:03 +0000 2015      1  [@AP  horrible 😭, @AP Haven't we had it with r...            8             8  [@AP  horrible 😭, @AP Haven't we had it with r...\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "label                   int64\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "dict_replies            int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n",
            "(4641, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z38ccGYUhrY",
        "outputId": "2ce296de-53f9-46ef-c3d9-e02aec4797cd"
      },
      "source": [
        "diff =Corpus[Corpus.num_replies != Corpus.dict_replies]\n",
        "print(len(diff))\n",
        "diff\n",
        "print(diff)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "Empty DataFrame\n",
            "Columns: [id, tweet, created_at, label, all_replies, num_replies, dict_replies, all_replies_by_time]\n",
            "Index: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWAzRC_fZ86L",
        "outputId": "d5948c45-e4f3-4e6a-88a5-ba8bee16b7dc"
      },
      "source": [
        "with open(\"dev.data.jsonl\", 'r', encoding='utf-8') as dev_data_file:\n",
        "  list_of_rlists = []\n",
        "  chunks = []\n",
        "  num_replies = []\n",
        "  dict_replies= {}\n",
        "  list_replies = []\n",
        "  dict_replies_list = []\n",
        "  for line in dev_data_file:\n",
        "    dev_Data_list = json.loads(line)\n",
        "    df = pd.DataFrame.from_dict(dev_Data_list)\n",
        "    df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "    df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "    df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "\n",
        "    # replies_list,len_reply_list = get_replies(df)\n",
        "    replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "    replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "    # print(len(replies_list_2d))\n",
        "    replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "    # print(len(replies_list_1d))\n",
        "    reply_length = len(replies_list_1d)\n",
        "    list_replies.append(reply_length) \n",
        "    dict_replies_list.append(dict_replies)\n",
        "\n",
        "    chunks.append(df)\n",
        "    num_replies.append(len_reply_list)\n",
        "    list_of_rlists.append(replies_list)\n",
        "  dfs_dev = pd.concat(chunks)\n",
        "dev_data_file.close()\n",
        "#print(dfs_dev)\n",
        "\n",
        "with open(\"dev.label.json\") as dev_label_file:\n",
        "    dev_label_json_file = json.load(dev_label_file)\n",
        "    df_dev_label = pd.DataFrame(list(dev_label_json_file.items()),columns = ['id', 'label'])\n",
        "dev_label_file.close()\n",
        "\n",
        "#print(df_train_label)\n",
        "#print(df_train_label.groupby('label').size())\n",
        "# non - rumour\n",
        "# 3058\n",
        "# rumour\n",
        "# 1583\n",
        "\n",
        "#check the id and if it is present in rumours/non-rumours labels, then add it to the row of that dataframe (check from the labels dataframe)\n",
        "df_final1_dev = pd.merge(dfs_dev, df_dev_label, how='left', left_on='id', right_on='id')\n",
        "df_final2_dev = pd.merge(dfs_dev, df_dev_label, how='inner', left_on='id', right_on='id')\n",
        "\n",
        "# print(df_final1_dev)\n",
        "print(df_final1_dev.shape)\n",
        "# print(df_final2_dev)\n",
        "print(df_final2_dev.shape)\n",
        "\n",
        "reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "sorted_replies = []\n",
        "for i in range(len(reply_dict_list)):\n",
        "  values_2d = reply_dict_list[i].values()\n",
        "  values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "  sorted_replies.append(values_list_1d)\n",
        "\n",
        "#only text and label - drop columns from old one and store in new one \n",
        "Corpus_dev = df_final2_dev.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "le = preprocessing.LabelEncoder()\n",
        "Corpus_dev['label'] = le.fit_transform(Corpus_dev['label'])\n",
        "Corpus_dev[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "Corpus_dev[\"num_replies\"] = num_replies\n",
        "Corpus_dev[\"all_replies_by_time\"] = sorted_replies\n",
        "\n",
        "print(Corpus_dev.head(10))\n",
        "print(Corpus_dev.shape)\n",
        "print(Corpus_dev.dtypes)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10546, 8)\n",
            "(580, 8)\n",
            "The length of sorted list is =  580\n",
            "                   id                                              tweet                      created_at  label                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      1  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "1  524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014      0  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "2  553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015      0  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "3  580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015      1  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "4  544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014      1  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "5  499363921661140993  Mound City Bar Association has agreed to donat...  Wed Aug 13 01:16:51 +0000 2014      0  [@Nettaaaaaaaa why would a witness need an att...            3  [@Nettaaaaaaaa why would a witness need an att...\n",
            "6  500177556768034816  Thousands of New Yorkers shut down Times Squar...  Fri Aug 15 07:09:57 +0000 2014      0  [@OccupyLondon all of em white to, @allys_sons...           42  [@OccupyLondon all of em white to, @allys_sons...\n",
            "7  553191800408911872  Mosques attacked in France following 'Charlie ...  Thu Jan 08 14:09:37 +0000 2015      0  [The idiots are at it again. “@TIME: Mosques a...           22  [The idiots are at it again. “@TIME: Mosques a...\n",
            "8  524927281048080385  Snipers set up on National Art Gallery as we r...  Wed Oct 22 14:16:31 +0000 2014      1  [@dmatthewmillar @Russell_Barth @devo1400 @spa...           42  [@dmatthewmillar @Russell_Barth @devo1400 @spa...\n",
            "9  544308793623207936  Remember, Sydney terrorists have nothing to do...  Mon Dec 15 01:51:43 +0000 2014      0  [@alanBStardmp @bobbington99 Really? Let's ban...           48  [@alanBStardmp @bobbington99 Really? Let's ban...\n",
            "(580, 7)\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "label                   int64\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOFL_hf0Z8vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8614e96-f153-42f8-a41e-04928aca9f5f"
      },
      "source": [
        "with open(\"test.data.jsonl\", 'r', encoding='utf-8') as test_data_file:\n",
        "  list_of_rlists = []\n",
        "  chunks = []\n",
        "  num_replies = []\n",
        "  dict_replies= {}\n",
        "  list_replies = []\n",
        "  dict_replies_list = []\n",
        "\n",
        "  for line in test_data_file:\n",
        "    train_Data_list = json.loads(line)\n",
        "    df = pd.DataFrame.from_dict(train_Data_list)\n",
        "    df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "    df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "    df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "    # replies_list,len_reply_list = get_replies(df)\n",
        "    \n",
        "    # replies_list,len_reply_list = get_replies(df)\n",
        "    replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "    replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "    # print(len(replies_list_2d))\n",
        "    replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "    # print(len(replies_list_1d))\n",
        "    reply_length = len(replies_list_1d)\n",
        "    list_replies.append(reply_length) \n",
        "    dict_replies_list.append(dict_replies)\n",
        "\n",
        "    chunks.append(df)\n",
        "    num_replies.append(len_reply_list)\n",
        "    list_of_rlists.append(replies_list)\n",
        "  dfs1 = pd.concat(chunks)\n",
        "test_data_file.close()\n",
        "#print(dfs1)\n",
        "finalll = dfs1[dfs1['in_reply_to_status_id_str'].isna()] #source tweets\n",
        "# print(finalll)\n",
        "\n",
        "# print(dfs1[\"in_reply_to_status_id_str\"].isnull().sum())\n",
        "# c=0\n",
        "# if (dfs1[\"in_reply_to_status_id_str\"].isnull()).all():\n",
        "#     c = c+1\n",
        "# print(c)\n",
        "# cli = np.where(dfs1[\"in_reply_to_status_id_str\"].isnull())\n",
        "# print(cli)\n",
        "reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "sorted_replies = []\n",
        "for i in range(len(reply_dict_list)):\n",
        "  values_2d = reply_dict_list[i].values()\n",
        "  values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "  sorted_replies.append(values_list_1d)\n",
        "\n",
        "#only text - drop columns from old one and store in new one \n",
        "Corpus_test = finalll.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "Corpus_test[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "Corpus_test[\"num_replies\"] = num_replies\n",
        "Corpus_test[\"all_replies_by_time\"] = sorted_replies\n",
        "\n",
        "print(Corpus_test.head(10))\n",
        "print(Corpus_test.shape)\n",
        "print(Corpus_test.dtypes)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The length of sorted list is =  581\n",
            "                   id                                              tweet                      created_at                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  544382249178001408  5 people have been able to get out of Sydney c...  Mon Dec 15 06:43:36 +0000 2014  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...           16  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...\n",
            "0  525027317551079424  NEW: Sources: Deceased gunman who killed soldi...  Wed Oct 22 20:54:01 +0000 2014  [@albanyly @ABC @theviewtv @WorldNews piss off...           11  [@albanyly @ABC @theviewtv @WorldNews piss off...\n",
            "0  544273220128739329  ISIS FLAG VISIBLE AS GUNMAN SEIZES SYDNEY CAFE...  Sun Dec 14 23:30:22 +0000 2014  [@rConflictNews @PzFeed Its not ISIS flag its ...           39  [@rConflictNews @PzFeed Its not ISIS flag its ...\n",
            "0  499571799764770816  People of #Ferguson: Stop #attacking our brave...  Wed Aug 13 15:02:53 +0000 2014  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...           34  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...\n",
            "0  552844104418091008  #CharlieHebdo editor, assassinated today, said...  Wed Jan 07 15:08:00 +0000 2015  [@PeterPannier I'd be curious to know more abo...           46  [@PeterPannier I'd be curious to know more abo...\n",
            "0  524977651476623360  Soldier shot at War Memorial in Ottawa has die...  Wed Oct 22 17:36:40 +0000 2014  [Very sad news “@globeandmail: Soldier shot at...           15  [Very sad news “@globeandmail: Soldier shot at...\n",
            "0  544514988078280704  Gunman in #Sydneysiege identified as Muslim cl...  Mon Dec 15 15:31:04 +0000 2014  [@WSJ who believes he was a lonely wolf?, .@mk...           23  [@WSJ who believes he was a lonely wolf?, .@mk...\n",
            "0  524928863714168832  BREAKING: Injury reported after shooting at Ca...  Wed Oct 22 14:22:48 +0000 2014  [“@cnni: BREAKING: Injury reported after shoot...            3  [“@cnni: BREAKING: Injury reported after shoot...\n",
            "0  544390718253699072  LIVE: Updates on #SydneySiege via BBC http://t...  Mon Dec 15 07:17:16 +0000 2014  [@BBCOS @BBCWorld OMG.., @bbcos EUA ATACA BRAS...           10  [@BBCOS @BBCWorld OMG.., @bbcos EUA ATACA BRAS...\n",
            "0  580322349569994752  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:44 +0000 2015                            [@WSJeurope @rezel RIP]            1                            [@WSJeurope @rezel RIP]\n",
            "(581, 6)\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-vfxKZvl6M"
      },
      "source": [
        "We will randomly split the entire training data into two sets: a train set with 90% of the data and a validation set with 10% of the data. We will perform hyperparameter tuning using cross-validation on the train set and use the validation set to compare models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4HKAFTbvMwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c25592-b110-4fff-9943-ddb567f6f73b"
      },
      "source": [
        "replies_train = [' [SEP] '.join([str(tweet) for tweet in all_replies]) for all_replies in Corpus.all_replies_by_time.values]\n",
        "replies_dev = ['[SEP] '.join([str(tweet) for tweet in all_replies]) for all_replies in Corpus_dev.all_replies_by_time.values]\n",
        "replies_test = ['[SEP] '.join([str(tweet) for tweet in all_replies]) for all_replies in Corpus_test.all_replies_by_time.values]\n",
        "# X = Corpus.tweet.values \n",
        "X = Corpus.tweet.values + replies_train\n",
        "y = Corpus.label.values\n",
        "# print(Corpus.all_replies.values)\n",
        "\n",
        "# print(len(replies))\n",
        "# count=0\n",
        "# for i in Corpus.all_replies.values:\n",
        "#   # print(' '.join([str(elem) for elem in i]))\n",
        "#   list_to_str_reply = ' '.join([str(elem) for elem in i])\n",
        "#   count = count + 1\n",
        "# print(count, len(list_to_str_reply)) #4641\n",
        "# X_train = Corpus.tweet.values\n",
        "# X_val = Corpus_dev.tweet.values \n",
        "# input = f\"[CLS] {Source_tweet} [SEP] \" + ''.join([f\"{reply} [SEP]\" for reply in all_replies])\n",
        "\n",
        "X_train = list(zip(Corpus.tweet.values, replies_train))\n",
        "# X_train = Corpus.tweet.values + replies_train\n",
        "X_val = list(zip(Corpus_dev.tweet.values, replies_dev))\n",
        "y_train = Corpus.label.values\n",
        "y_val = Corpus_dev.label.values\n",
        "X_test = list(zip(Corpus_test.tweet.values, replies_test))\n",
        "print(len(X_train),len(X_val),len(X_test))\n",
        "print(type(X_train))\n",
        "for i in range(10):\n",
        "  print(X_train[i],\"\\n\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4641 580 581\n",
            "<class 'list'>\n",
            "('How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j', \"@Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. [SEP] @Heresy_Corner @KrustyAllslopp \\nNo one does. [SEP] @Heresy_Corner #ImCharlieHebdo [SEP] @KrustyAllslopp Ditto [SEP] @Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. [SEP] @Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours. [SEP] @Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings. [SEP] @KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims. [SEP] @berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp [SEP] @Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite! [SEP] @berg_han @Heresy_Corner It's a good point [SEP] @Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h [SEP] @Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. [SEP] @theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v [SEP] @Heresy_Corner @berg_han explored. [SEP] @berg_han @KrustyAllslopp And if that's the case, that is your problem. [SEP] @Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it [SEP] @Heresy_Corner Oh dear... Just saw those tweets... Blocked him. [SEP] @berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why. [SEP] @Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either [SEP] @Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not [SEP] @Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity. [SEP] @KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian. [SEP] @Heresy_Corner I know!  Gives me the creeps. [SEP] @Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( [SEP] @Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack. [SEP] @KrustyAllslopp It's remarkable how quickly they come out the woodwork. [SEP] @Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\") \n",
            "\n",
            "(\"You can't condemn an entire race, nation or religion based on the actions of a few radicals, please keep that in mind #sydneysiege\", '@LozzaCopland How many times are we going to have the \"Don\\'t judge Islam by this act of terrorism\" conversation? #sydneysiege [SEP] @LozzaCopland indeed not a question of Race or nation. It is this religion og Islam that creates Zombies. [SEP] @LozzaCopland Only very few people can maintain the perspective that you have in this hour of crisis. [SEP] @LozzaCopland .....forthcoming from ppl when all Christians are branded as pedophiles or \"just after your money\" etc. But apparently in.... [SEP] @LozzaCopland at the response to child sex abuse cases, and ppl slamming Hillsong etc. A full throated defence for these is not often..... [SEP] @LozzaCopland How odd its not an athiest holding hostages? I condemn all religious nuts for as long as they cause pain and suffering... [SEP] @LozzaCopland love the first 4 words in your bio 🙈🙉🙊 [SEP] .@lozzacopland Such wise bull Shit. My cat said as much. This is a real world oh wise one. Nazis were only doing jobs given them. History! [SEP] @LozzaCopland Actually I have, I condemn Islam and I also condemn you, for talking total crap #LeftistLuzer #sydneysiege [SEP] @LozzaCopland What\\'s you definition of a few?  Al Qaeda, al Nusra, ISIS, Iranian Ayatollahs, Boko Haram ... there are many more... [SEP] @LozzaCopland @mermayden you can blame their ideology. [SEP] @LozzaCopland quite the opposite. I\\'m saying that churches/Christianty in general are frequently slandered due to the actions of a few. Look [SEP] @LozzaCopland ....future that will change? [SEP] @gibosity Are you saying you condemn all Christians due to those horrible events? Shall we ban crosses, priest robes &amp; large church groups? [SEP] @LozzaCopland ...is VASTLY different [SEP] @LozzaCopland This is so right. People need to realize it\\'s not a whole. It\\'s a few. [SEP] @LozzaCopland We can condemn an ideology on the basis of the merits &amp; consequences of its ideas. Islam, like all religions, is an ideology. [SEP] @LozzaCopland I look forward to you jumping to the defence of Christianity when the next child abuse case is aired [SEP] @rogerlsimon 1.6 billion muslims worldwide, thats a little over a fifth of the world\\'s population, I think the word \"a few\" is appropriate [SEP] What if the entire race and religion has a majority of radicals that the minority don\\'t have the balls to condemn? #Islam @LozzaCopland [SEP] @LozzaCopland media is definitely to blame. But the reaction to this, compared to when an individual Christian makes the church look bad.... [SEP] @LozzaCopland said the well meaning naif. http://t.co/VWONoSuegw [SEP] @LozzaCopland Godwin - so quit condemining all Nazis /Godwin.  Islam is an IDEOLOGY, and therefore has direct influence on behavior. [SEP] @LozzaCopland True [SEP] @LozzaCopland may not be the majority..but wow, there is sure a lot of them, aren\\'t there ? [SEP] @gibosity I don\\'t think it helps when news is focused primarily on negative/fear inducing. People like to blame region not the individuals. [SEP] @LozzaCopland \\n\\nDefine \"few\" [SEP] @gibosity I agree there is a huge difference but I think it\\'s due to a lack of understanding/frequent media reports/mass hysteria &amp; fear [SEP] @LozzaCopland you can\\'t terrorize the entire country based on the actions of few .....keep in mind.....#sydneysiege') \n",
            "\n",
            "('Attempts to extend blame for this to all Muslims should be treated with the same disgust as attempts to justify the attacks. #CharlieHebdo', '@iyad_elbaghdadi @Axxeen when will we see mass protests from \\'moderate\\' Muslims against these killings? I think highly unlikely [SEP] @iyad_elbaghdadi @chris15474 Most people are blaming mad extremists [SEP] @iyad_elbaghdadi Amen. [SEP] @iyad_elbaghdadi it is crazy. Video footage will back such claims... [SEP] @iyad_elbaghdadi @ParidePalumbo Muslims are good people and they should find, fight and condemn killers. [SEP] @iyad_elbaghdadi @jgriffiths Of course. But the solution of doing nothing isn\\'t working. [SEP] @iyad_elbaghdadi Not sure about that anymore...there is something about the religion that is driving people towards terrorism [SEP] @iyad_elbaghdadi There must be muslims knowing these criminals. Will they talk? [SEP] @iyad_elbaghdadi @rickardc87 My religion was no different  in 11 century  but its evolved  in the 21 Century IRA relised dialogue wins peace [SEP] @iyad_elbaghdadi @edzsplace then every muslim needs to stand up and say that and stop being silent #nonmuslimlivesmatter [SEP] @iyad_elbaghdadi Then I can only suggest that condemnation comes from within your religion and is denounced more loudly than current efforts [SEP] @iyad_elbaghdadi @MattGoldstein26 Can we at least assume that these murderers demand and expect the support of all Muslims? [SEP] .@iyad_elbaghdadi you are not serious? Who else to blame? so called Islamic renewal on the hand of \"moderates\" radicalized  the masses! [SEP] @iyad_elbaghdadi @acarvin It\\'s a good point. But I\\'m not sure I would TOTALLY equate hatred with actual physical violence. [SEP] @iyad_elbaghdadi Without your own mechanism to control this, you will be seen as complicit. No one cares what a nice guy you are.') \n",
            "\n",
            "('Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/YzLXYX5JJt http://t.co/8F0qAcj9sg', '@GigiGraciette hot guy [SEP] @Corey_Frizzell hey frizz I was right downtown…crazy stuff [SEP] So sad. \"@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/YBIYHliJpy http://t.co/MjvFKwrD4W” [SEP] @GigiGraciette So sad, RIP Cpl. Nathan Cirillo and God Bless your family left behind. [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/c5HtPz4n6l http://t.co/VBGdDYutSO” [SEP] So sad \"@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/p6xCfWMgg1 http://t.co/gI92I5vhoB” [SEP] @PEItopcop Were u? That\\'s crazy! Were u in uni? The Sgt. Arms is a hero: he needs a medal. [SEP] 🙏😢 #Hero “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/eRzccSD7aE http://t.co/bOWMxA2WEQ” [SEP] @GigiGraciette HOW TOTALLY. SENSELESS. AND SHAME. IM SO SORRY OUT TO THE FAMLIE.  :( [SEP] @GigiGraciette WhatAreTheValuesOfCanadians?LookAtThisManHere\\nSincereCondolences\\nToTheFamilyFriends&amp;ColleaguesOfCplNathanCIRILLO\\nWeAreSoSorry [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed 2day in #OttawaShooting\\nhttp://t.co/7jtcHQr7wc http://t.co/t5ZYaeQO22” God bless [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/x4c34sMhFs http://t.co/YcTgpHSCbV” [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/EoWtbw9HJk http://t.co/goYw4BBRoj') \n",
            "\n",
            "(\"People DEBATING whether #MikeBrown shoplifted or not-- IT DOESN'T MATTER.\\nShoplifting isn't punishable by DEATH IN THE STREET. #Ferguson\", \"@kurbster3 @VABVOX @commonman80 yeah and shooting him 10 times while not even knowing if he did commit a crime is right..makes so much sense [SEP] @commonman80 I was just responding to all the people saying he'd shoplifted. It wouldn't matter if he had. You don't shoot unarmed kids. [SEP] @VABVOX \\nReports are saying HE STOLE NOTHING. Reports say, He was walking in the street with friends and the cop got pissed... [SEP] @__aviana Racists need to believe #MikeBrown was a criminal. They criminalize all young black men. He was UNARMED.@kurbster3 @commonman80 [SEP] @kurbster3 Legally it is. Now stop tweeting your racist excuses at me, please. @__aviana @commonman80 [SEP] @kurbster3 Says the dude with Hitler on his profile page. @__aviana @commonman80 [SEP] @commonman80 Me, too. It's so vile. Someone just made a joke to me about #MikeBrown. No empathy at all. @__aviana [SEP] @__aviana There's no arguing w/racists.Their minds are made up.They see all blacks as criminals.Even kids #MikeBrown @kurbster3 @commonman80 [SEP] @kurbster3 legally yes tf he is. [SEP] @VABVOX @kurbster3 @__aviana I already BLOCKED the Dumb Ass... [SEP] @kurbster3 @VABVOX @commonman80 reports say his hands were up and he was walking AWAY when he was shot 10 times [SEP] @kurbster3 @VABVOX @commonman80 like always blaming the victim and not holding the officer accountable for being unreasonable\") \n",
            "\n",
            "('Update - PA: gunman holding hostages in #Paris grocery has been killed', '@SkyNewsBreak Wish we\\'d been as decisive with the Lee Rigby killers [SEP] @SkyNewsBreak rot in hell! [SEP] “@SkyNewsBreak: Update - PA: gunman holding hostages in #Paris grocery has been killed” 🙏🙏 [SEP] @SkyNewsBreak @SteeryBlade GOOD 😃 [SEP] @SkyNewsBreak surely you mean Islamist terrorist not gunman !! [SEP] @SkyNewsBreak good shoot them all #Paris will mourn forever [SEP] @SkyNewsBreak Any news on the woman hostage taker? [SEP] @SkyNewsBreak thank fuck for that! [SEP] @SkyNewsBreak @SteeryBlade GET IN! Fucking lunatic prick [SEP] “@SkyNewsBreak: Update - PA: gunman holding hostages in #Paris grocery has been killed” #TangoDown [SEP] @maisy59 @SkyNewsBreak \"them all\" there was two.... [SEP] @SkyNewsBreak Amen to that [SEP] @SkyNewsBreak 👏👍 well done 🇫🇷best way to deal with the scum [SEP] Hope he fucking rots in hell. @SkyNewsBreak [SEP] @SkyNewsBreak good. [SEP] @SkyNewsBreak great result ,,,,no messing french love em , we would still be fucking around if happened here , PC begrade  rule the uk [SEP] @SkyNewsBreak ...... I hope it was VERY painful . Rot in hell. [SEP] @SkyNewsBreak @cumbermon good! hope he/she rots in hell,sending thoughts to the victims and hostages at this awful time. [SEP] @SkyNewsBreak Evil Bastards!! [SEP] .@SkyNewsBreak any news on the hostages or what? Dont care about the terrorists') \n",
            "\n",
            "(\"Here's a recap of the key points so far in the #GermanWings Alps plane crash. Live updates: http://t.co/8UPMsinQkX http://t.co/hNjZvcruqq\", '.@SkyNews Correction:- \"Germanwings\" is the airline name. [SEP] “@SkyNews: Here\\'s a recap of the key points so far in the #GermanWings Alps plane crash. http://t.co/avlzDaVkAs” so so sad :( [SEP] @SkyNews so many people are making planes crash. Some people are so selfish [SEP] @SkyNews terrible :(') \n",
            "\n",
            "('#Ferguson protestors have shut down a major intersection  https://t.co/2sLJdYKrP4', \"“@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/z2WvBI8C94” [SEP] @Dreamdefenders @NoWayNRA1 now? [SEP] “@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/7yNe8LkWFO” [SEP] @DrDoreenDupont @Dreamdefenders reported about 2 hours ago. Don't know if still going on now. [SEP] NO PEACE RT @Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/rZ4LwCOgss [SEP] @Dreamdefenders Wow..yeah., its gonna be a looong week for sure. Folks are sick and tired of being SICK and tired.. [SEP] I would've Tony Stewart all they ass “@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/BA9wSzoC9u” [SEP] @NoWayNRA1 @Dreamdefenders thanks. All police should wear cameras. That would save so much grief [SEP] @Dreamdefenders protests arent gonna do shit, we need to revolutionize Its the only way something will change. Protests only raise awareness [SEP] @Dreamdefenders @NoWayNRA1 the power of the people !!!! [SEP] “@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/PhuqegZHBR”\") \n",
            "\n",
            "('#BREAKING: Police have confirmed Sydney hostage taking is over. #Sydneysiege', '@CTVNews @JenniDavis91 It\\'s not over.  As long as Islam remains unfettered it\\'s not over by a long shot. [SEP] @CTVNews [SEP] @Harry_Styles “@CTVNews: #BREAKING: Police have confirmed Sydney hostage taking is over. #Sydneysiege” [SEP] Good \"@CTVNews: #BREAKING: Police have confirmed Sydney hostage taking is over. #Sydneysiege”') \n",
            "\n",
            "('France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/dWlaFSDKjL', \"@AP  horrible 😭 [SEP] @AP Haven't we had it with religion already? Enough is enough. Stop killing over dueling mythologies, and stop ignoring the cause #religion [SEP] “@AP: France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/bffFX60c0s” @LpHarvin [SEP] @AP \\nthe majority of Muslims have no desire to ostracize the cancer that is eating away at Islam... the world needs to do that [SEP] oh my... @AP RT: France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/RDrHnGu8SN [SEP] &gt; RT “@AP: France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/KB7OQuiUXA” [SEP] @AP 6 hostages, @FRANCE24 said. [SEP] @AP France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/OMI0El49pD\") \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pErITNxtyNpe"
      },
      "source": [
        "### 2.3. Load Test Data\n",
        "The test data contains 4555 examples with no label. About 300 examples are non-complaining tweets. Our task is to identify their `id` and examine manually whether our results are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "RB8DrPbfa-G1",
        "outputId": "6504c7a6-768e-4f08-c1ed-f30aaf59ff0b"
      },
      "source": [
        "# Load test data\n",
        "test_data = Corpus_test\n",
        "# Display 5 samples from the test data\n",
        "test_data.sample(5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>created_at</th>\n",
              "      <th>all_replies</th>\n",
              "      <th>num_replies</th>\n",
              "      <th>all_replies_by_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>500366417674268673</td>\n",
              "      <td>Wait, why aren't the cops pointing assault rif...</td>\n",
              "      <td>Fri Aug 15 19:40:24 +0000 2014</td>\n",
              "      <td>[@occupythemob @monteiro Why would they point ...</td>\n",
              "      <td>19</td>\n",
              "      <td>[@occupythemob @monteiro Why would they point ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553548404102414337</td>\n",
              "      <td>Still amazed by Uderzo's tributes to #CharlieH...</td>\n",
              "      <td>Fri Jan 09 13:46:38 +0000 2015</td>\n",
              "      <td>[@bdutt @mihirssharma lets pay our tributes to...</td>\n",
              "      <td>4</td>\n",
              "      <td>[@bdutt @mihirssharma lets pay our tributes to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553591049541914624</td>\n",
              "      <td>BREAKING: Suspects in the #CharlieHebdo massac...</td>\n",
              "      <td>Fri Jan 09 16:36:06 +0000 2015</td>\n",
              "      <td>[@cnni I have been watching live coverage and ...</td>\n",
              "      <td>30</td>\n",
              "      <td>[@cnni I have been watching live coverage and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553203041705664513</td>\n",
              "      <td>The hashtag #JeSuisAhmed is a powerful one. Fo...</td>\n",
              "      <td>Thu Jan 08 14:54:17 +0000 2015</td>\n",
              "      <td>[@moneyries @leonidasmujahid Stop Lying This A...</td>\n",
              "      <td>9</td>\n",
              "      <td>[@moneyries @leonidasmujahid Stop Lying This A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>553487579077550082</td>\n",
              "      <td>Hundreds of massacres committed daily against ...</td>\n",
              "      <td>Fri Jan 09 09:44:56 +0000 2015</td>\n",
              "      <td>[@shu2un_arabiyya  you are fucking lying .Fake...</td>\n",
              "      <td>18</td>\n",
              "      <td>[@shu2un_arabiyya  you are fucking lying .Fake...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id                                              tweet                      created_at                                        all_replies  num_replies                                all_replies_by_time\n",
              "0  500366417674268673  Wait, why aren't the cops pointing assault rif...  Fri Aug 15 19:40:24 +0000 2014  [@occupythemob @monteiro Why would they point ...           19  [@occupythemob @monteiro Why would they point ...\n",
              "0  553548404102414337  Still amazed by Uderzo's tributes to #CharlieH...  Fri Jan 09 13:46:38 +0000 2015  [@bdutt @mihirssharma lets pay our tributes to...            4  [@bdutt @mihirssharma lets pay our tributes to...\n",
              "0  553591049541914624  BREAKING: Suspects in the #CharlieHebdo massac...  Fri Jan 09 16:36:06 +0000 2015  [@cnni I have been watching live coverage and ...           30  [@cnni I have been watching live coverage and ...\n",
              "0  553203041705664513  The hashtag #JeSuisAhmed is a powerful one. Fo...  Thu Jan 08 14:54:17 +0000 2015  [@moneyries @leonidasmujahid Stop Lying This A...            9  [@moneyries @leonidasmujahid Stop Lying This A...\n",
              "0  553487579077550082  Hundreds of massacres committed daily against ...  Fri Jan 09 09:44:56 +0000 2015  [@shu2un_arabiyya  you are fucking lying .Fake...           18  [@shu2un_arabiyya  you are fucking lying .Fake..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79dYY3sxDCi"
      },
      "source": [
        "## 3. Set up GPU for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi1CoEOL1puh"
      },
      "source": [
        "Google Colab offers free GPUs and TPUs. Since we'll be training a large neural network it's best to utilize these features.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "`Runtime -> Change runtime type -> Hardware accelerator: GPU`\n",
        "\n",
        "Then we need to run the following cell to specify the GPU as the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7hxtI4l0SUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca94641f-1422-4c4a-f706-8e6e95610158"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98rwWTSw_dEI"
      },
      "source": [
        "# import nltk\n",
        "# # Uncomment to download \"stopwords\"\n",
        "# nltk.download(\"stopwords\")\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# def text_preprocessing(s):\n",
        "#     \"\"\"\n",
        "#     - Lowercase the sentence\n",
        "#     - Change \"'t\" to \"not\"\n",
        "#     - Remove \"@name\"\n",
        "#     - Isolate and remove punctuations except \"?\"\n",
        "#     - Remove other special characters\n",
        "#     - Remove stop words except \"not\" and \"can\"\n",
        "#     - Remove trailing whitespace\n",
        "#     \"\"\"\n",
        "#     s = s.lower()\n",
        "#     # Change 't to 'not'\n",
        "#     s = re.sub(r\"\\'t\", \" not\", s)\n",
        "#     # Remove @name\n",
        "#     s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
        "#     # Isolate and remove punctuations except '?'\n",
        "#     s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "#     s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "#     # Remove some special characters\n",
        "#     s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "#     # Remove stopwords except 'not' and 'can'\n",
        "#     s = \" \".join([word for word in s.split()\n",
        "#                   if word not in stopwords.words('english')\n",
        "#                   or word in ['not', 'can']])\n",
        "#     # Remove trailing whitespace\n",
        "#     s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    \n",
        "#     return s"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8jpfxygCvww"
      },
      "source": [
        "### 1.2. TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbD689AMC-aB"
      },
      "source": [
        "In information retrieval, **TF-IDF**, short for **term frequency–inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We will use TF-IDF to vectorize our text data before feeding them to machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQ3X7hPDYhn"
      },
      "source": [
        "# %%time\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# # Preprocess text\n",
        "# X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
        "# X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
        "# X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
        "\n",
        "# # Calculate TF-IDF\n",
        "# tf_idf = TfidfVectorizer(ngram_range=(1, 3), binary=True, smooth_idf=False) #initialise vectorisor\n",
        "# X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
        "# X_val_tfidf = tf_idf.transform(X_val_preprocessed)\n",
        "# X_test_tfidf = tf_idf.transform(X_test_preprocessed)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku1zIjIGVwR3"
      },
      "source": [
        "# # print(tf_idf.get_feature_names())\n",
        "# print(X_train_tfidf.shape)\n",
        "# print(X_val_tfidf.shape)\n",
        "# print(X_test_tfidf.shape)\n",
        "# # print(X_val_tfidf)\n",
        "# print(\"Sparse Matrix form of test data : \\n\")\n",
        "# X_val_tfidf.todense()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arsEHOKzFxdv"
      },
      "source": [
        "## 2. Train Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63HQtpzOInq-"
      },
      "source": [
        "### 2.1. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5E0Fa6GUyd"
      },
      "source": [
        "We will use cross-validation and AUC score to tune hyperparameters of our model. The function `get_auc_CV` will return the average AUC score from cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueXJsrhNGqlS"
      },
      "source": [
        "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "# def get_auc_CV(model):\n",
        "#     \"\"\"\n",
        "#     Return the average AUC score from cross-validation.\n",
        "#     \"\"\"\n",
        "#     # Set KFold to shuffle data before the split\n",
        "#     kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "#     # Get AUC scores\n",
        "#     auc = cross_val_score(\n",
        "#         model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
        "\n",
        "#     return auc.mean()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53MgflYPHNxh"
      },
      "source": [
        "The `MultinominalNB` class only have one hypterparameter - **alpha**. The code below will help us find the alpha value that gives us the highest CV AUC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKatLhhJGzn0"
      },
      "source": [
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
        "#                  for i in np.arange(1, 10, 0.1)],\n",
        "#                 index=np.arange(1, 10, 0.1))\n",
        "\n",
        "# best_alpha = np.round(res.idxmax(), 2)\n",
        "# print('Best alpha: ', best_alpha)\n",
        "\n",
        "# plt.plot(res)\n",
        "# plt.title('AUC vs. Alpha')\n",
        "# plt.xlabel('Alpha')\n",
        "# plt.ylabel('AUC')\n",
        "# plt.show()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaATcy1nIeE9"
      },
      "source": [
        "### 2.2. Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne-eoqM4Muna"
      },
      "source": [
        "To evaluate the performance of our model, we will calculate the accuracy rate and the AUC score of our model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2gb-9mJK2w"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    # print(\"preds\", preds)\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    # print(fpr, tpr, threshold)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnCfezJSM-41"
      },
      "source": [
        "By combining TF-IDF and the Naive Bayes algorithm, we achieve the accuracy rate of **72.65%** on the validation set. This value is the baseline performance and will be used to evaluate the performance of our fine-tune BERT model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwVGbLHLIwpl"
      },
      "source": [
        "# # Compute predicted probabilities\n",
        "# nb_model = MultinomialNB(alpha=1.8)\n",
        "# nb_model.fit(X_train_tfidf, y_train)\n",
        "# #Predict the response for test dataset\n",
        "# probs = nb_model.predict_proba(X_val_tfidf)\n",
        "# print(\"PROBS=\",probs)\n",
        "# pred = nb_model.predict(X_val_tfidf)\n",
        "# print(pred)\n",
        "# # Evaluate the classifier\n",
        "# evaluate_roc(probs, y_val)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1ur9cjdiMr"
      },
      "source": [
        "# pred = nb_model.predict(X_test_tfidf)\n",
        "# print(pred)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nt25QAXdiKm"
      },
      "source": [
        "# y_predicted = nb_model.predict(X_test_tfidf)\n",
        "# y_predicted_labels = le.inverse_transform(y_predicted) \n",
        "# print(y_predicted,y_predicted_labels)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2qN00wBGkE6"
      },
      "source": [
        "# y_predicted = nb_model.predict(X_val_tfidf)\n",
        "# y_predicted_labels = le.inverse_transform(y_predicted) \n",
        "# # print(y_predicted,y_predicted_labels)\n",
        "# print(len(y_predicted_labels))\n",
        "# unique, counts = np.unique(y_predicted, return_counts=True)\n",
        "# dict(zip(unique, counts))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GR2kFSsXjbL"
      },
      "source": [
        "# print(df_dev_label.groupby('label').size())\n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "# fig.suptitle(\"label\", fontsize=12)\n",
        "# Corpus_dev[\"label\"].reset_index().groupby(\"label\").count().sort_values(by= \n",
        "#        \"index\").plot(kind=\"barh\", legend=False, \n",
        "#         ax=ax).grid(axis='x')\n",
        "# plt.show()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0dlIEiYdiGV"
      },
      "source": [
        "# Corpus_pred = Corpus_dev\n",
        "# print(Corpus_pred.head())\n",
        "# Corpus_pred['label'] = y_predicted_labels\n",
        "# print(Corpus_pred.head())\n",
        "# Corpus_pred.set_index('id',inplace=True)\n",
        "# dictt = Corpus_pred.to_dict()['label']\n",
        "# print(dictt)\n",
        "# with open(\"dev.baseline1.json\", \"w\") as outfile: \n",
        "#     json.dump(dictt, outfile)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K847SfzNdh-k"
      },
      "source": [
        "# print(Corpus_pred.shape)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEPPYHa62JXF"
      },
      "source": [
        "# D - Fine-tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYJRzWI73eBJ"
      },
      "source": [
        "## 1. Install the Hugging Face Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxv-EJ2j31Iv"
      },
      "source": [
        "The transformer library of Hugging Face contains PyTorch implementation of state-of-the-art NLP models including BERT (from Google), GPT (from OpenAI) ... and pre-trained model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFiv8WGl4p40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982feea7-1a35-4729-861e-612a374fa667"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 11.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 22.6MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/2d/804332ee1eaf36c8be737e9c44da2f2aa449339c220a96b9a15ae7f61443/boto3-1.17.69-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/e3/5e49e9a83fb605aaa34a1c1173e607302fecae529428c28696fb18f1c2c9/tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 21.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.5MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.69\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/bb/06710dce8770adf852785785df7e15fc1363596b712766898b1c529e358e/botocore-1.20.69-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.69->boto3->transformers==2.8.0) (2.8.1)\n",
            "\u001b[31mERROR: botocore 1.20.69 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, jmespath, botocore, s3transfer, boto3, tokenizers, sacremoses, transformers\n",
            "Successfully installed boto3-1.17.69 botocore-1.20.69 jmespath-0.10.0 s3transfer-0.4.2 sacremoses-0.0.45 sentencepiece-0.1.95 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4sXctSh4sq0"
      },
      "source": [
        "## 2. Tokenization and Input Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygbZpK6qbIYE"
      },
      "source": [
        "Before tokenizing our text, we will perform some slight processing on our text including removing entity mentions (eg. @united) and some special character. The level of processing here is much less than in previous approachs because BERT was trained with the entire sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L_Rc7l4bgzJ"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text_ = [re.sub(r'(@.*?)[\\s]', ' ', i) for i in text]\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text_ = [re.sub(r'&amp;', '&', i) for i in text_]\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text_ = [re.sub(r'\\s+', ' ', i).strip() for i in text_]\n",
        "    text_ = ' [CLS] ' + text_[0]+ ' [SEP] ' + text_[1] + ' [SEP] '\n",
        "    return text_"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyYmHR8McE0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db44431c-0583-440f-8fc9-2c416d79c56b"
      },
      "source": [
        "# Print sentence 0\n",
        "print('Original: ', X_train[1])\n",
        "# for i in range(10):\n",
        "#     print(X_train[i])\n",
        "print('Processed: ', text_preprocessing(X_train[0]))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  (\"You can't condemn an entire race, nation or religion based on the actions of a few radicals, please keep that in mind #sydneysiege\", '@LozzaCopland How many times are we going to have the \"Don\\'t judge Islam by this act of terrorism\" conversation? #sydneysiege [SEP] @LozzaCopland indeed not a question of Race or nation. It is this religion og Islam that creates Zombies. [SEP] @LozzaCopland Only very few people can maintain the perspective that you have in this hour of crisis. [SEP] @LozzaCopland .....forthcoming from ppl when all Christians are branded as pedophiles or \"just after your money\" etc. But apparently in.... [SEP] @LozzaCopland at the response to child sex abuse cases, and ppl slamming Hillsong etc. A full throated defence for these is not often..... [SEP] @LozzaCopland How odd its not an athiest holding hostages? I condemn all religious nuts for as long as they cause pain and suffering... [SEP] @LozzaCopland love the first 4 words in your bio 🙈🙉🙊 [SEP] .@lozzacopland Such wise bull Shit. My cat said as much. This is a real world oh wise one. Nazis were only doing jobs given them. History! [SEP] @LozzaCopland Actually I have, I condemn Islam and I also condemn you, for talking total crap #LeftistLuzer #sydneysiege [SEP] @LozzaCopland What\\'s you definition of a few?  Al Qaeda, al Nusra, ISIS, Iranian Ayatollahs, Boko Haram ... there are many more... [SEP] @LozzaCopland @mermayden you can blame their ideology. [SEP] @LozzaCopland quite the opposite. I\\'m saying that churches/Christianty in general are frequently slandered due to the actions of a few. Look [SEP] @LozzaCopland ....future that will change? [SEP] @gibosity Are you saying you condemn all Christians due to those horrible events? Shall we ban crosses, priest robes &amp; large church groups? [SEP] @LozzaCopland ...is VASTLY different [SEP] @LozzaCopland This is so right. People need to realize it\\'s not a whole. It\\'s a few. [SEP] @LozzaCopland We can condemn an ideology on the basis of the merits &amp; consequences of its ideas. Islam, like all religions, is an ideology. [SEP] @LozzaCopland I look forward to you jumping to the defence of Christianity when the next child abuse case is aired [SEP] @rogerlsimon 1.6 billion muslims worldwide, thats a little over a fifth of the world\\'s population, I think the word \"a few\" is appropriate [SEP] What if the entire race and religion has a majority of radicals that the minority don\\'t have the balls to condemn? #Islam @LozzaCopland [SEP] @LozzaCopland media is definitely to blame. But the reaction to this, compared to when an individual Christian makes the church look bad.... [SEP] @LozzaCopland said the well meaning naif. http://t.co/VWONoSuegw [SEP] @LozzaCopland Godwin - so quit condemining all Nazis /Godwin.  Islam is an IDEOLOGY, and therefore has direct influence on behavior. [SEP] @LozzaCopland True [SEP] @LozzaCopland may not be the majority..but wow, there is sure a lot of them, aren\\'t there ? [SEP] @gibosity I don\\'t think it helps when news is focused primarily on negative/fear inducing. People like to blame region not the individuals. [SEP] @LozzaCopland \\n\\nDefine \"few\" [SEP] @gibosity I agree there is a huge difference but I think it\\'s due to a lack of understanding/frequent media reports/mass hysteria &amp; fear [SEP] @LozzaCopland you can\\'t terrorize the entire country based on the actions of few .....keep in mind.....#sydneysiege')\n",
            "Processed:   [CLS] How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j [SEP] Jews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. [SEP] No one does. [SEP] #ImCharlieHebdo [SEP] Ditto [SEP] What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. [SEP] Yes, until it becomes yours. [SEP] Why insult people who have nothing to do with this? People are genuinely offended by such drawings. [SEP] And neither am I! I think this has little to do with actual Muslims. [SEP] Ah, you don't like Jews. Bye bye. [SEP] Also they kid you along with benign stuff then ... WHAM it's like a river of shite! [SEP] It's a good point [SEP] How about this? http://t.co/d2qcaVkf2h [SEP] Organised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. [SEP] ...and this: http://t.co/LmYxpmzw3v [SEP] explored. [SEP] And if that's the case, that is your problem. [SEP] No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it [SEP] Oh dear... Just saw those tweets... Blocked him. [SEP] Because they have to learn to not be offended, that's why. [SEP] But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either [SEP] I get that ... I defend the right to free speech however there is a much broader context to this which is not [SEP] just for the record. I am not in any way, shape or form defending this atrocity. [SEP] Yes, remind me when was the last time Jews bombed the Guardian. [SEP] I know! Gives me the creeps. [SEP] There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( [SEP] If people insult something that's important to you, you feel that your identity is under attack. [SEP] It's remarkable how quickly they come out the woodwork. [SEP] Why is the correct response to brutality to offend lots of people who *don't* support that brutality? [SEP] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3acv6s95YYr"
      },
      "source": [
        "### 2.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1fRHtdU5dEn"
      },
      "source": [
        "In order to apply the pre-trained BERT, we must use the tokenizer provided by the library. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.\n",
        "\n",
        "In addition, we are required to add special tokens to the start and end of each sentence, pad & truncate all sentences to a single constant length, and explicitly specify what are padding tokens with the \"attention mask\".\n",
        "\n",
        "The `encode_plus` method of BERT tokenizer will:\n",
        "\n",
        "(1) split our text into tokens,\n",
        "\n",
        "(2) add the special `[CLS]` and `[SEP]` tokens, and\n",
        "\n",
        "(3) convert these tokens into indexes of the tokenizer vocabulary,\n",
        "\n",
        "(4) pad or truncate sentences to max length, and\n",
        "\n",
        "(5) create attention mask.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDAfbCle59tP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "adfd875dc62242e48df319736b0a3c9d",
            "758d96e8ee3446578532ce4078b587db",
            "5d256c42c1cc438d91d9aa837676408f",
            "85a4c7b239f44b62994c2442e09f2aad",
            "070ab29288cf4dda84776a775111f22e",
            "945a5b5969b146c1a361f9b164428bd1",
            "efa411894ca84a18bb16547db0486b41",
            "bdd72cb812d9413baee81500c84413c3"
          ]
        },
        "outputId": "8fe437d4-5c86-4930-9cb4-e77d23017f33"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "      \n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "#         Set MAX_LEN = 512\n",
        "\n",
        "# Pad input_structure to MAX_LEN\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adfd875dc62242e48df319736b0a3c9d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNE9oASMZ1bN"
      },
      "source": [
        "Before tokenizing, we need to specify the maximum length of our sentences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrbvKGNAlMtt"
      },
      "source": [
        "# # Concatenate train data and test data\n",
        "# all_tweets = np.concatenate([Corpus.tweet.values, test_data.tweet.values])\n",
        "# print(all_tweets)\n",
        "# print(type(all_tweets))\n",
        "# # Encode our concatenated data\n",
        "# encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "# print(encoded_tweets)\n",
        "# # Find the maximum length\n",
        "# max_len = max([len(sent) for sent in encoded_tweets])\n",
        "# print('Max length: ', max_len)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpdjBB9fmbu2"
      },
      "source": [
        "Now let's tokenize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTlQzTzAfCy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f4af52-06af-4392-a6be-2e6f259803ed"
      },
      "source": [
        "# Specify `MAX_LEN`\n",
        "# MAX_LEN = 64\n",
        "MAX_LEN = 512\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "# token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "# print('Original: ', X[0])\n",
        "# print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "# print(X_train, type(X_train), len(X_train))\n",
        "# print(X_val, type(X_val), len(X_val))\n",
        "\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
        "\n",
        "print(train_inputs, train_masks, val_inputs, val_masks)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "tensor([[  101,   101,  2129,  ...,  2123,  1005,   102],\n",
            "        [  101,   101,  2017,  ...,  7395,  2000,   102],\n",
            "        [  101,   101,  4740,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,   101,  9499,  ...,     0,     0,     0],\n",
            "        [  101,   101,  1999,  ...,     0,     0,     0],\n",
            "        [  101,   101, 12583,  ...,     0,     0,     0]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]) tensor([[  101,   101,  1001,  ...,     0,     0,     0],\n",
            "        [  101,   101,  2017,  ...,     0,     0,     0],\n",
            "        [  101,   101,  2031,  ...,  2017,  2123,   102],\n",
            "        ...,\n",
            "        [  101,   101,  2057,  ...,  2288,  2083,   102],\n",
            "        [  101,   101,  2197,  ...,  1012,  1012,   102],\n",
            "        [  101,   101, 13970,  ...,     0,     0,     0]]) tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZU8t5VNfvhY"
      },
      "source": [
        "### 2.2. Create PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoHdl3gFgMZY"
      },
      "source": [
        "We will create an iterator for our dataset using the torch DataLoader class. This will help save on memory during training and boost the training speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuYEc61gcGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea95d60-6d69-4d6b-fd47-c32205b9654d"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "\n",
        "Corpus_dev['label'] = le.fit_transform(Corpus_dev['label'])\n",
        "y_val=Corpus_dev.label.values\n",
        "print(type(y_val))\n",
        "print(y_val)\n",
        "print(Corpus_dev.dtypes)\n",
        "print(Corpus_dev.head())\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 10\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "print(train_data,train_sampler,train_dataloader)\n",
        "# Create the DataLoader for our validation set\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "label                   int64\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n",
            "                   id                                              tweet                      created_at  label                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      1  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "1  524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014      0  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "2  553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015      0  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "3  580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015      1  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "4  544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014      1  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7fb3f5ccadd0> <torch.utils.data.sampler.RandomSampler object at 0x7fb3b47e6650> <torch.utils.data.dataloader.DataLoader object at 0x7fb44a01db50>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmo6sdz0uYgP"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSRAga-yj17q"
      },
      "source": [
        "## 3. Train Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoOdsDgG8b_Z"
      },
      "source": [
        "### 3.1. Create BertClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA_yESCl5nuK"
      },
      "source": [
        "BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the `[CLS]` token is used as the features of the sequence to feed a classifier.\n",
        "\n",
        "The `transformers` library has the [`BertForSequenceClassification`](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification) class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n",
        "\n",
        "Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the `[CLS]` token and a single-hidden-layer feed-forward neural network as our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK41aBFSj5jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11430bf5-9724-43a3-aac1-669e8e02becd"
      },
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 40 µs, sys: 0 ns, total: 40 µs\n",
            "Wall time: 43.4 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwNrCgPh-yR7"
      },
      "source": [
        "### 3.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6iOXiN8-8gc"
      },
      "source": [
        "To fine-tune our Bert Classifier, we need to create an optimizer. The authors recommend following hyper-parameters:\n",
        "\n",
        "- Batch size: 16 or 32\n",
        "- Learning rate (Adam): 5e-5, 3e-5 or 2e-5\n",
        "- Number of epochs: 2, 3, 4\n",
        "\n",
        "Huggingface provided the [run_glue.py](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109) script, an examples of implementing the `transformers` library. In the script, the AdamW optimizer is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX7su7Q_269U"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=2):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41DRNjv4B0Ow"
      },
      "source": [
        "### 3.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYU-GQRZG0y8"
      },
      "source": [
        "We will train our Bert Classifier for 4 epochs. In each epoch, we will train our model and evaluate its performance on the validation set. In more details, we will:\n",
        "\n",
        "Training:\n",
        "- Unpack our data from the dataloader and load the data onto the GPU\n",
        "- Zero out gradients calculated in the previous pass\n",
        "- Perform a forward pass to compute logits and loss\n",
        "- Perform a backward pass to compute gradients (`loss.backward()`)\n",
        "- Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "- Update the model's parameters (`optimizer.step()`)\n",
        "- Update the learning rate (`scheduler.step()`)\n",
        "\n",
        "Evaluation:\n",
        "- Unpack our data and load onto the GPU\n",
        "- Forward pass\n",
        "- Compute loss and accuracy rate over the validation set\n",
        "\n",
        "The script below is commented with the details of our training and evaluation loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4HkhyECibW"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSfTy9LqiFD-"
      },
      "source": [
        "Now, let's start training our BertClassifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfYw7dJ0U0v6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810,
          "referenced_widgets": [
            "ebada6c0013945e4a1d6e87c7cb3b97e",
            "392c0425861540ab8377a587b39aac30",
            "1e93f835346f46f9adbed8fb9dba1167",
            "d669efb902554dc79b753cfe2b9629e0",
            "22a5022a14cb4bcb8151c20ec1078fd0",
            "4b8f12ad9d044f2c98412a78c32c81d2",
            "8a76f98286a243bdbe4ca33c2a283a37",
            "57b7b0bd21db4fa9917e882633d8c000",
            "8a70383b0dc94ac0a81591fe100b9ce4",
            "c21ba027254a4c80a87c99947679d058",
            "2041b68cb63848f6940275cc4539621b",
            "3b0db88d7e95410ca48aa076ece413f8",
            "76f350f9e89c4ac1adac955871f9bd45",
            "8d40cca2329c42b98e31d6412140cd14",
            "24efb8330d294b189ad5a655ec33ea13",
            "96c9bb74699e49bb9d1c17147014814c"
          ]
        },
        "outputId": "46a84428-9cb7-4fe3-a1d2-397a0a586ec7"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=1)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebada6c0013945e4a1d6e87c7cb3b97e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a70383b0dc94ac0a81591fe100b9ce4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   1    |   20    |   0.672378   |     -      |     -     |   36.78  \n",
            "   1    |   40    |   0.523072   |     -      |     -     |   35.38  \n",
            "   1    |   60    |   0.518171   |     -      |     -     |   35.93  \n",
            "   1    |   80    |   0.485604   |     -      |     -     |   36.35  \n",
            "   1    |   100   |   0.367349   |     -      |     -     |   36.40  \n",
            "   1    |   120   |   0.488889   |     -      |     -     |   36.33  \n",
            "   1    |   140   |   0.451202   |     -      |     -     |   36.48  \n",
            "   1    |   160   |   0.529305   |     -      |     -     |   36.59  \n",
            "   1    |   180   |   0.585137   |     -      |     -     |   36.58  \n",
            "   1    |   200   |   0.421160   |     -      |     -     |   36.48  \n",
            "   1    |   220   |   0.384849   |     -      |     -     |   36.44  \n",
            "   1    |   240   |   0.440666   |     -      |     -     |   36.46  \n",
            "   1    |   260   |   0.403065   |     -      |     -     |   36.59  \n",
            "   1    |   280   |   0.397604   |     -      |     -     |   36.56  \n",
            "   1    |   300   |   0.425134   |     -      |     -     |   36.42  \n",
            "   1    |   320   |   0.387032   |     -      |     -     |   36.48  \n",
            "   1    |   340   |   0.353648   |     -      |     -     |   36.55  \n",
            "   1    |   360   |   0.346373   |     -      |     -     |   36.41  \n",
            "   1    |   380   |   0.373903   |     -      |     -     |   36.52  \n",
            "   1    |   400   |   0.379503   |     -      |     -     |   36.62  \n",
            "   1    |   420   |   0.408057   |     -      |     -     |   36.44  \n",
            "   1    |   440   |   0.346870   |     -      |     -     |   36.51  \n",
            "   1    |   460   |   0.351641   |     -      |     -     |   36.45  \n",
            "   1    |   464   |   0.277125   |     -      |     -     |   5.81   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.435684   |  0.307133  |   86.90   |  882.46  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1FL3ztWeQ4p",
        "outputId": "4ee31917-bfeb-42e7-9bdc-d58b22d2af29"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 14165632252060421286\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 894238720\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5830177380436950917\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhdYFJmTecW6"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5ostg9kPlra"
      },
      "source": [
        "### 3.4. Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIlSTDA7Z9DF"
      },
      "source": [
        "The prediction step is similar to the evaluation step that we did in the training loop, but simpler. We will perform a forward pass to compute logits and apply softmax to calculate probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_w4erqGzpe"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XekOuD6KbS2Q"
      },
      "source": [
        "The Bert Classifer achieves 0.90 AUC score and 82.65% accuracy rate on the validation set. This result is 10 points better than the baseline method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcmj5s0eRMUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77f199d5-2b03-4619-957a-4f6a59b0434b"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "print(probs)\n",
        "# Evaluate the Bert classifier\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.19983636 0.8001637 ]\n",
            " [0.98131096 0.01868905]\n",
            " [0.9866703  0.01332965]\n",
            " ...\n",
            " [0.8821548  0.11784513]\n",
            " [0.46516824 0.53483176]\n",
            " [0.9538681  0.04613192]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "x71ccKpuCrQY",
        "outputId": "9a6504f5-0113-4399-97ac-aa1123d0e789"
      },
      "source": [
        "evaluate_roc(probs, y_val)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.9378\n",
            "Accuracy: 86.90%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS6C5yfSInyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d78e7626-dd07-493a-c536-9bf4df15f6f5"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "# print(probs)\n",
        "print(len(probs))\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.55\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "print(preds)\n",
        "print(len(preds))\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets predicted rumour: \", preds.sum())\n",
        "# label\n",
        "# non-rumour    393\n",
        "# rumour        187"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 1\n",
            " 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0]\n",
            "580\n",
            "Number of tweets predicted rumour:  180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_KpA1mraMk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3e3387-9750-4904-9588-a7573a19473e"
      },
      "source": [
        "unique, counts = np.unique(preds, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 400, 1: 180}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om6D0BajcpV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a85dde-d57b-4c3f-fd74-e75660d89f12"
      },
      "source": [
        "Corpus_pred_bert = Corpus_dev\n",
        "print(Corpus_pred_bert.head())\n",
        "# y_predicted = nb_model.predict(X_test_tfidf)\n",
        "y_labels = [\"non-rumour\" if i==0 else \"rumour\" for i in preds]\n",
        "# print(y_predicted,y_predicted_labels)\n",
        "Corpus_pred_bert['label'] = y_labels\n",
        "print(Corpus_pred_bert.head())\n",
        "# Corpus_pred_bert.set_index('id',inplace=True)\n",
        "dictt_bert = Corpus_pred_bert.to_dict()['label']\n",
        "print(dictt_bert)\n",
        "print(len(dictt_bert))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                   id                                              tweet                      created_at  label                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      1  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "1  524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014      0  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "2  553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015      0  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "3  580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015      1  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "4  544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014      1  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "                   id                                              tweet                      created_at       label                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      rumour  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "1  524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014  non-rumour  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "2  553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015  non-rumour  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "3  580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015  non-rumour  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "4  544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014  non-rumour  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "{0: 'rumour', 1: 'non-rumour', 2: 'non-rumour', 3: 'non-rumour', 4: 'non-rumour', 5: 'non-rumour', 6: 'non-rumour', 7: 'non-rumour', 8: 'non-rumour', 9: 'non-rumour', 10: 'non-rumour', 11: 'non-rumour', 12: 'non-rumour', 13: 'non-rumour', 14: 'non-rumour', 15: 'rumour', 16: 'non-rumour', 17: 'non-rumour', 18: 'rumour', 19: 'non-rumour', 20: 'rumour', 21: 'rumour', 22: 'non-rumour', 23: 'rumour', 24: 'non-rumour', 25: 'non-rumour', 26: 'non-rumour', 27: 'non-rumour', 28: 'rumour', 29: 'rumour', 30: 'non-rumour', 31: 'rumour', 32: 'rumour', 33: 'non-rumour', 34: 'rumour', 35: 'rumour', 36: 'rumour', 37: 'non-rumour', 38: 'non-rumour', 39: 'non-rumour', 40: 'rumour', 41: 'rumour', 42: 'rumour', 43: 'non-rumour', 44: 'rumour', 45: 'non-rumour', 46: 'rumour', 47: 'non-rumour', 48: 'non-rumour', 49: 'non-rumour', 50: 'non-rumour', 51: 'non-rumour', 52: 'non-rumour', 53: 'non-rumour', 54: 'non-rumour', 55: 'rumour', 56: 'non-rumour', 57: 'non-rumour', 58: 'rumour', 59: 'non-rumour', 60: 'non-rumour', 61: 'non-rumour', 62: 'rumour', 63: 'non-rumour', 64: 'non-rumour', 65: 'rumour', 66: 'non-rumour', 67: 'rumour', 68: 'rumour', 69: 'non-rumour', 70: 'rumour', 71: 'non-rumour', 72: 'rumour', 73: 'non-rumour', 74: 'non-rumour', 75: 'rumour', 76: 'non-rumour', 77: 'rumour', 78: 'non-rumour', 79: 'rumour', 80: 'non-rumour', 81: 'non-rumour', 82: 'rumour', 83: 'non-rumour', 84: 'rumour', 85: 'non-rumour', 86: 'non-rumour', 87: 'rumour', 88: 'non-rumour', 89: 'non-rumour', 90: 'non-rumour', 91: 'rumour', 92: 'non-rumour', 93: 'non-rumour', 94: 'rumour', 95: 'non-rumour', 96: 'non-rumour', 97: 'non-rumour', 98: 'rumour', 99: 'rumour', 100: 'rumour', 101: 'non-rumour', 102: 'rumour', 103: 'non-rumour', 104: 'rumour', 105: 'non-rumour', 106: 'rumour', 107: 'non-rumour', 108: 'non-rumour', 109: 'rumour', 110: 'rumour', 111: 'non-rumour', 112: 'non-rumour', 113: 'non-rumour', 114: 'non-rumour', 115: 'rumour', 116: 'rumour', 117: 'non-rumour', 118: 'non-rumour', 119: 'non-rumour', 120: 'rumour', 121: 'non-rumour', 122: 'non-rumour', 123: 'rumour', 124: 'non-rumour', 125: 'non-rumour', 126: 'non-rumour', 127: 'rumour', 128: 'non-rumour', 129: 'non-rumour', 130: 'rumour', 131: 'non-rumour', 132: 'non-rumour', 133: 'non-rumour', 134: 'non-rumour', 135: 'non-rumour', 136: 'rumour', 137: 'rumour', 138: 'non-rumour', 139: 'rumour', 140: 'non-rumour', 141: 'non-rumour', 142: 'non-rumour', 143: 'rumour', 144: 'rumour', 145: 'non-rumour', 146: 'non-rumour', 147: 'rumour', 148: 'non-rumour', 149: 'non-rumour', 150: 'rumour', 151: 'non-rumour', 152: 'non-rumour', 153: 'non-rumour', 154: 'non-rumour', 155: 'non-rumour', 156: 'non-rumour', 157: 'non-rumour', 158: 'non-rumour', 159: 'rumour', 160: 'non-rumour', 161: 'rumour', 162: 'rumour', 163: 'non-rumour', 164: 'rumour', 165: 'non-rumour', 166: 'non-rumour', 167: 'non-rumour', 168: 'non-rumour', 169: 'rumour', 170: 'non-rumour', 171: 'non-rumour', 172: 'rumour', 173: 'non-rumour', 174: 'non-rumour', 175: 'non-rumour', 176: 'rumour', 177: 'non-rumour', 178: 'non-rumour', 179: 'non-rumour', 180: 'non-rumour', 181: 'non-rumour', 182: 'rumour', 183: 'non-rumour', 184: 'non-rumour', 185: 'non-rumour', 186: 'non-rumour', 187: 'non-rumour', 188: 'non-rumour', 189: 'non-rumour', 190: 'non-rumour', 191: 'rumour', 192: 'non-rumour', 193: 'rumour', 194: 'non-rumour', 195: 'non-rumour', 196: 'non-rumour', 197: 'rumour', 198: 'non-rumour', 199: 'rumour', 200: 'non-rumour', 201: 'rumour', 202: 'non-rumour', 203: 'rumour', 204: 'non-rumour', 205: 'rumour', 206: 'rumour', 207: 'non-rumour', 208: 'non-rumour', 209: 'non-rumour', 210: 'non-rumour', 211: 'non-rumour', 212: 'non-rumour', 213: 'non-rumour', 214: 'non-rumour', 215: 'non-rumour', 216: 'non-rumour', 217: 'non-rumour', 218: 'non-rumour', 219: 'non-rumour', 220: 'non-rumour', 221: 'non-rumour', 222: 'non-rumour', 223: 'non-rumour', 224: 'non-rumour', 225: 'rumour', 226: 'non-rumour', 227: 'non-rumour', 228: 'non-rumour', 229: 'rumour', 230: 'non-rumour', 231: 'rumour', 232: 'non-rumour', 233: 'rumour', 234: 'non-rumour', 235: 'non-rumour', 236: 'non-rumour', 237: 'non-rumour', 238: 'non-rumour', 239: 'non-rumour', 240: 'rumour', 241: 'non-rumour', 242: 'rumour', 243: 'rumour', 244: 'non-rumour', 245: 'non-rumour', 246: 'non-rumour', 247: 'non-rumour', 248: 'non-rumour', 249: 'non-rumour', 250: 'rumour', 251: 'non-rumour', 252: 'non-rumour', 253: 'non-rumour', 254: 'rumour', 255: 'rumour', 256: 'rumour', 257: 'non-rumour', 258: 'non-rumour', 259: 'non-rumour', 260: 'non-rumour', 261: 'non-rumour', 262: 'non-rumour', 263: 'non-rumour', 264: 'non-rumour', 265: 'non-rumour', 266: 'non-rumour', 267: 'non-rumour', 268: 'rumour', 269: 'non-rumour', 270: 'rumour', 271: 'non-rumour', 272: 'rumour', 273: 'rumour', 274: 'rumour', 275: 'non-rumour', 276: 'non-rumour', 277: 'non-rumour', 278: 'rumour', 279: 'rumour', 280: 'rumour', 281: 'non-rumour', 282: 'rumour', 283: 'rumour', 284: 'non-rumour', 285: 'non-rumour', 286: 'non-rumour', 287: 'non-rumour', 288: 'non-rumour', 289: 'rumour', 290: 'non-rumour', 291: 'non-rumour', 292: 'non-rumour', 293: 'non-rumour', 294: 'rumour', 295: 'rumour', 296: 'non-rumour', 297: 'rumour', 298: 'rumour', 299: 'non-rumour', 300: 'non-rumour', 301: 'non-rumour', 302: 'non-rumour', 303: 'non-rumour', 304: 'non-rumour', 305: 'non-rumour', 306: 'rumour', 307: 'non-rumour', 308: 'rumour', 309: 'non-rumour', 310: 'non-rumour', 311: 'rumour', 312: 'non-rumour', 313: 'non-rumour', 314: 'non-rumour', 315: 'non-rumour', 316: 'rumour', 317: 'non-rumour', 318: 'rumour', 319: 'non-rumour', 320: 'rumour', 321: 'rumour', 322: 'non-rumour', 323: 'non-rumour', 324: 'non-rumour', 325: 'non-rumour', 326: 'non-rumour', 327: 'rumour', 328: 'non-rumour', 329: 'rumour', 330: 'non-rumour', 331: 'non-rumour', 332: 'non-rumour', 333: 'non-rumour', 334: 'rumour', 335: 'rumour', 336: 'rumour', 337: 'rumour', 338: 'non-rumour', 339: 'rumour', 340: 'non-rumour', 341: 'non-rumour', 342: 'non-rumour', 343: 'non-rumour', 344: 'non-rumour', 345: 'non-rumour', 346: 'rumour', 347: 'non-rumour', 348: 'non-rumour', 349: 'non-rumour', 350: 'non-rumour', 351: 'rumour', 352: 'non-rumour', 353: 'non-rumour', 354: 'rumour', 355: 'non-rumour', 356: 'non-rumour', 357: 'non-rumour', 358: 'non-rumour', 359: 'non-rumour', 360: 'non-rumour', 361: 'rumour', 362: 'non-rumour', 363: 'non-rumour', 364: 'non-rumour', 365: 'non-rumour', 366: 'non-rumour', 367: 'non-rumour', 368: 'non-rumour', 369: 'non-rumour', 370: 'non-rumour', 371: 'rumour', 372: 'non-rumour', 373: 'non-rumour', 374: 'non-rumour', 375: 'non-rumour', 376: 'non-rumour', 377: 'non-rumour', 378: 'rumour', 379: 'rumour', 380: 'rumour', 381: 'non-rumour', 382: 'rumour', 383: 'non-rumour', 384: 'non-rumour', 385: 'non-rumour', 386: 'rumour', 387: 'non-rumour', 388: 'rumour', 389: 'rumour', 390: 'non-rumour', 391: 'rumour', 392: 'non-rumour', 393: 'non-rumour', 394: 'non-rumour', 395: 'non-rumour', 396: 'rumour', 397: 'non-rumour', 398: 'non-rumour', 399: 'rumour', 400: 'rumour', 401: 'rumour', 402: 'rumour', 403: 'rumour', 404: 'non-rumour', 405: 'non-rumour', 406: 'non-rumour', 407: 'rumour', 408: 'rumour', 409: 'non-rumour', 410: 'rumour', 411: 'non-rumour', 412: 'non-rumour', 413: 'non-rumour', 414: 'non-rumour', 415: 'non-rumour', 416: 'rumour', 417: 'non-rumour', 418: 'non-rumour', 419: 'non-rumour', 420: 'non-rumour', 421: 'non-rumour', 422: 'non-rumour', 423: 'non-rumour', 424: 'non-rumour', 425: 'non-rumour', 426: 'non-rumour', 427: 'non-rumour', 428: 'non-rumour', 429: 'non-rumour', 430: 'rumour', 431: 'non-rumour', 432: 'non-rumour', 433: 'non-rumour', 434: 'rumour', 435: 'rumour', 436: 'non-rumour', 437: 'non-rumour', 438: 'non-rumour', 439: 'non-rumour', 440: 'non-rumour', 441: 'non-rumour', 442: 'non-rumour', 443: 'non-rumour', 444: 'non-rumour', 445: 'rumour', 446: 'non-rumour', 447: 'rumour', 448: 'rumour', 449: 'non-rumour', 450: 'non-rumour', 451: 'non-rumour', 452: 'non-rumour', 453: 'non-rumour', 454: 'rumour', 455: 'non-rumour', 456: 'rumour', 457: 'non-rumour', 458: 'non-rumour', 459: 'non-rumour', 460: 'rumour', 461: 'non-rumour', 462: 'non-rumour', 463: 'non-rumour', 464: 'rumour', 465: 'rumour', 466: 'non-rumour', 467: 'non-rumour', 468: 'non-rumour', 469: 'non-rumour', 470: 'non-rumour', 471: 'non-rumour', 472: 'non-rumour', 473: 'non-rumour', 474: 'rumour', 475: 'rumour', 476: 'non-rumour', 477: 'rumour', 478: 'non-rumour', 479: 'non-rumour', 480: 'rumour', 481: 'non-rumour', 482: 'non-rumour', 483: 'non-rumour', 484: 'non-rumour', 485: 'rumour', 486: 'non-rumour', 487: 'rumour', 488: 'non-rumour', 489: 'rumour', 490: 'non-rumour', 491: 'non-rumour', 492: 'non-rumour', 493: 'rumour', 494: 'non-rumour', 495: 'non-rumour', 496: 'non-rumour', 497: 'non-rumour', 498: 'non-rumour', 499: 'non-rumour', 500: 'non-rumour', 501: 'rumour', 502: 'non-rumour', 503: 'rumour', 504: 'rumour', 505: 'non-rumour', 506: 'non-rumour', 507: 'non-rumour', 508: 'non-rumour', 509: 'non-rumour', 510: 'non-rumour', 511: 'rumour', 512: 'non-rumour', 513: 'non-rumour', 514: 'rumour', 515: 'rumour', 516: 'non-rumour', 517: 'rumour', 518: 'rumour', 519: 'non-rumour', 520: 'non-rumour', 521: 'rumour', 522: 'non-rumour', 523: 'non-rumour', 524: 'non-rumour', 525: 'non-rumour', 526: 'rumour', 527: 'rumour', 528: 'non-rumour', 529: 'rumour', 530: 'rumour', 531: 'rumour', 532: 'rumour', 533: 'rumour', 534: 'non-rumour', 535: 'non-rumour', 536: 'non-rumour', 537: 'non-rumour', 538: 'non-rumour', 539: 'non-rumour', 540: 'non-rumour', 541: 'rumour', 542: 'rumour', 543: 'rumour', 544: 'non-rumour', 545: 'rumour', 546: 'non-rumour', 547: 'non-rumour', 548: 'non-rumour', 549: 'non-rumour', 550: 'non-rumour', 551: 'non-rumour', 552: 'rumour', 553: 'non-rumour', 554: 'non-rumour', 555: 'non-rumour', 556: 'non-rumour', 557: 'rumour', 558: 'non-rumour', 559: 'rumour', 560: 'rumour', 561: 'rumour', 562: 'non-rumour', 563: 'non-rumour', 564: 'non-rumour', 565: 'non-rumour', 566: 'non-rumour', 567: 'non-rumour', 568: 'rumour', 569: 'non-rumour', 570: 'non-rumour', 571: 'non-rumour', 572: 'non-rumour', 573: 'non-rumour', 574: 'non-rumour', 575: 'rumour', 576: 'non-rumour', 577: 'non-rumour', 578: 'non-rumour', 579: 'non-rumour'}\n",
            "580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUkUJ05wdhNz"
      },
      "source": [
        "with open(\"dev.baseline_bertA.json\", \"w\") as outfile: \n",
        "    json.dump(dictt_bert, outfile)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPWW75T9Le0D",
        "outputId": "876b1c91-868b-4e14-a373-174a4ccf4cd3"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, test_dataloader)\n",
        "# print(probs)\n",
        "print(len(probs))\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.55\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "print(preds)\n",
        "print(len(preds))\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets predicted rumour: \", preds.sum())\n",
        "# label\n",
        "# non-rumour    393\n",
        "# rumour        187\n",
        "\n",
        "unique, counts = np.unique(preds, return_counts=True)\n",
        "print(dict(zip(unique, counts)))\n",
        "\n",
        "Corpus_pred_bert = Corpus_test\n",
        "print(Corpus_pred_bert.head())\n",
        "# y_predicted = nb_model.predict(X_test_tfidf)\n",
        "y_labels = [\"non-rumour\" if i==0 else \"rumour\" for i in preds]\n",
        "# print(y_predicted,y_predicted_labels)\n",
        "Corpus_pred_bert['label'] = y_labels\n",
        "print(Corpus_pred_bert.head())\n",
        "# Corpus_pred_bert.set_index('id',inplace=True)\n",
        "dictt_bert = Corpus_pred_bert.to_dict()['label']\n",
        "print(dictt_bert)\n",
        "print(len(dictt_bert))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "581\n",
            "[1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0\n",
            " 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0\n",
            " 0 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0\n",
            " 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
            " 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 1\n",
            " 0 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 0\n",
            " 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
            " 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
            " 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 1 1 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
            " 0 0 1 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0\n",
            " 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
            "581\n",
            "Number of tweets predicted rumour:  204\n",
            "{0: 377, 1: 204}\n",
            "                   id                                              tweet                      created_at                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  544382249178001408  5 people have been able to get out of Sydney c...  Mon Dec 15 06:43:36 +0000 2014  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...           16  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...\n",
            "0  525027317551079424  NEW: Sources: Deceased gunman who killed soldi...  Wed Oct 22 20:54:01 +0000 2014  [@albanyly @ABC @theviewtv @WorldNews piss off...           11  [@albanyly @ABC @theviewtv @WorldNews piss off...\n",
            "0  544273220128739329  ISIS FLAG VISIBLE AS GUNMAN SEIZES SYDNEY CAFE...  Sun Dec 14 23:30:22 +0000 2014  [@rConflictNews @PzFeed Its not ISIS flag its ...           39  [@rConflictNews @PzFeed Its not ISIS flag its ...\n",
            "0  499571799764770816  People of #Ferguson: Stop #attacking our brave...  Wed Aug 13 15:02:53 +0000 2014  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...           34  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...\n",
            "0  552844104418091008  #CharlieHebdo editor, assassinated today, said...  Wed Jan 07 15:08:00 +0000 2015  [@PeterPannier I'd be curious to know more abo...           46  [@PeterPannier I'd be curious to know more abo...\n",
            "                   id                                              tweet                      created_at                                        all_replies  num_replies                                all_replies_by_time       label\n",
            "0  544382249178001408  5 people have been able to get out of Sydney c...  Mon Dec 15 06:43:36 +0000 2014  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...           16  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...      rumour\n",
            "0  525027317551079424  NEW: Sources: Deceased gunman who killed soldi...  Wed Oct 22 20:54:01 +0000 2014  [@albanyly @ABC @theviewtv @WorldNews piss off...           11  [@albanyly @ABC @theviewtv @WorldNews piss off...      rumour\n",
            "0  544273220128739329  ISIS FLAG VISIBLE AS GUNMAN SEIZES SYDNEY CAFE...  Sun Dec 14 23:30:22 +0000 2014  [@rConflictNews @PzFeed Its not ISIS flag its ...           39  [@rConflictNews @PzFeed Its not ISIS flag its ...      rumour\n",
            "0  499571799764770816  People of #Ferguson: Stop #attacking our brave...  Wed Aug 13 15:02:53 +0000 2014  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...           34  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...  non-rumour\n",
            "0  552844104418091008  #CharlieHebdo editor, assassinated today, said...  Wed Jan 07 15:08:00 +0000 2015  [@PeterPannier I'd be curious to know more abo...           46  [@PeterPannier I'd be curious to know more abo...  non-rumour\n",
            "{0: 'non-rumour'}\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaQc2PvlLSt9"
      },
      "source": [
        "with open(\"test.baseline_bertB.json\", \"w\") as outfile: \n",
        "    json.dump(dictt_bert, outfile)"
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}