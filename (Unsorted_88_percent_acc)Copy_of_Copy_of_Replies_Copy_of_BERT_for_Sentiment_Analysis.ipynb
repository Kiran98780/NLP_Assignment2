{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(Unsorted - 88 percent acc)Copy_of_Copy_of_Replies_Copy_of_BERT_for_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiran98780/NLP_Assignment2/blob/main/(Unsorted_88_percent_acc)Copy_of_Copy_of_Replies_Copy_of_BERT_for_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MstNuBjkCGsG"
      },
      "source": [
        "# Step 1 - Mount drive\n",
        "Mount your google drive which has the training, test and dev jsonlfiles and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouuno_uYZkxa",
        "outputId": "c7d6ed6a-16f0-41e4-ea76-db277a0c09f7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHZ0vJLqZwKH",
        "outputId": "5f49ccd3-74f2-49f8-f90d-e4e0bcd2f862"
      },
      "source": [
        "%cd /content/drive/My Drive/NLP_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxMCKPVZZ2Gp",
        "outputId": "85ff3340-43ec-4d57-b683-a9f3dd175f29"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copy_of_10_bert (1).ipynb'   sstcls_0.dat                 test.data.jsonl\n",
            " dev.baseline_03_BERT.json    sstcls_1.dat                 train.data.jsonl\n",
            " dev.data.jsonl               sstcls_2.dat                 train.label.json\n",
            " dev.label.json               sstcls_4.dat\n",
            " \u001b[0m\u001b[01;34mmodel_save\u001b[0m/                  test.baseline_03_BERT.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2MGn7cpVp83",
        "outputId": "2fc5274e-fb9a-433f-9d04-44fdb2c0bfd4"
      },
      "source": [
        "%cd /content/drive/My Drive/NLP_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/NLP_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqt0gJncVpxk",
        "outputId": "57eb903c-d5e7-48cc-9de0-2f0caa6dce44"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copy_of_10_bert (1).ipynb'   sstcls_0.dat                 test.data.jsonl\n",
            " dev.baseline_03_BERT.json    sstcls_1.dat                 train.data.jsonl\n",
            " dev.data.jsonl               sstcls_2.dat                 train.label.json\n",
            " dev.label.json               sstcls_4.dat\n",
            " \u001b[0m\u001b[01;34mmodel_save\u001b[0m/                  test.baseline_03_BERT.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slO_rmYgwmmE"
      },
      "source": [
        "# Step 2 - Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31OW0dhozvli"
      },
      "source": [
        "## 1. Load Essential Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lTXsMK3sNYr"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import time\n",
        "import collections\n",
        "import json\n",
        "from sklearn import preprocessing\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u07WRKnxsX96"
      },
      "source": [
        "## 2. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK07OJ21IS9Y"
      },
      "source": [
        "#function to return the replies as a list for a source tweet \n",
        "def get_replies(df):\n",
        "    dict_time = {}\n",
        "    for index, row in df.iterrows():\n",
        "        replyTw_list = []\n",
        "        #check if the in_reply_to_status_id_str string matches a number\n",
        "        x = re.search(r'\\d+', str(row['in_reply_to_status_id_str']))  \n",
        "        if x:\n",
        "            #append the reply tweets in a big_list and return the list\n",
        "            big_list.append(row[\"tweet\"]) \n",
        "            replyTw_list.append(row[\"tweet\"])\n",
        "            #if the tweets are created at the same time - append it to existing list\n",
        "            if (row[\"created_at\"] in dict_time):\n",
        "                replyTw_list.append(row[\"tweet\"])\n",
        "            dict_time[row[\"created_at\"]] = replyTw_list\n",
        "        else:\n",
        "            big_list = []\n",
        "    #returns the list of replies for a source tweet, length of the reply list and dict of {created_at_Time:reply_tweet}\n",
        "    return big_list,len(big_list),dict_time \n",
        "\n",
        "#function to return the list of reply tweets sorted by time for all source tweets\n",
        "#takes the list of dictionaries as a parameter of the form [{created_At_time1:[tweet1, tweet2]},{created_At_time2:[tweet3]}]\n",
        "def get_sorted_by_time(dict_replies_list):\n",
        "    sorted_list = []\n",
        "    for i in range(len(dict_replies_list)):\n",
        "        # dtime = 'Mon Dec 15 21:50:30 +0000 2014'\n",
        "        newkeys_list = []\n",
        "        for k, v in dict_replies_list[i].items():  \n",
        "            new_datetime = datetime.strftime(datetime.strptime(k,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S') \n",
        "            date_to_float = datetime.fromisoformat(new_datetime).timestamp()#1420646211.0\n",
        "            newkeys_list.append(date_to_float) #list of keys for the new dictionary \n",
        "            vals = list(dict_replies_list[i].values()) #list of values for the new dictionary \n",
        "        newdictionary = {k: v for k, v in zip(newkeys_list, vals)} #zip the keys and values to form a new dictionary \n",
        "        sorted_list.append(newdictionary)\n",
        "    print(\"The length of sorted list is = \", len(sorted_list))\n",
        "    return sorted_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVkXcFzrtREn"
      },
      "source": [
        "### 2.1. Load Train Data\n",
        "The train.data.jsonl has about 4k source tweets and about 81k source + reply tweets.\n",
        "\n",
        "The label files ([train,dev].label.json), are standard JSON files:\n",
        "{\n",
        "\"552800070199148544\": \"non-rumour\",\n",
        "\"544388259359387648\": \"non-rumour\",\n",
        "\"552805970536333314\": \"non-rumour\",\n",
        "\"525071376084791297\": \"rumour\"\n",
        "} \n",
        "\n",
        "All data files ([train,dev,test,covid].data.jsonl) are JSONL files, where each line is a JSON string. For these files, each line is an event: a list of tweets where the first tweet is a source tweet and the rest are reply tweets.\n",
        "\n",
        "Here the train data is loaded and merged with the labels and the resulting columns are \"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\", \"label\"]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-ltEHJIZ9Bi",
        "outputId": "3d95599d-41d5-49ae-af5a-cfd949c51f32"
      },
      "source": [
        "# pd.set_option('display.width', 1000)\n",
        "# pd.set_option('display.max_columns', 500)\n",
        "# pd.set_option('display.min_rows', 100)\n",
        "# pd.set_option('display.max_rows', 500)\n",
        "# def load_data(data_filename, label_filename):\n",
        "#     with open(data_filename, 'r', encoding='utf-8') as train_data_file:\n",
        "#       list_of_rlists = []\n",
        "#       chunks = [] #list of all the dataframes for all source and reply tweets\n",
        "#       num_replies = [] #list of number of replies for every source tweet \n",
        "#       dict_replies= {} #dict of {created_At_time:[reply tweet]} for every source tweet\n",
        "#       list_replies = []\n",
        "#       dict_replies_list = []\n",
        "#       for line in train_data_file:\n",
        "#         train_Data_list = json.loads(line)\n",
        "#         df = pd.DataFrame.from_dict(train_Data_list)\n",
        "#         df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "#         df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "#         df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "\n",
        "#         replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "#         replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "#         # print(len(replies_list_2d))\n",
        "#         replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "#         # print(len(replies_list_1d))\n",
        "#         reply_length = len(replies_list_1d)\n",
        "#         list_replies.append(reply_length) \n",
        "#         dict_replies_list.append(dict_replies)\n",
        "\n",
        "#         chunks.append(df)\n",
        "#         list_of_rlists.append(replies_list)\n",
        "#         num_replies.append(len_reply_list)\n",
        "#         # print(\"The output of list_of_rlists in every iteration=\",list_of_rlists,\"\\n\",len(list_of_rlists))\n",
        "#       dfs1 = pd.concat(chunks)\n",
        "#     train_data_file.close()\n",
        "#     #print(dfs1)\n",
        "#     print(\"The len of list of list of replies=\", len(list_of_rlists))\n",
        "\n",
        "\n",
        "#     with open(label_filename) as train_label_file:\n",
        "#         train_label_json_file = json.load(train_label_file)\n",
        "#         df_train_label = pd.DataFrame(list(train_label_json_file.items()),columns = ['id', 'label'])\n",
        "#     train_label_file.close()\n",
        "\n",
        "#     #check for class imbalance\n",
        "#     print(df_train_label)\n",
        "#     print(df_train_label.groupby('label').size())\n",
        "#     # non - rumour\n",
        "#     # 3058\n",
        "#     # rumour\n",
        "#     # 1583a\n",
        "\n",
        "#     #check the id and if it is present in rumours/non-rumours labels, then add it to the row of that dataframe (check from the labels dataframe)\n",
        "#     df_final1 = pd.merge(dfs1, df_train_label, how='left', left_on='id', right_on='id')\n",
        "#     df_final2 = pd.merge(dfs1, df_train_label, how='inner', left_on='id', right_on='id')\n",
        "\n",
        "#     # print(df_final1)\n",
        "#     print(df_final1.shape) #(81120, 7)\n",
        "\n",
        "#     # print(df_final2)\n",
        "#     print(df_final2.shape) #(4641, 7)\n",
        "\n",
        "\n",
        "#     #only text and label - drop columns from old one and store in new one \n",
        "#     Corpus = df_final2.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "#     le = preprocessing.LabelEncoder()\n",
        "\n",
        "#     reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "#     sorted_replies = []\n",
        "#     for i in range(len(reply_dict_list)):\n",
        "#       values_2d = reply_dict_list[i].values()\n",
        "#       values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "#       sorted_replies.append(values_list_1d)\n",
        "\n",
        "#     Corpus['label'] = le.fit_transform(Corpus['label'])\n",
        "#     Corpus[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "#     Corpus[\"num_replies\"] = num_replies\n",
        "#     Corpus[\"dict_replies\"] = list_replies\n",
        "#     Corpus[\"all_replies_by_time\"] = sorted_replies\n",
        "#     print(Corpus.head(10))\n",
        "#     print(Corpus.dtypes)\n",
        "#     print(Corpus.shape)\n",
        "\n",
        "#     return Corpus\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The len of list of list of replies= 4641\n",
            "(81120, 8)\n",
            "(4641, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDtDlbkgHA-c"
      },
      "source": [
        "# Check for class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMEN93mUMt8I"
      },
      "source": [
        "# Corpus = load_data(\"train.data.jsonl\",\"train.label.json\")\n",
        "# Corpus_dev = load_data(\"dev.data.jsonl\",\"dev.label.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7lA1-RtI3i5"
      },
      "source": [
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.min_rows', 100)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "with open(data_filename, 'r', encoding='utf-8') as train_data_file:\n",
        "  list_of_rlists = []\n",
        "  chunks = [] #list of all the dataframes for all source and reply tweets\n",
        "  num_replies = [] #list of number of replies for every source tweet \n",
        "  dict_replies= {} #dict of {created_At_time:[reply tweet]} for every source tweet\n",
        "  list_replies = []\n",
        "  dict_replies_list = []\n",
        "  for line in train_data_file:\n",
        "    train_Data_list = json.loads(line)\n",
        "    df = pd.DataFrame.from_dict(train_Data_list)\n",
        "    df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "    df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "    df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "\n",
        "    replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "    replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "    # print(len(replies_list_2d))\n",
        "    replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "    # print(len(replies_list_1d))\n",
        "    reply_length = len(replies_list_1d)\n",
        "    list_replies.append(reply_length) \n",
        "    dict_replies_list.append(dict_replies)\n",
        "\n",
        "    chunks.append(df)\n",
        "    list_of_rlists.append(replies_list)\n",
        "    num_replies.append(len_reply_list)\n",
        "    # print(\"The output of list_of_rlists in every iteration=\",list_of_rlists,\"\\n\",len(list_of_rlists))\n",
        "  dfs1 = pd.concat(chunks)\n",
        "train_data_file.close()\n",
        "#print(dfs1)\n",
        "print(\"The len of list of list of replies=\", len(list_of_rlists))\n",
        "\n",
        "\n",
        "with open(label_filename) as train_label_file:\n",
        "    train_label_json_file = json.load(train_label_file)\n",
        "    df_train_label = pd.DataFrame(list(train_label_json_file.items()),columns = ['id', 'label'])\n",
        "train_label_file.close()\n",
        "\n",
        "#check for class imbalance\n",
        "print(df_train_label)\n",
        "print(df_train_label.groupby('label').size())\n",
        "# non - rumour\n",
        "# 3058\n",
        "# rumour\n",
        "# 1583a\n",
        "\n",
        "#check the id and if it is present in rumours/non-rumours labels, then add it to the row of that dataframe (check from the labels dataframe)\n",
        "df_final1 = pd.merge(dfs1, df_train_label, how='left', left_on='id', right_on='id')\n",
        "df_final2 = pd.merge(dfs1, df_train_label, how='inner', left_on='id', right_on='id')\n",
        "\n",
        "# print(df_final1)\n",
        "print(df_final1.shape) #(81120, 7)\n",
        "\n",
        "# print(df_final2)\n",
        "print(df_final2.shape) #(4641, 7)\n",
        "\n",
        "\n",
        "#only text and label - drop columns from old one and store in new one \n",
        "Corpus = df_final2.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "sorted_replies = []\n",
        "for i in range(len(reply_dict_list)):\n",
        "  values_2d = reply_dict_list[i].values()\n",
        "  values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "  sorted_replies.append(values_list_1d)\n",
        "\n",
        "Corpus['label'] = le.fit_transform(Corpus['label'])\n",
        "Corpus[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "Corpus[\"num_replies\"] = num_replies\n",
        "Corpus[\"dict_replies\"] = list_replies\n",
        "Corpus[\"all_replies_by_time\"] = sorted_replies\n",
        "print(Corpus.head(10))\n",
        "print(Corpus.dtypes)\n",
        "print(Corpus.shape)\n",
        "\n",
        "return Corpus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z38ccGYUhrY",
        "outputId": "fbe8e999-b3a4-422d-ab18-bf6fee01aeee"
      },
      "source": [
        "diff =Corpus[Corpus.num_replies != Corpus.dict_replies]\n",
        "print(len(diff))\n",
        "diff\n",
        "print(diff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n",
            "                      id                                              tweet                      created_at  label                                        all_replies  num_replies  dict_replies                                all_replies_by_time\n",
            "23    544286688436969472  thinking of the people in Martin Place in Sydn...  Mon Dec 15 00:23:53 +0000 2014      0  [@canwejustzalfie @youtubings @pointlesszoeee ...          133           130  [@canwejustzalfie @youtubings @pointlesszoeee ...\n",
            "166   544346359948910592  Hearing about the hostage situation in Sydney,...  Mon Dec 15 04:21:00 +0000 2014      1  [@Ashton5SOS everything is gonna be fine 💪, @t...           27            21  [@Ashton5SOS everything is gonna be fine 💪, @t...\n",
            "862   544480083982168064  Sending all my love to Sydney. Horrible situat...  Mon Dec 15 13:12:22 +0000 2014      0  [@haleighsmithhh @AlexAllTimeLow there was a a...           24            23  [@haleighsmithhh @AlexAllTimeLow there was a a...\n",
            "1024  553546063752478722  At least two killed in hostage drama east of P...  Fri Jan 09 13:37:20 +0000 2015      1  [2 muertos más en París “@AFPphoto: At least t...           23            22  [2 muertos más en París “@AFPphoto: At least t...\n",
            "1448  544520379176599552  Glad to hear the #sydneysiege is over, but sad...  Mon Dec 15 15:52:29 +0000 2014      0  [@AndreaRussett HOW are U?, @AndreaRussett Chr...           18            17  [@AndreaRussett HOW are U?, @AndreaRussett Chr...\n",
            "2054  544301149348982784        My thoughts go out to the people of Sydney.  Mon Dec 15 01:21:21 +0000 2014      0  [@Caspar_Lee congrats you did it, @Caspar_Lee ...           46            45  [@Caspar_Lee cute, @Caspar_Lee cute, @Caspar_L...\n",
            "2582  499695918547861504  For those not in #Ferguson: what questions do ...  Wed Aug 13 23:16:05 +0000 2014      0  [@cbrodrick @WesleyLowery Thanks, you're right...          228           227  [@cbrodrick @WesleyLowery Thanks, you're right...\n",
            "3798  553555293645590528  Here’s the grocery store in eastern Paris wher...  Fri Jan 09 14:14:01 +0000 2015      0  [RT @MailOnline: Grocery store in Paris where ...           19             5  [RT @MailOnline: Grocery store in Paris where ...\n",
            "3886  553567076728963073  Zuckerberg on #CharlieHebdo: Facebook will pro...  Fri Jan 09 15:00:50 +0000 2015      0  [@mashable we can post what we want as long as...           36            35  [@mashable we can post what we want as long as...\n",
            "3933  544307028815253504  The CEO of Lindt says there are around 50 peop...  Mon Dec 15 01:44:43 +0000 2014      1  [@masna_bila   agree. I just got my car out of...           47            46  [@masna_bila   agree. I just got my car out of...\n",
            "4498  524970488590651395                              Whoops sorry. Ottawa*  Wed Oct 22 17:08:12 +0000 2014      0  [@riandawson bless you, @riandawson dummy, @ri...           18            17  [@riandawson dork ily, @riandawson dork ily, @...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWAzRC_fZ86L",
        "outputId": "dae4562c-f22d-4ee5-dda1-41dfdac92ac4"
      },
      "source": [
        "# with open(\"dev.data.jsonl\", 'r', encoding='utf-8') as dev_data_file:\n",
        "#   list_of_rlists = []\n",
        "#   chunks = []\n",
        "#   num_replies = []\n",
        "#   dict_replies= {}\n",
        "#   list_replies = []\n",
        "#   dict_replies_list = []\n",
        "#   for line in dev_data_file:\n",
        "#     dev_Data_list = json.loads(line)\n",
        "#     df = pd.DataFrame.from_dict(dev_Data_list)\n",
        "#     df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "#     df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "#     df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "\n",
        "#     # replies_list,len_reply_list = get_replies(df)\n",
        "#     replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "#     replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "#     # print(len(replies_list_2d))\n",
        "#     replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "#     # print(len(replies_list_1d))\n",
        "#     reply_length = len(replies_list_1d)\n",
        "#     list_replies.append(reply_length) \n",
        "#     dict_replies_list.append(dict_replies)\n",
        "\n",
        "#     chunks.append(df)\n",
        "#     num_replies.append(len_reply_list)\n",
        "#     list_of_rlists.append(replies_list)\n",
        "#   dfs_dev = pd.concat(chunks)\n",
        "# dev_data_file.close()\n",
        "# #print(dfs_dev)\n",
        "\n",
        "# with open(\"dev.label.json\") as dev_label_file:\n",
        "#     dev_label_json_file = json.load(dev_label_file)\n",
        "#     df_dev_label = pd.DataFrame(list(dev_label_json_file.items()),columns = ['id', 'label'])\n",
        "# dev_label_file.close()\n",
        "\n",
        "# #print(df_train_label)\n",
        "# #print(df_train_label.groupby('label').size())\n",
        "# # non - rumour\n",
        "# # 3058\n",
        "# # rumour\n",
        "# # 1583\n",
        "\n",
        "# #check the id and if it is present in rumours/non-rumours labels, then add it to the row of that dataframe (check from the labels dataframe)\n",
        "# df_final1_dev = pd.merge(dfs_dev, df_dev_label, how='left', left_on='id', right_on='id')\n",
        "# df_final2_dev = pd.merge(dfs_dev, df_dev_label, how='inner', left_on='id', right_on='id')\n",
        "\n",
        "# # print(df_final1_dev)\n",
        "# print(df_final1_dev.shape)\n",
        "# # print(df_final2_dev)\n",
        "# print(df_final2_dev.shape)\n",
        "\n",
        "# reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "# sorted_replies = []\n",
        "# for i in range(len(reply_dict_list)):\n",
        "#   values_2d = reply_dict_list[i].values()\n",
        "#   values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "#   sorted_replies.append(values_list_1d)\n",
        "\n",
        "# #only text and label - drop columns from old one and store in new one \n",
        "# Corpus_dev = df_final2_dev.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "# le = preprocessing.LabelEncoder()\n",
        "# Corpus_dev['label'] = le.fit_transform(Corpus_dev['label'])\n",
        "# Corpus_dev[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "# Corpus_dev[\"num_replies\"] = num_replies\n",
        "# Corpus_dev[\"all_replies_by_time\"] = sorted_replies\n",
        "\n",
        "# print(Corpus_dev.head(10))\n",
        "# print(Corpus_dev.shape)\n",
        "# print(Corpus_dev.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10546, 8)\n",
            "(580, 8)\n",
            "The length of sorted list is =  580\n",
            "                   id                                              tweet                      created_at  label                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      1  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "1  524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014      0  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "2  553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015      0  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "3  580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015      1  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "4  544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014      1  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "5  499363921661140993  Mound City Bar Association has agreed to donat...  Wed Aug 13 01:16:51 +0000 2014      0  [@Nettaaaaaaaa why would a witness need an att...            3  [@Nettaaaaaaaa why would a witness need an att...\n",
            "6  500177556768034816  Thousands of New Yorkers shut down Times Squar...  Fri Aug 15 07:09:57 +0000 2014      0  [@OccupyLondon all of em white to, @allys_sons...           42  [@OccupyLondon all of em white to, @allys_sons...\n",
            "7  553191800408911872  Mosques attacked in France following 'Charlie ...  Thu Jan 08 14:09:37 +0000 2015      0  [The idiots are at it again. “@TIME: Mosques a...           22  [The idiots are at it again. “@TIME: Mosques a...\n",
            "8  524927281048080385  Snipers set up on National Art Gallery as we r...  Wed Oct 22 14:16:31 +0000 2014      1  [@dmatthewmillar @Russell_Barth @devo1400 @spa...           42  [@dmatthewmillar @Russell_Barth @devo1400 @spa...\n",
            "9  544308793623207936  Remember, Sydney terrorists have nothing to do...  Mon Dec 15 01:51:43 +0000 2014      0  [@alanBStardmp @bobbington99 Really? Let's ban...           48  [@alanBStardmp @bobbington99 Really? Let's ban...\n",
            "(580, 7)\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "label                   int64\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOFL_hf0Z8vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0d5788-2b9f-4ae7-a9e6-b1aeb62d32af"
      },
      "source": [
        "with open(\"test.data.jsonl\", 'r', encoding='utf-8') as test_data_file:\n",
        "  list_of_rlists = []\n",
        "  chunks = []\n",
        "  num_replies = []\n",
        "  dict_replies= {}\n",
        "  list_replies = []\n",
        "  dict_replies_list = []\n",
        "\n",
        "  for line in test_data_file:\n",
        "    train_Data_list = json.loads(line)\n",
        "    df = pd.DataFrame.from_dict(train_Data_list)\n",
        "    df = df[[\"id_str\",\"in_reply_to_status_id_str\",\"text\", \"user\",\"favorited\",\"in_reply_to_user_id_str\",\"created_at\"]]\n",
        "    df.rename(columns={'id_str': 'id'}, inplace=True)\n",
        "    df.rename(columns={'text': 'tweet'}, inplace=True)\n",
        "    # replies_list,len_reply_list = get_replies(df)\n",
        "    \n",
        "    # replies_list,len_reply_list = get_replies(df)\n",
        "    replies_list,len_reply_list,dict_replies = get_replies(df) #reply list for a single source tweet\n",
        "    replies_list_2d = list(dict_replies.values()) #get all the replies for a single source tweet\n",
        "    # print(len(replies_list_2d))\n",
        "    replies_list_1d = [j for sub in replies_list_2d for j in sub]\n",
        "    # print(len(replies_list_1d))\n",
        "    reply_length = len(replies_list_1d)\n",
        "    list_replies.append(reply_length) \n",
        "    dict_replies_list.append(dict_replies)\n",
        "\n",
        "    chunks.append(df)\n",
        "    num_replies.append(len_reply_list)\n",
        "    list_of_rlists.append(replies_list)\n",
        "  dfs1 = pd.concat(chunks)\n",
        "test_data_file.close()\n",
        "#print(dfs1)\n",
        "finalll = dfs1[dfs1['in_reply_to_status_id_str'].isna()] #source tweets\n",
        "# print(finalll)\n",
        "\n",
        "# print(dfs1[\"in_reply_to_status_id_str\"].isnull().sum())\n",
        "# c=0\n",
        "# if (dfs1[\"in_reply_to_status_id_str\"].isnull()).all():\n",
        "#     c = c+1\n",
        "# print(c)\n",
        "# cli = np.where(dfs1[\"in_reply_to_status_id_str\"].isnull())\n",
        "# print(cli)\n",
        "reply_dict_list = get_sorted_by_time(dict_replies_list) #list of dicts \n",
        "sorted_replies = []\n",
        "for i in range(len(reply_dict_list)):\n",
        "  values_2d = reply_dict_list[i].values()\n",
        "  values_list_1d = [j for sub in values_2d for j in sub ]\n",
        "  sorted_replies.append(values_list_1d)\n",
        "\n",
        "#only text - drop columns from old one and store in new one \n",
        "Corpus_test = finalll.drop([\"in_reply_to_status_id_str\",\"user\",\"favorited\",\"in_reply_to_user_id_str\"], axis=1)\n",
        "Corpus_test[\"all_replies\"] = list_of_rlists #has a list of replies \n",
        "Corpus_test[\"num_replies\"] = num_replies\n",
        "Corpus_test[\"all_replies_by_time\"] = sorted_replies\n",
        "\n",
        "print(Corpus_test.head(10))\n",
        "print(Corpus_test.shape)\n",
        "print(Corpus_test.dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The length of sorted list is =  581\n",
            "                   id                                              tweet                      created_at                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  544382249178001408  5 people have been able to get out of Sydney c...  Mon Dec 15 06:43:36 +0000 2014  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...           16  [@ABC @SSLATERBOARDS fucking terrorists, @ABC ...\n",
            "0  525027317551079424  NEW: Sources: Deceased gunman who killed soldi...  Wed Oct 22 20:54:01 +0000 2014  [@albanyly @ABC @theviewtv @WorldNews piss off...           11  [@albanyly @ABC @theviewtv @WorldNews piss off...\n",
            "0  544273220128739329  ISIS FLAG VISIBLE AS GUNMAN SEIZES SYDNEY CAFE...  Sun Dec 14 23:30:22 +0000 2014  [@rConflictNews @PzFeed Its not ISIS flag its ...           39  [@rConflictNews @PzFeed Its not ISIS flag its ...\n",
            "0  499571799764770816  People of #Ferguson: Stop #attacking our brave...  Wed Aug 13 15:02:53 +0000 2014  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...           34  [@NuaEabhrac @GOPTeens yes, @keirhaug haha it'...\n",
            "0  552844104418091008  #CharlieHebdo editor, assassinated today, said...  Wed Jan 07 15:08:00 +0000 2015  [@PeterPannier I'd be curious to know more abo...           46  [@PeterPannier I'd be curious to know more abo...\n",
            "0  524977651476623360  Soldier shot at War Memorial in Ottawa has die...  Wed Oct 22 17:36:40 +0000 2014  [Very sad news “@globeandmail: Soldier shot at...           15  [Very sad news “@globeandmail: Soldier shot at...\n",
            "0  544514988078280704  Gunman in #Sydneysiege identified as Muslim cl...  Mon Dec 15 15:31:04 +0000 2014  [@WSJ who believes he was a lonely wolf?, .@mk...           23  [@WSJ who believes he was a lonely wolf?, .@mk...\n",
            "0  524928863714168832  BREAKING: Injury reported after shooting at Ca...  Wed Oct 22 14:22:48 +0000 2014  [“@cnni: BREAKING: Injury reported after shoot...            3  [“@cnni: BREAKING: Injury reported after shoot...\n",
            "0  544390718253699072  LIVE: Updates on #SydneySiege via BBC http://t...  Mon Dec 15 07:17:16 +0000 2014  [@BBCOS @BBCWorld OMG.., @bbcos EUA ATACA BRAS...           10  [@BBCOS @BBCWorld OMG.., @bbcos EUA ATACA BRAS...\n",
            "0  580322349569994752  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:44 +0000 2015                            [@WSJeurope @rezel RIP]            1                            [@WSJeurope @rezel RIP]\n",
            "(581, 6)\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp-vfxKZvl6M"
      },
      "source": [
        "We will randomly split the entire training data into two sets: a train set with 90% of the data and a validation set with 10% of the data. We will perform hyperparameter tuning using cross-validation on the train set and use the validation set to compare models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4HKAFTbvMwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934dd455-5163-4e71-8ee6-f0be4a513ca0"
      },
      "source": [
        "replies_train = [' [SEP] '.join([str(tweet) for tweet in all_replies]) for all_replies in Corpus.all_replies_by_time.values]\n",
        "replies_dev = ['[SEP] '.join([str(tweet) for tweet in all_replies]) for all_replies in Corpus_dev.all_replies_by_time.values]\n",
        "replies_test = ['[SEP] '.join([str(tweet) for tweet in all_replies]) for all_replies in Corpus_test.all_replies_by_time.values]\n",
        "# X = Corpus.tweet.values \n",
        "X = Corpus.tweet.values + replies_train\n",
        "y = Corpus.label.values\n",
        "# print(Corpus.all_replies.values)\n",
        "\n",
        "# print(len(replies))\n",
        "# count=0\n",
        "# for i in Corpus.all_replies.values:\n",
        "#   # print(' '.join([str(elem) for elem in i]))\n",
        "#   list_to_str_reply = ' '.join([str(elem) for elem in i])\n",
        "#   count = count + 1\n",
        "# print(count, len(list_to_str_reply)) #4641\n",
        "# X_train = Corpus.tweet.values\n",
        "# X_val = Corpus_dev.tweet.values \n",
        "# input = f\"[CLS] {Source_tweet} [SEP] \" + ''.join([f\"{reply} [SEP]\" for reply in all_replies])\n",
        "\n",
        "X_train = list(zip(Corpus.tweet.values, replies_train))\n",
        "# X_train = Corpus.tweet.values + replies_train\n",
        "X_val = list(zip(Corpus_dev.tweet.values, replies_dev))\n",
        "y_train = Corpus.label.values\n",
        "y_val = Corpus_dev.label.values\n",
        "X_test = list(zip(Corpus_test.tweet.values + replies_test))\n",
        "print(len(X_train),len(X_val),len(X_test))\n",
        "print(type(X_train))\n",
        "for i in range(10):\n",
        "  print(X_train[i],\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4641 580 581\n",
            "<class 'list'>\n",
            "('How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j', \"@Heresy_Corner @KrustyAllslopp \\nJews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. [SEP] @Heresy_Corner @KrustyAllslopp \\nNo one does. [SEP] @Heresy_Corner #ImCharlieHebdo [SEP] @KrustyAllslopp Ditto [SEP] @Grizzly_Stats @tom_wein What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. [SEP] @Heresy_Corner @KrustyAllslopp \\nYes, until it becomes yours. [SEP] @Heresy_Corner @KrustyAllslopp \\nWhy insult people who have nothing to do with this? People are genuinely offended by such drawings. [SEP] @KrustyAllslopp @Heresy_Corner \\nAnd neither am I! I think this has little to do with actual Muslims. [SEP] @berg_han Ah, you don't like Jews. Bye bye. @KrustyAllslopp [SEP] @Heresy_Corner Also they kid you along with benign stuff then ... WHAM it's like a river of shite! [SEP] @berg_han @Heresy_Corner It's a good point [SEP] @Heresy_Corner @pjfny How about this? http://t.co/d2qcaVkf2h [SEP] @Heresy_Corner @KrustyAllslopp \\nOrganised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. [SEP] @theedwardian81 @Heresy_Corner ...and this: http://t.co/LmYxpmzw3v [SEP] @Heresy_Corner @berg_han explored. [SEP] @berg_han @KrustyAllslopp And if that's the case, that is your problem. [SEP] @Heresy_Corner @tom_wein No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it [SEP] @Heresy_Corner Oh dear... Just saw those tweets... Blocked him. [SEP] @berg_han @KrustyAllslopp Because they have to learn to not be offended, that's why. [SEP] @Heresy_Corner @berg_han But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either [SEP] @Heresy_Corner @berg_han I get that ... I defend the right to free speech however there is a much broader context to this which is not [SEP] @Heresy_Corner @berg_han just for the record. I am not in any way, shape or form defending this atrocity. [SEP] @KrustyAllslopp @berg_han Yes, remind me when was the last time Jews bombed the Guardian. [SEP] @Heresy_Corner I know!  Gives me the creeps. [SEP] @Heresy_Corner There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( [SEP] @Heresy_Corner @KrustyAllslopp \\nIf people insult something that's important to you, you feel that your identity is under attack. [SEP] @KrustyAllslopp It's remarkable how quickly they come out the woodwork. [SEP] @Heresy_Corner @EdzardErnst Why is the correct response to brutality to offend lots of people who *don't* support that brutality?\") \n",
            "\n",
            "(\"You can't condemn an entire race, nation or religion based on the actions of a few radicals, please keep that in mind #sydneysiege\", '@LozzaCopland How many times are we going to have the \"Don\\'t judge Islam by this act of terrorism\" conversation? #sydneysiege [SEP] @LozzaCopland indeed not a question of Race or nation. It is this religion og Islam that creates Zombies. [SEP] @LozzaCopland Only very few people can maintain the perspective that you have in this hour of crisis. [SEP] @LozzaCopland .....forthcoming from ppl when all Christians are branded as pedophiles or \"just after your money\" etc. But apparently in.... [SEP] @LozzaCopland at the response to child sex abuse cases, and ppl slamming Hillsong etc. A full throated defence for these is not often..... [SEP] @LozzaCopland How odd its not an athiest holding hostages? I condemn all religious nuts for as long as they cause pain and suffering... [SEP] @LozzaCopland love the first 4 words in your bio 🙈🙉🙊 [SEP] .@lozzacopland Such wise bull Shit. My cat said as much. This is a real world oh wise one. Nazis were only doing jobs given them. History! [SEP] @LozzaCopland Actually I have, I condemn Islam and I also condemn you, for talking total crap #LeftistLuzer #sydneysiege [SEP] @LozzaCopland What\\'s you definition of a few?  Al Qaeda, al Nusra, ISIS, Iranian Ayatollahs, Boko Haram ... there are many more... [SEP] @LozzaCopland @mermayden you can blame their ideology. [SEP] @LozzaCopland quite the opposite. I\\'m saying that churches/Christianty in general are frequently slandered due to the actions of a few. Look [SEP] @LozzaCopland ....future that will change? [SEP] @gibosity Are you saying you condemn all Christians due to those horrible events? Shall we ban crosses, priest robes &amp; large church groups? [SEP] @LozzaCopland ...is VASTLY different [SEP] @LozzaCopland This is so right. People need to realize it\\'s not a whole. It\\'s a few. [SEP] @LozzaCopland We can condemn an ideology on the basis of the merits &amp; consequences of its ideas. Islam, like all religions, is an ideology. [SEP] @LozzaCopland I look forward to you jumping to the defence of Christianity when the next child abuse case is aired [SEP] @rogerlsimon 1.6 billion muslims worldwide, thats a little over a fifth of the world\\'s population, I think the word \"a few\" is appropriate [SEP] What if the entire race and religion has a majority of radicals that the minority don\\'t have the balls to condemn? #Islam @LozzaCopland [SEP] @LozzaCopland media is definitely to blame. But the reaction to this, compared to when an individual Christian makes the church look bad.... [SEP] @LozzaCopland said the well meaning naif. http://t.co/VWONoSuegw [SEP] @LozzaCopland Godwin - so quit condemining all Nazis /Godwin.  Islam is an IDEOLOGY, and therefore has direct influence on behavior. [SEP] @LozzaCopland True [SEP] @LozzaCopland may not be the majority..but wow, there is sure a lot of them, aren\\'t there ? [SEP] @gibosity I don\\'t think it helps when news is focused primarily on negative/fear inducing. People like to blame region not the individuals. [SEP] @LozzaCopland \\n\\nDefine \"few\" [SEP] @gibosity I agree there is a huge difference but I think it\\'s due to a lack of understanding/frequent media reports/mass hysteria &amp; fear [SEP] @LozzaCopland you can\\'t terrorize the entire country based on the actions of few .....keep in mind.....#sydneysiege') \n",
            "\n",
            "('Attempts to extend blame for this to all Muslims should be treated with the same disgust as attempts to justify the attacks. #CharlieHebdo', '@iyad_elbaghdadi @Axxeen when will we see mass protests from \\'moderate\\' Muslims against these killings? I think highly unlikely [SEP] @iyad_elbaghdadi @chris15474 Most people are blaming mad extremists [SEP] @iyad_elbaghdadi Amen. [SEP] @iyad_elbaghdadi it is crazy. Video footage will back such claims... [SEP] @iyad_elbaghdadi @ParidePalumbo Muslims are good people and they should find, fight and condemn killers. [SEP] @iyad_elbaghdadi @jgriffiths Of course. But the solution of doing nothing isn\\'t working. [SEP] @iyad_elbaghdadi Not sure about that anymore...there is something about the religion that is driving people towards terrorism [SEP] @iyad_elbaghdadi There must be muslims knowing these criminals. Will they talk? [SEP] @iyad_elbaghdadi @rickardc87 My religion was no different  in 11 century  but its evolved  in the 21 Century IRA relised dialogue wins peace [SEP] @iyad_elbaghdadi @edzsplace then every muslim needs to stand up and say that and stop being silent #nonmuslimlivesmatter [SEP] @iyad_elbaghdadi Then I can only suggest that condemnation comes from within your religion and is denounced more loudly than current efforts [SEP] @iyad_elbaghdadi @MattGoldstein26 Can we at least assume that these murderers demand and expect the support of all Muslims? [SEP] .@iyad_elbaghdadi you are not serious? Who else to blame? so called Islamic renewal on the hand of \"moderates\" radicalized  the masses! [SEP] @iyad_elbaghdadi @acarvin It\\'s a good point. But I\\'m not sure I would TOTALLY equate hatred with actual physical violence. [SEP] @iyad_elbaghdadi Without your own mechanism to control this, you will be seen as complicit. No one cares what a nice guy you are.') \n",
            "\n",
            "('Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/YzLXYX5JJt http://t.co/8F0qAcj9sg', '@GigiGraciette hot guy [SEP] @Corey_Frizzell hey frizz I was right downtown…crazy stuff [SEP] So sad. \"@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/YBIYHliJpy http://t.co/MjvFKwrD4W” [SEP] @GigiGraciette So sad, RIP Cpl. Nathan Cirillo and God Bless your family left behind. [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/c5HtPz4n6l http://t.co/VBGdDYutSO” [SEP] So sad \"@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/p6xCfWMgg1 http://t.co/gI92I5vhoB” [SEP] @PEItopcop Were u? That\\'s crazy! Were u in uni? The Sgt. Arms is a hero: he needs a medal. [SEP] 🙏😢 #Hero “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/eRzccSD7aE http://t.co/bOWMxA2WEQ” [SEP] @GigiGraciette HOW TOTALLY. SENSELESS. AND SHAME. IM SO SORRY OUT TO THE FAMLIE.  :( [SEP] @GigiGraciette WhatAreTheValuesOfCanadians?LookAtThisManHere\\nSincereCondolences\\nToTheFamilyFriends&amp;ColleaguesOfCplNathanCIRILLO\\nWeAreSoSorry [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed 2day in #OttawaShooting\\nhttp://t.co/7jtcHQr7wc http://t.co/t5ZYaeQO22” God bless [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/x4c34sMhFs http://t.co/YcTgpHSCbV” [SEP] “@GigiGraciette: Rest in Peace, Cpl. Nathan Cirillo. Killed today in #OttawaShooting\\nhttp://t.co/EoWtbw9HJk http://t.co/goYw4BBRoj') \n",
            "\n",
            "(\"People DEBATING whether #MikeBrown shoplifted or not-- IT DOESN'T MATTER.\\nShoplifting isn't punishable by DEATH IN THE STREET. #Ferguson\", \"@kurbster3 @VABVOX @commonman80 yeah and shooting him 10 times while not even knowing if he did commit a crime is right..makes so much sense [SEP] @commonman80 I was just responding to all the people saying he'd shoplifted. It wouldn't matter if he had. You don't shoot unarmed kids. [SEP] @VABVOX \\nReports are saying HE STOLE NOTHING. Reports say, He was walking in the street with friends and the cop got pissed... [SEP] @__aviana Racists need to believe #MikeBrown was a criminal. They criminalize all young black men. He was UNARMED.@kurbster3 @commonman80 [SEP] @kurbster3 Legally it is. Now stop tweeting your racist excuses at me, please. @__aviana @commonman80 [SEP] @kurbster3 Says the dude with Hitler on his profile page. @__aviana @commonman80 [SEP] @commonman80 Me, too. It's so vile. Someone just made a joke to me about #MikeBrown. No empathy at all. @__aviana [SEP] @__aviana There's no arguing w/racists.Their minds are made up.They see all blacks as criminals.Even kids #MikeBrown @kurbster3 @commonman80 [SEP] @kurbster3 legally yes tf he is. [SEP] @VABVOX @kurbster3 @__aviana I already BLOCKED the Dumb Ass... [SEP] @kurbster3 @VABVOX @commonman80 reports say his hands were up and he was walking AWAY when he was shot 10 times [SEP] @kurbster3 @VABVOX @commonman80 like always blaming the victim and not holding the officer accountable for being unreasonable\") \n",
            "\n",
            "('Update - PA: gunman holding hostages in #Paris grocery has been killed', '@SkyNewsBreak Wish we\\'d been as decisive with the Lee Rigby killers [SEP] @SkyNewsBreak rot in hell! [SEP] “@SkyNewsBreak: Update - PA: gunman holding hostages in #Paris grocery has been killed” 🙏🙏 [SEP] @SkyNewsBreak @SteeryBlade GOOD 😃 [SEP] @SkyNewsBreak surely you mean Islamist terrorist not gunman !! [SEP] @SkyNewsBreak good shoot them all #Paris will mourn forever [SEP] @SkyNewsBreak Any news on the woman hostage taker? [SEP] @SkyNewsBreak thank fuck for that! [SEP] @SkyNewsBreak @SteeryBlade GET IN! Fucking lunatic prick [SEP] “@SkyNewsBreak: Update - PA: gunman holding hostages in #Paris grocery has been killed” #TangoDown [SEP] @maisy59 @SkyNewsBreak \"them all\" there was two.... [SEP] @SkyNewsBreak Amen to that [SEP] @SkyNewsBreak 👏👍 well done 🇫🇷best way to deal with the scum [SEP] Hope he fucking rots in hell. @SkyNewsBreak [SEP] @SkyNewsBreak good. [SEP] @SkyNewsBreak great result ,,,,no messing french love em , we would still be fucking around if happened here , PC begrade  rule the uk [SEP] @SkyNewsBreak ...... I hope it was VERY painful . Rot in hell. [SEP] @SkyNewsBreak @cumbermon good! hope he/she rots in hell,sending thoughts to the victims and hostages at this awful time. [SEP] @SkyNewsBreak Evil Bastards!! [SEP] .@SkyNewsBreak any news on the hostages or what? Dont care about the terrorists') \n",
            "\n",
            "(\"Here's a recap of the key points so far in the #GermanWings Alps plane crash. Live updates: http://t.co/8UPMsinQkX http://t.co/hNjZvcruqq\", '.@SkyNews Correction:- \"Germanwings\" is the airline name. [SEP] “@SkyNews: Here\\'s a recap of the key points so far in the #GermanWings Alps plane crash. http://t.co/avlzDaVkAs” so so sad :( [SEP] @SkyNews so many people are making planes crash. Some people are so selfish [SEP] @SkyNews terrible :(') \n",
            "\n",
            "('#Ferguson protestors have shut down a major intersection  https://t.co/2sLJdYKrP4', \"“@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/z2WvBI8C94” [SEP] @Dreamdefenders @NoWayNRA1 now? [SEP] “@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/7yNe8LkWFO” [SEP] @DrDoreenDupont @Dreamdefenders reported about 2 hours ago. Don't know if still going on now. [SEP] NO PEACE RT @Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/rZ4LwCOgss [SEP] @Dreamdefenders Wow..yeah., its gonna be a looong week for sure. Folks are sick and tired of being SICK and tired.. [SEP] I would've Tony Stewart all they ass “@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/BA9wSzoC9u” [SEP] @NoWayNRA1 @Dreamdefenders thanks. All police should wear cameras. That would save so much grief [SEP] @Dreamdefenders protests arent gonna do shit, we need to revolutionize Its the only way something will change. Protests only raise awareness [SEP] @Dreamdefenders @NoWayNRA1 the power of the people !!!! [SEP] “@Dreamdefenders: #Ferguson protestors have shut down a major intersection  https://t.co/PhuqegZHBR”\") \n",
            "\n",
            "('#BREAKING: Police have confirmed Sydney hostage taking is over. #Sydneysiege', '@CTVNews @JenniDavis91 It\\'s not over.  As long as Islam remains unfettered it\\'s not over by a long shot. [SEP] @CTVNews [SEP] @Harry_Styles “@CTVNews: #BREAKING: Police have confirmed Sydney hostage taking is over. #Sydneysiege” [SEP] Good \"@CTVNews: #BREAKING: Police have confirmed Sydney hostage taking is over. #Sydneysiege”') \n",
            "\n",
            "('France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/dWlaFSDKjL', \"@AP  horrible 😭 [SEP] @AP Haven't we had it with religion already? Enough is enough. Stop killing over dueling mythologies, and stop ignoring the cause #religion [SEP] “@AP: France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/bffFX60c0s” @LpHarvin [SEP] @AP \\nthe majority of Muslims have no desire to ostracize the cancer that is eating away at Islam... the world needs to do that [SEP] oh my... @AP RT: France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/RDrHnGu8SN [SEP] &gt; RT “@AP: France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/KB7OQuiUXA” [SEP] @AP 6 hostages, @FRANCE24 said. [SEP] @AP France faces 2 hostage-taking attacks; Paris kosher market attack has 5 hostages, some wounded: http://t.co/OMI0El49pD\") \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRRPzuWaasbR"
      },
      "source": [
        "print(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pErITNxtyNpe"
      },
      "source": [
        "### 2.3. Load Test Data\n",
        "The test data contains 4555 examples with no label. About 300 examples are non-complaining tweets. Our task is to identify their `id` and examine manually whether our results are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "RB8DrPbfa-G1",
        "outputId": "6721c67a-ed3a-45cd-9163-bd271e518d4d"
      },
      "source": [
        "# Load test data\n",
        "test_data = Corpus_test\n",
        "# Display 5 samples from the test data\n",
        "test_data.sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>created_at</th>\n",
              "      <th>all_replies</th>\n",
              "      <th>num_replies</th>\n",
              "      <th>all_replies_by_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>552825473160736768</td>\n",
              "      <td>As a Muslim, I strongly condemn this brutal te...</td>\n",
              "      <td>Wed Jan 07 13:53:58 +0000 2015</td>\n",
              "      <td>[@SonVesayet @gulkork22 you don't need to be m...</td>\n",
              "      <td>1</td>\n",
              "      <td>[@SonVesayet @gulkork22 you don't need to be m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>580347361039413248</td>\n",
              "      <td>Airline #Germanwings says 144 passengers and s...</td>\n",
              "      <td>Tue Mar 24 12:36:07 +0000 2015</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>499703651079045121</td>\n",
              "      <td>This. RT @grasswire: State Senator asks the #F...</td>\n",
              "      <td>Wed Aug 13 23:46:49 +0000 2014</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>524979179235069952</td>\n",
              "      <td>Remembrance Day ceremony in Ottawa is emotiona...</td>\n",
              "      <td>Wed Oct 22 17:42:44 +0000 2014</td>\n",
              "      <td>[@fs1otoole if they won't be all there, the en...</td>\n",
              "      <td>13</td>\n",
              "      <td>[@fs1otoole if they won't be all there, the en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>524966596897685504</td>\n",
              "      <td>Stunning photos from Ottawa http://t.co/xQTgZ8...</td>\n",
              "      <td>Wed Oct 22 16:52:44 +0000 2014</td>\n",
              "      <td>[@HuffPostCanada  so sorry they now have \"work...</td>\n",
              "      <td>7</td>\n",
              "      <td>[@HuffPostCanada  so sorry they now have \"work...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id                                              tweet                      created_at                                        all_replies  num_replies                                all_replies_by_time\n",
              "0  552825473160736768  As a Muslim, I strongly condemn this brutal te...  Wed Jan 07 13:53:58 +0000 2015  [@SonVesayet @gulkork22 you don't need to be m...            1  [@SonVesayet @gulkork22 you don't need to be m...\n",
              "0  580347361039413248  Airline #Germanwings says 144 passengers and s...  Tue Mar 24 12:36:07 +0000 2015                                                 []            0                                                 []\n",
              "0  499703651079045121  This. RT @grasswire: State Senator asks the #F...  Wed Aug 13 23:46:49 +0000 2014                                                 []            0                                                 []\n",
              "0  524979179235069952  Remembrance Day ceremony in Ottawa is emotiona...  Wed Oct 22 17:42:44 +0000 2014  [@fs1otoole if they won't be all there, the en...           13  [@fs1otoole if they won't be all there, the en...\n",
              "0  524966596897685504  Stunning photos from Ottawa http://t.co/xQTgZ8...  Wed Oct 22 16:52:44 +0000 2014  [@HuffPostCanada  so sorry they now have \"work...            7  [@HuffPostCanada  so sorry they now have \"work..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79dYY3sxDCi"
      },
      "source": [
        "## 3. Set up GPU for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi1CoEOL1puh"
      },
      "source": [
        "Google Colab offers free GPUs and TPUs. Since we'll be training a large neural network it's best to utilize these features.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "`Runtime -> Change runtime type -> Hardware accelerator: GPU`\n",
        "\n",
        "Then we need to run the following cell to specify the GPU as the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7hxtI4l0SUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d685474d-70ad-4320-9340-5d52ddaf8c77"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j6EbXLs12Kz"
      },
      "source": [
        "# C - Baseline: TF-IDF + Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eWj6qFpA3TE"
      },
      "source": [
        "In this baseline approach, first we will use TF-IDF to vectorize our text data. Then we will use the Naive Bayes model as our classifier.\n",
        "\n",
        "Why Naive Bayse? I have experiemented different machine learning algorithms including Random Forest, Support Vectors Machine, XGBoost and observed that Naive Bayes yields the best performance. In [Scikit-learn's guide](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) to choose the right estimator, it is also suggested that Naive Bayes should be used for text data. I also tried using SVD to reduce dimensionality; however, it did not yield a better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeljUmsqAUpt"
      },
      "source": [
        "## 1. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU754-QPAwBt"
      },
      "source": [
        "### 1.1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_OzXFcfCBOa"
      },
      "source": [
        "In the bag-of-words model, a text is represented as the bag of its words, disregarding grammar and word order. Therefore, we will want to remove stop words, punctuations and characters that don't contribute much to the sentence's meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98rwWTSw_dEI"
      },
      "source": [
        "import nltk\n",
        "# Uncomment to download \"stopwords\"\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def text_preprocessing(s):\n",
        "    \"\"\"\n",
        "    - Lowercase the sentence\n",
        "    - Change \"'t\" to \"not\"\n",
        "    - Remove \"@name\"\n",
        "    - Isolate and remove punctuations except \"?\"\n",
        "    - Remove other special characters\n",
        "    - Remove stop words except \"not\" and \"can\"\n",
        "    - Remove trailing whitespace\n",
        "    \"\"\"\n",
        "    s = s.lower()\n",
        "    # Change 't to 'not'\n",
        "    s = re.sub(r\"\\'t\", \" not\", s)\n",
        "    # Remove @name\n",
        "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
        "    # Isolate and remove punctuations except '?'\n",
        "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
        "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
        "    # Remove some special characters\n",
        "    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
        "    # Remove stopwords except 'not' and 'can'\n",
        "    s = \" \".join([word for word in s.split()\n",
        "                  if word not in stopwords.words('english')\n",
        "                  or word in ['not', 'can']])\n",
        "    # Remove trailing whitespace\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    \n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8jpfxygCvww"
      },
      "source": [
        "### 1.2. TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbD689AMC-aB"
      },
      "source": [
        "In information retrieval, **TF-IDF**, short for **term frequency–inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We will use TF-IDF to vectorize our text data before feeding them to machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOQ3X7hPDYhn"
      },
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Preprocess text\n",
        "X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
        "X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
        "X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
        "\n",
        "# Calculate TF-IDF\n",
        "tf_idf = TfidfVectorizer(ngram_range=(1, 3), binary=True, smooth_idf=False) #initialise vectorisor\n",
        "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
        "X_val_tfidf = tf_idf.transform(X_val_preprocessed)\n",
        "X_test_tfidf = tf_idf.transform(X_test_preprocessed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ku1zIjIGVwR3"
      },
      "source": [
        "# print(tf_idf.get_feature_names())\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_val_tfidf.shape)\n",
        "print(X_test_tfidf.shape)\n",
        "# print(X_val_tfidf)\n",
        "print(\"Sparse Matrix form of test data : \\n\")\n",
        "X_val_tfidf.todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arsEHOKzFxdv"
      },
      "source": [
        "## 2. Train Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63HQtpzOInq-"
      },
      "source": [
        "### 2.1. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5E0Fa6GUyd"
      },
      "source": [
        "We will use cross-validation and AUC score to tune hyperparameters of our model. The function `get_auc_CV` will return the average AUC score from cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueXJsrhNGqlS"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "def get_auc_CV(model):\n",
        "    \"\"\"\n",
        "    Return the average AUC score from cross-validation.\n",
        "    \"\"\"\n",
        "    # Set KFold to shuffle data before the split\n",
        "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
        "\n",
        "    # Get AUC scores\n",
        "    auc = cross_val_score(\n",
        "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
        "\n",
        "    return auc.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53MgflYPHNxh"
      },
      "source": [
        "The `MultinominalNB` class only have one hypterparameter - **alpha**. The code below will help us find the alpha value that gives us the highest CV AUC score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKatLhhJGzn0"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
        "                 for i in np.arange(1, 10, 0.1)],\n",
        "                index=np.arange(1, 10, 0.1))\n",
        "\n",
        "best_alpha = np.round(res.idxmax(), 2)\n",
        "print('Best alpha: ', best_alpha)\n",
        "\n",
        "plt.plot(res)\n",
        "plt.title('AUC vs. Alpha')\n",
        "plt.xlabel('Alpha')\n",
        "plt.ylabel('AUC')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaATcy1nIeE9"
      },
      "source": [
        "### 2.2. Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne-eoqM4Muna"
      },
      "source": [
        "To evaluate the performance of our model, we will calculate the accuracy rate and the AUC score of our model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2gb-9mJK2w"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "    # print(\"preds\", preds)\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    # print(fpr, tpr, threshold)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    \n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnCfezJSM-41"
      },
      "source": [
        "By combining TF-IDF and the Naive Bayes algorithm, we achieve the accuracy rate of **72.65%** on the validation set. This value is the baseline performance and will be used to evaluate the performance of our fine-tune BERT model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwVGbLHLIwpl"
      },
      "source": [
        "# Compute predicted probabilities\n",
        "nb_model = MultinomialNB(alpha=1.8)\n",
        "nb_model.fit(X_train_tfidf, y_train)\n",
        "#Predict the response for test dataset\n",
        "probs = nb_model.predict_proba(X_val_tfidf)\n",
        "print(\"PROBS=\",probs)\n",
        "pred = nb_model.predict(X_val_tfidf)\n",
        "print(pred)\n",
        "# Evaluate the classifier\n",
        "evaluate_roc(probs, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1ur9cjdiMr"
      },
      "source": [
        "pred = nb_model.predict(X_test_tfidf)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nt25QAXdiKm"
      },
      "source": [
        "y_predicted = nb_model.predict(X_test_tfidf)\n",
        "y_predicted_labels = le.inverse_transform(y_predicted) \n",
        "print(y_predicted,y_predicted_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2qN00wBGkE6"
      },
      "source": [
        "y_predicted = nb_model.predict(X_val_tfidf)\n",
        "y_predicted_labels = le.inverse_transform(y_predicted) \n",
        "# print(y_predicted,y_predicted_labels)\n",
        "print(len(y_predicted_labels))\n",
        "unique, counts = np.unique(y_predicted, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GR2kFSsXjbL"
      },
      "source": [
        "print(df_dev_label.groupby('label').size())\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "fig.suptitle(\"label\", fontsize=12)\n",
        "Corpus_dev[\"label\"].reset_index().groupby(\"label\").count().sort_values(by= \n",
        "       \"index\").plot(kind=\"barh\", legend=False, \n",
        "        ax=ax).grid(axis='x')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0dlIEiYdiGV"
      },
      "source": [
        "Corpus_pred = Corpus_dev\n",
        "print(Corpus_pred.head())\n",
        "Corpus_pred['label'] = y_predicted_labels\n",
        "print(Corpus_pred.head())\n",
        "Corpus_pred.set_index('id',inplace=True)\n",
        "dictt = Corpus_pred.to_dict()['label']\n",
        "print(dictt)\n",
        "with open(\"dev.baseline1.json\", \"w\") as outfile: \n",
        "    json.dump(dictt, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K847SfzNdh-k"
      },
      "source": [
        "print(Corpus_pred.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEPPYHa62JXF"
      },
      "source": [
        "# D - Fine-tuning BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYJRzWI73eBJ"
      },
      "source": [
        "## 1. Install the Hugging Face Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxv-EJ2j31Iv"
      },
      "source": [
        "The transformer library of Hugging Face contains PyTorch implementation of state-of-the-art NLP models including BERT (from Google), GPT (from OpenAI) ... and pre-trained model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFiv8WGl4p40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9745142-31be-47a6-fa50-6f2d11fede76"
      },
      "source": [
        "!pip install transformers==2.8.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.17.67)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.67 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.8.0) (1.20.67)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.67->boto3->transformers==2.8.0) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4sXctSh4sq0"
      },
      "source": [
        "## 2. Tokenization and Input Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygbZpK6qbIYE"
      },
      "source": [
        "Before tokenizing our text, we will perform some slight processing on our text including removing entity mentions (eg. @united) and some special character. The level of processing here is much less than in previous approachs because BERT was trained with the entire sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L_Rc7l4bgzJ"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text_ = [re.sub(r'(@.*?)[\\s]', ' ', i) for i in text]\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text_ = [re.sub(r'&amp;', '&', i) for i in text_]\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text_ = [re.sub(r'\\s+', ' ', i).strip() for i in text_]\n",
        "    text_ = ' [CLS] ' + text_[0]+ ' [SEP] ' + text_[1] + ' [SEP] '\n",
        "    return text_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyYmHR8McE0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d72993-3a8f-4fdc-aeb9-228c43fc6fa3"
      },
      "source": [
        "# Print sentence 0\n",
        "print('Original: ', X_train[1])\n",
        "# for i in range(10):\n",
        "#     print(X_train[i])\n",
        "print('Processed: ', text_preprocessing(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  (\"You can't condemn an entire race, nation or religion based on the actions of a few radicals, please keep that in mind #sydneysiege\", '@LozzaCopland How many times are we going to have the \"Don\\'t judge Islam by this act of terrorism\" conversation? #sydneysiege [SEP] @LozzaCopland indeed not a question of Race or nation. It is this religion og Islam that creates Zombies. [SEP] @LozzaCopland Only very few people can maintain the perspective that you have in this hour of crisis. [SEP] @LozzaCopland .....forthcoming from ppl when all Christians are branded as pedophiles or \"just after your money\" etc. But apparently in.... [SEP] @LozzaCopland at the response to child sex abuse cases, and ppl slamming Hillsong etc. A full throated defence for these is not often..... [SEP] @LozzaCopland How odd its not an athiest holding hostages? I condemn all religious nuts for as long as they cause pain and suffering... [SEP] @LozzaCopland love the first 4 words in your bio 🙈🙉🙊 [SEP] .@lozzacopland Such wise bull Shit. My cat said as much. This is a real world oh wise one. Nazis were only doing jobs given them. History! [SEP] @LozzaCopland Actually I have, I condemn Islam and I also condemn you, for talking total crap #LeftistLuzer #sydneysiege [SEP] @LozzaCopland What\\'s you definition of a few?  Al Qaeda, al Nusra, ISIS, Iranian Ayatollahs, Boko Haram ... there are many more... [SEP] @LozzaCopland @mermayden you can blame their ideology. [SEP] @LozzaCopland quite the opposite. I\\'m saying that churches/Christianty in general are frequently slandered due to the actions of a few. Look [SEP] @LozzaCopland ....future that will change? [SEP] @gibosity Are you saying you condemn all Christians due to those horrible events? Shall we ban crosses, priest robes &amp; large church groups? [SEP] @LozzaCopland ...is VASTLY different [SEP] @LozzaCopland This is so right. People need to realize it\\'s not a whole. It\\'s a few. [SEP] @LozzaCopland We can condemn an ideology on the basis of the merits &amp; consequences of its ideas. Islam, like all religions, is an ideology. [SEP] @LozzaCopland I look forward to you jumping to the defence of Christianity when the next child abuse case is aired [SEP] @rogerlsimon 1.6 billion muslims worldwide, thats a little over a fifth of the world\\'s population, I think the word \"a few\" is appropriate [SEP] What if the entire race and religion has a majority of radicals that the minority don\\'t have the balls to condemn? #Islam @LozzaCopland [SEP] @LozzaCopland media is definitely to blame. But the reaction to this, compared to when an individual Christian makes the church look bad.... [SEP] @LozzaCopland said the well meaning naif. http://t.co/VWONoSuegw [SEP] @LozzaCopland Godwin - so quit condemining all Nazis /Godwin.  Islam is an IDEOLOGY, and therefore has direct influence on behavior. [SEP] @LozzaCopland True [SEP] @LozzaCopland may not be the majority..but wow, there is sure a lot of them, aren\\'t there ? [SEP] @gibosity I don\\'t think it helps when news is focused primarily on negative/fear inducing. People like to blame region not the individuals. [SEP] @LozzaCopland \\n\\nDefine \"few\" [SEP] @gibosity I agree there is a huge difference but I think it\\'s due to a lack of understanding/frequent media reports/mass hysteria &amp; fear [SEP] @LozzaCopland you can\\'t terrorize the entire country based on the actions of few .....keep in mind.....#sydneysiege')\n",
            "Processed:   [CLS] How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j [SEP] Jews label anyone they don't like as Anti-Semite and campaign until that person/company is finished. [SEP] No one does. [SEP] #ImCharlieHebdo [SEP] Ditto [SEP] What innocent Muslims ought to find insulting is an atrocity committed in their name, not a sodding cartoon. [SEP] Yes, until it becomes yours. [SEP] Why insult people who have nothing to do with this? People are genuinely offended by such drawings. [SEP] And neither am I! I think this has little to do with actual Muslims. [SEP] Ah, you don't like Jews. Bye bye. [SEP] Also they kid you along with benign stuff then ... WHAM it's like a river of shite! [SEP] It's a good point [SEP] How about this? http://t.co/d2qcaVkf2h [SEP] Organised Jewry, I mean, not the actual people. Otherwise I'd be hating on my own ancestors. [SEP] ...and this: http://t.co/LmYxpmzw3v [SEP] explored. [SEP] And if that's the case, that is your problem. [SEP] No point insulting billions of innocent muslims just to thumb our noses at a bunch of lunatics.They're not worth it [SEP] Oh dear... Just saw those tweets... Blocked him. [SEP] Because they have to learn to not be offended, that's why. [SEP] But by that token Jews, Blacks and Irish people would have to 'learn' not to be offended either [SEP] I get that ... I defend the right to free speech however there is a much broader context to this which is not [SEP] just for the record. I am not in any way, shape or form defending this atrocity. [SEP] Yes, remind me when was the last time Jews bombed the Guardian. [SEP] I know! Gives me the creeps. [SEP] There's a lot of very dodgy Twitter accounts who seem benign then you see the real side :( [SEP] If people insult something that's important to you, you feel that your identity is under attack. [SEP] It's remarkable how quickly they come out the woodwork. [SEP] Why is the correct response to brutality to offend lots of people who *don't* support that brutality? [SEP] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3acv6s95YYr"
      },
      "source": [
        "### 2.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1fRHtdU5dEn"
      },
      "source": [
        "In order to apply the pre-trained BERT, we must use the tokenizer provided by the library. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.\n",
        "\n",
        "In addition, we are required to add special tokens to the start and end of each sentence, pad & truncate all sentences to a single constant length, and explicitly specify what are padding tokens with the \"attention mask\".\n",
        "\n",
        "The `encode_plus` method of BERT tokenizer will:\n",
        "\n",
        "(1) split our text into tokens,\n",
        "\n",
        "(2) add the special `[CLS]` and `[SEP]` tokens, and\n",
        "\n",
        "(3) convert these tokens into indexes of the tokenizer vocabulary,\n",
        "\n",
        "(4) pad or truncate sentences to max length, and\n",
        "\n",
        "(5) create attention mask.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDAfbCle59tP"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "      \n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "#         Set MAX_LEN = 512\n",
        "\n",
        "# Pad input_structure to MAX_LEN\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNE9oASMZ1bN"
      },
      "source": [
        "Before tokenizing, we need to specify the maximum length of our sentences.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrbvKGNAlMtt"
      },
      "source": [
        "# # Concatenate train data and test data\n",
        "# all_tweets = np.concatenate([Corpus.tweet.values, test_data.tweet.values])\n",
        "# print(all_tweets)\n",
        "# print(type(all_tweets))\n",
        "# # Encode our concatenated data\n",
        "# encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
        "# print(encoded_tweets)\n",
        "# # Find the maximum length\n",
        "# max_len = max([len(sent) for sent in encoded_tweets])\n",
        "# print('Max length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpdjBB9fmbu2"
      },
      "source": [
        "Now let's tokenize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTlQzTzAfCy7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e07ffd-cc44-41f9-8b51-23a7721f28c9"
      },
      "source": [
        "# Specify `MAX_LEN`\n",
        "# MAX_LEN = 64\n",
        "MAX_LEN = 512\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "# token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "# print('Original: ', X[0])\n",
        "# print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "# print(X_train, type(X_train), len(X_train))\n",
        "# print(X_val, type(X_val), len(X_val))\n",
        "\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "print(train_inputs, train_masks, val_inputs, val_masks)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing data...\n",
            "tensor([[  101,   101,  2129,  ...,  2123,  1005,   102],\n",
            "        [  101,   101,  2017,  ...,  7395,  2000,   102],\n",
            "        [  101,   101,  4740,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,   101,  9499,  ...,     0,     0,     0],\n",
            "        [  101,   101,  1999,  ...,     0,     0,     0],\n",
            "        [  101,   101, 12583,  ...,     0,     0,     0]]) tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]) tensor([[  101,   101,  1001,  ...,     0,     0,     0],\n",
            "        [  101,   101,  2017,  ...,     0,     0,     0],\n",
            "        [  101,   101,  2031,  ...,  2017,  2123,   102],\n",
            "        ...,\n",
            "        [  101,   101,  2057,  ...,  2288,  2083,   102],\n",
            "        [  101,   101,  2197,  ...,  1012,  1012,   102],\n",
            "        [  101,   101, 13970,  ...,     0,     0,     0]]) tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZU8t5VNfvhY"
      },
      "source": [
        "### 2.2. Create PyTorch DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoHdl3gFgMZY"
      },
      "source": [
        "We will create an iterator for our dataset using the torch DataLoader class. This will help save on memory during training and boost the training speed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHuYEc61gcGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dffee8b-4132-4b81-cc0e-774c4f43fbee"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "\n",
        "Corpus_dev['label'] = le.fit_transform(Corpus_dev['label'])\n",
        "y_val=Corpus_dev.label.values\n",
        "print(type(y_val))\n",
        "print(y_val)\n",
        "print(Corpus_dev.dtypes)\n",
        "print(Corpus_dev.head())\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 10\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "print(train_data,train_sampler,train_dataloader)\n",
        "# Create the DataLoader for our validation set\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1\n",
            " 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1\n",
            " 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1\n",
            " 1 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
            " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1\n",
            " 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
            "id                     object\n",
            "tweet                  object\n",
            "created_at             object\n",
            "label                   int64\n",
            "all_replies            object\n",
            "num_replies             int64\n",
            "all_replies_by_time    object\n",
            "dtype: object\n",
            "                   id                                              tweet                      created_at  label                                        all_replies  num_replies                                all_replies_by_time\n",
            "0  553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      1  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "1  524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014      0  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "2  553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015      0  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "3  580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015      1  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "4  544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014      1  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "<torch.utils.data.dataset.TensorDataset object at 0x7fc26a7fd7d0> <torch.utils.data.sampler.RandomSampler object at 0x7fc26a7fd890> <torch.utils.data.dataloader.DataLoader object at 0x7fc26a7fd110>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmo6sdz0uYgP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSRAga-yj17q"
      },
      "source": [
        "## 3. Train Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoOdsDgG8b_Z"
      },
      "source": [
        "### 3.1. Create BertClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA_yESCl5nuK"
      },
      "source": [
        "BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the `[CLS]` token is used as the features of the sequence to feed a classifier.\n",
        "\n",
        "The `transformers` library has the [`BertForSequenceClassification`](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification) class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n",
        "\n",
        "Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the `[CLS]` token and a single-hidden-layer feed-forward neural network as our classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK41aBFSj5jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a82352-bcbb-4f0f-e758-42959e39a594"
      },
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 38 µs, sys: 0 ns, total: 38 µs\n",
            "Wall time: 55.6 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwNrCgPh-yR7"
      },
      "source": [
        "### 3.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6iOXiN8-8gc"
      },
      "source": [
        "To fine-tune our Bert Classifier, we need to create an optimizer. The authors recommend following hyper-parameters:\n",
        "\n",
        "- Batch size: 16 or 32\n",
        "- Learning rate (Adam): 5e-5, 3e-5 or 2e-5\n",
        "- Number of epochs: 2, 3, 4\n",
        "\n",
        "Huggingface provided the [run_glue.py](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109) script, an examples of implementing the `transformers` library. In the script, the AdamW optimizer is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX7su7Q_269U"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=2):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41DRNjv4B0Ow"
      },
      "source": [
        "### 3.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYU-GQRZG0y8"
      },
      "source": [
        "We will train our Bert Classifier for 4 epochs. In each epoch, we will train our model and evaluate its performance on the validation set. In more details, we will:\n",
        "\n",
        "Training:\n",
        "- Unpack our data from the dataloader and load the data onto the GPU\n",
        "- Zero out gradients calculated in the previous pass\n",
        "- Perform a forward pass to compute logits and loss\n",
        "- Perform a backward pass to compute gradients (`loss.backward()`)\n",
        "- Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "- Update the model's parameters (`optimizer.step()`)\n",
        "- Update the learning rate (`scheduler.step()`)\n",
        "\n",
        "Evaluation:\n",
        "- Unpack our data and load onto the GPU\n",
        "- Forward pass\n",
        "- Compute loss and accuracy rate over the validation set\n",
        "\n",
        "The script below is commented with the details of our training and evaluation loop. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy4HkhyECibW"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT0aR2wSoI-i"
      },
      "source": [
        "  import torch,gc \n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "blpDWittofZ0",
        "outputId": "dc3854b7-a004-4bee-dd80-6c0526a8594f"
      },
      "source": [
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSfTy9LqiFD-"
      },
      "source": [
        "Now, let's start training our BertClassifier!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfYw7dJ0U0v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8345eeae-26b3-4465-aabe-185a07c85de1"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=1)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
            "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "   1    |   20    |   0.672575   |     -      |     -     |   37.10  \n",
            "   1    |   40    |   0.549966   |     -      |     -     |   35.61  \n",
            "   1    |   60    |   0.510329   |     -      |     -     |   35.74  \n",
            "   1    |   80    |   0.463734   |     -      |     -     |   35.94  \n",
            "   1    |   100   |   0.364506   |     -      |     -     |   35.97  \n",
            "   1    |   120   |   0.485028   |     -      |     -     |   35.85  \n",
            "   1    |   140   |   0.516070   |     -      |     -     |   35.93  \n",
            "   1    |   160   |   0.465323   |     -      |     -     |   35.95  \n",
            "   1    |   180   |   0.564190   |     -      |     -     |   35.96  \n",
            "   1    |   200   |   0.455357   |     -      |     -     |   35.88  \n",
            "   1    |   220   |   0.456416   |     -      |     -     |   35.82  \n",
            "   1    |   240   |   0.443678   |     -      |     -     |   35.84  \n",
            "   1    |   260   |   0.434360   |     -      |     -     |   35.94  \n",
            "   1    |   280   |   0.355030   |     -      |     -     |   35.97  \n",
            "   1    |   300   |   0.407414   |     -      |     -     |   35.82  \n",
            "   1    |   320   |   0.388079   |     -      |     -     |   35.86  \n",
            "   1    |   340   |   0.353596   |     -      |     -     |   35.92  \n",
            "   1    |   360   |   0.344413   |     -      |     -     |   35.78  \n",
            "   1    |   380   |   0.366376   |     -      |     -     |   35.85  \n",
            "   1    |   400   |   0.351989   |     -      |     -     |   36.04  \n",
            "   1    |   420   |   0.417313   |     -      |     -     |   35.93  \n",
            "   1    |   440   |   0.330890   |     -      |     -     |   35.91  \n",
            "   1    |   460   |   0.360512   |     -      |     -     |   35.92  \n",
            "   1    |   464   |   0.337485   |     -      |     -     |   5.73   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.436915   |  0.312316  |   88.10   |  869.95  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1FL3ztWeQ4p",
        "outputId": "dae9b5e7-a2da-414d-bd3c-a113ebdeb645"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 13741171366449112329\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 894238720\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 12846664507398174825\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhdYFJmTecW6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5ostg9kPlra"
      },
      "source": [
        "### 3.4. Evaluation on Validation Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIlSTDA7Z9DF"
      },
      "source": [
        "The prediction step is similar to the evaluation step that we did in the training loop, but simpler. We will perform a forward pass to compute logits and apply softmax to calculate probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5_w4erqGzpe"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XekOuD6KbS2Q"
      },
      "source": [
        "The Bert Classifer achieves 0.90 AUC score and 82.65% accuracy rate on the validation set. This result is 10 points better than the baseline method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcmj5s0eRMUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01c9c348-cb07-49d4-c2ed-03068614823a"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "print(probs)\n",
        "# Evaluate the Bert classifier\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.15040998 0.84959   ]\n",
            " [0.982042   0.01795791]\n",
            " [0.98735434 0.01264566]\n",
            " ...\n",
            " [0.91690964 0.08309044]\n",
            " [0.42244667 0.57755333]\n",
            " [0.9751995  0.0248005 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "x71ccKpuCrQY",
        "outputId": "131f5a7c-8aca-4bc0-92e8-9b59f707a193"
      },
      "source": [
        "evaluate_roc(probs, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC: 0.9380\n",
            "Accuracy: 88.10%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS6C5yfSInyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ccaa48-5609-4363-f60f-d356403e72df"
      },
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "# print(probs)\n",
        "print(len(probs))\n",
        "# Get predictions from the probabilities\n",
        "threshold = 0.55\n",
        "preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
        "print(preds)\n",
        "print(len(preds))\n",
        "\n",
        "# Number of tweets predicted non-negative\n",
        "print(\"Number of tweets predicted rumour: \", preds.sum())\n",
        "# label\n",
        "# non-rumour    393\n",
        "# rumour        187"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
            " 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0\n",
            " 0 1 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 1\n",
            " 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1\n",
            " 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0\n",
            " 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0\n",
            " 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0\n",
            " 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
            " 0 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1\n",
            " 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 1\n",
            " 1 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
            "580\n",
            "Number of tweets predicted rumour:  177\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_KpA1mraMk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c16fb39f-0d87-402a-d37b-fad0e6e92028"
      },
      "source": [
        "unique, counts = np.unique(preds, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 403, 1: 177}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om6D0BajcpV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb22181-e2e7-4794-b0a6-673528d91724"
      },
      "source": [
        "Corpus_pred_bert = Corpus_dev\n",
        "print(Corpus_pred_bert.head())\n",
        "# y_predicted = nb_model.predict(X_test_tfidf)\n",
        "y_labels = [\"non-rumour\" if i==0 else \"rumour\" for i in preds]\n",
        "# print(y_predicted,y_predicted_labels)\n",
        "Corpus_pred_bert['label'] = y_labels\n",
        "print(Corpus_pred_bert.head())\n",
        "# Corpus_pred_bert.set_index('id',inplace=True)\n",
        "dictt_bert = Corpus_pred_bert.to_dict()['label']\n",
        "print(dictt_bert)\n",
        "print(len(dictt_bert))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                                tweet                      created_at       label                                        all_replies  num_replies                                all_replies_by_time\n",
            "id                                                                                                                                                                                                                                  \n",
            "553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      rumour  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014  non-rumour  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015  non-rumour  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015  non-rumour  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014  non-rumour  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "                                                                tweet                      created_at       label                                        all_replies  num_replies                                all_replies_by_time\n",
            "id                                                                                                                                                                                                                                  \n",
            "553588913747808256  #BREAKING Reports: 2 brothers suspected of Cha...  Fri Jan 09 16:27:36 +0000 2015      rumour  [@USATODAY :it's unfortunate that they got wha...           10  [@USATODAY :it's unfortunate that they got wha...\n",
            "524949003834634240  You are not alone today #Ottawa - we are here ...  Wed Oct 22 15:42:50 +0000 2014  non-rumour  [@DistressCentreO @CFRASnow interestingly - I'...            1  [@DistressCentreO @CFRASnow interestingly - I'...\n",
            "553221281181859841  Have said it before, but needs saying again: S...  Thu Jan 08 16:06:46 +0000 2015  non-rumour  [@kevinkrease @Max_Fisher That's a ludicrous s...           34  [@kevinkrease @Max_Fisher That's a ludicrous s...\n",
            "580322346508124160  Germanwings #A320 plane crashes in southern Fr...  Tue Mar 24 10:56:43 +0000 2015  non-rumour  [@WSJ A320 totally compromised, “@WSJ: Germanw...           13  [@WSJ A320 totally compromised, “@WSJ: Germanw...\n",
            "544307417677189121  HOSTAGE SITUATION IN SYDNEY\\nTo all our fans a...  Mon Dec 15 01:46:15 +0000 2014  non-rumour  [@Yeow_JX @SGAG_SG People praying is exactly w...            3  [@Yeow_JX @SGAG_SG People praying is exactly w...\n",
            "{'553588913747808256': 'rumour', '524949003834634240': 'non-rumour', '553221281181859841': 'non-rumour', '580322346508124160': 'non-rumour', '544307417677189121': 'non-rumour', '499363921661140993': 'non-rumour', '500177556768034816': 'non-rumour', '553191800408911872': 'non-rumour', '524927281048080385': 'non-rumour', '544308793623207936': 'non-rumour', '553223882128490496': 'non-rumour', '553587546614427649': 'non-rumour', '544308749725626368': 'non-rumour', '553567716414484481': 'non-rumour', '544360851646709760': 'non-rumour', '553160652567498752': 'rumour', '552814627399012352': 'non-rumour', '552827979093385216': 'non-rumour', '524932792925569024': 'rumour', '552978642968780800': 'non-rumour', '552794942759010304': 'rumour', '544304298243653632': 'rumour', '544425492971941888': 'non-rumour', '580885624883904512': 'rumour', '498560433021026304': 'non-rumour', '524994747883323392': 'non-rumour', '544354840042680320': 'non-rumour', '580371776619839488': 'non-rumour', '581248346309922816': 'rumour', '552812682626465792': 'non-rumour', '544476533302505472': 'non-rumour', '580318717860532224': 'rumour', '552993818816299008': 'rumour', '553215362058440704': 'non-rumour', '544285508441231361': 'rumour', '544515546633355264': 'rumour', '553486529231089664': 'rumour', '525041399344168961': 'non-rumour', '544506628239028224': 'non-rumour', '544279603259248640': 'non-rumour', '544293694166536192': 'rumour', '580329875891974145': 'rumour', '581301262269829120': 'rumour', '580377378226049024': 'non-rumour', '544454229960974336': 'rumour', '553098796108750848': 'non-rumour', '544512664769396736': 'rumour', '553537322218553344': 'non-rumour', '552828504060874752': 'non-rumour', '552817345887207425': 'non-rumour', '498286281894682624': 'non-rumour', '500382073593405442': 'non-rumour', '544282393801007104': 'non-rumour', '552825783337897984': 'non-rumour', '500392971636719618': 'non-rumour', '580347464907153408': 'rumour', '544515665319976960': 'non-rumour', '552793409438904321': 'non-rumour', '525020620636307456': 'rumour', '553200085132009475': 'non-rumour', '552840155728736256': 'non-rumour', '500387959468982273': 'non-rumour', '580325714567806976': 'rumour', '498515980205948928': 'non-rumour', '544330174351679489': 'non-rumour', '544511434840113152': 'rumour', '552818368487911425': 'non-rumour', '524944544681705472': 'rumour', '552788947219533824': 'rumour', '553195852643442688': 'non-rumour', '553588080964558849': 'rumour', '552844893148938240': 'non-rumour', '524932574909857792': 'rumour', '553162589744926720': 'non-rumour', '500384777514799104': 'non-rumour', '552795585095671808': 'rumour', '498300832648273920': 'non-rumour', '524934673760530432': 'rumour', '544444021066563584': 'non-rumour', '524990163446140928': 'rumour', '525045203648790528': 'non-rumour', '524953569762766848': 'non-rumour', '524999488067633152': 'rumour', '552827164677373952': 'non-rumour', '544518841020473345': 'rumour', '524937542131793920': 'non-rumour', '500376891623079937': 'non-rumour', '580366761398239232': 'rumour', '553127086706397185': 'non-rumour', '552835794646212608': 'non-rumour', '552837016350912512': 'non-rumour', '500280477920796672': 'rumour', '544363391641341952': 'rumour', '499388398529175552': 'non-rumour', '500294693356175360': 'non-rumour', '544405761925738496': 'non-rumour', '552822450824962048': 'non-rumour', '544346361101963265': 'non-rumour', '524967620588896256': 'rumour', '544315864397258752': 'non-rumour', '524976486902951936': 'rumour', '500160181184249856': 'non-rumour', '500280249629036544': 'rumour', '544279950929321984': 'non-rumour', '524955062486196224': 'rumour', '553480333954678784': 'non-rumour', '553589860892942337': 'rumour', '524925600318754816': 'non-rumour', '553540131756732416': 'non-rumour', '552787544837197824': 'rumour', '544393766287970304': 'non-rumour', '552827553493565441': 'non-rumour', '524969415318982657': 'non-rumour', '580330104846475264': 'non-rumour', '500353227951718401': 'non-rumour', '544373694068686848': 'rumour', '581287562947395584': 'rumour', '544417650680823808': 'non-rumour', '500279951275606017': 'non-rumour', '552833658017824768': 'non-rumour', '500293392060780546': 'rumour', '553161401733496832': 'non-rumour', '525044678765187072': 'non-rumour', '544297696308518912': 'rumour', '553113944219320320': 'non-rumour', '544296373466562560': 'non-rumour', '525007810871164929': 'non-rumour', '553537380209401856': 'rumour', '553166766977740801': 'non-rumour', '499366152049803264': 'non-rumour', '500404132394459136': 'non-rumour', '499610913112989696': 'non-rumour', '552814845016694786': 'non-rumour', '552851710880653312': 'non-rumour', '500176354185187328': 'non-rumour', '544273256174198785': 'non-rumour', '525051365195014144': 'rumour', '552830684021424128': 'rumour', '498272080564334592': 'non-rumour', '553229808260632578': 'rumour', '498286918011224065': 'non-rumour', '544400775775289344': 'non-rumour', '553137476135432194': 'non-rumour', '544351015936331777': 'rumour', '500242061279637504': 'rumour', '499439571520991232': 'non-rumour', '580336772128870400': 'non-rumour', '544411313779126272': 'rumour', '553152675014246400': 'non-rumour', '544308188909027328': 'non-rumour', '552792802309181440': 'rumour', '553480287045554176': 'non-rumour', '580330660537192448': 'non-rumour', '544513035118075904': 'non-rumour', '524981427260366848': 'non-rumour', '544520434767503361': 'non-rumour', '580336782782382081': 'non-rumour', '544443154964152320': 'non-rumour', '499527157023277056': 'non-rumour', '524964564018790400': 'rumour', '552815332931665921': 'non-rumour', '580326012749221890': 'rumour', '552792271243210754': 'non-rumour', '552846353416867841': 'non-rumour', '553591736598269952': 'non-rumour', '552847505763737600': 'non-rumour', '544443959662358528': 'non-rumour', '553590750701551616': 'non-rumour', '553561452318322688': 'non-rumour', '544356724871036928': 'rumour', '499361662743220224': 'non-rumour', '499699270883110913': 'non-rumour', '544496262629036032': 'rumour', '552827825603219456': 'non-rumour', '553503184174710784': 'non-rumour', '524934142958788608': 'non-rumour', '544359578612559872': 'rumour', '544328324601114625': 'non-rumour', '500256984768475136': 'non-rumour', '552811533303296001': 'non-rumour', '499363323335680000': 'non-rumour', '500309193526882306': 'non-rumour', '525017500061536258': 'rumour', '500413956230365185': 'non-rumour', '500291013521334272': 'rumour', '500426667446513666': 'non-rumour', '499699107615997952': 'non-rumour', '580330454869512192': 'non-rumour', '500286170035523585': 'non-rumour', '498289596463980544': 'non-rumour', '544334477011349505': 'non-rumour', '525038517844709376': 'rumour', '553125384246804480': 'non-rumour', '544508995361202176': 'rumour', '500366171716067328': 'non-rumour', '499655249398075392': 'non-rumour', '553144964168036352': 'non-rumour', '553585530504749057': 'rumour', '553570203678683136': 'non-rumour', '500287224957898752': 'rumour', '524930851747164160': 'non-rumour', '552834445301256192': 'rumour', '553582011324329984': 'non-rumour', '580368417556992000': 'rumour', '499553426133753857': 'non-rumour', '552812399997489152': 'non-rumour', '525025463648137216': 'rumour', '553499806883381249': 'non-rumour', '544499381731622912': 'non-rumour', '500303864495882240': 'non-rumour', '499694783691239424': 'non-rumour', '498457205801488385': 'non-rumour', '553231607449276416': 'non-rumour', '580327418084364288': 'non-rumour', '553492417023057920': 'non-rumour', '500306837103968257': 'non-rumour', '553502180666531840': 'non-rumour', '500324836355608576': 'rumour', '500331134040162304': 'non-rumour', '544345939054710785': 'non-rumour', '553168646051475456': 'non-rumour', '553199664720543744': 'non-rumour', '553590811355795456': 'non-rumour', '499524024012386304': 'non-rumour', '553117623568068608': 'non-rumour', '553196263064498176': 'non-rumour', '499571347375136768': 'non-rumour', '499478732315566080': 'non-rumour', '553112060788473856': 'non-rumour', '580320267777511425': 'rumour', '544514538797035520': 'rumour', '580363156721131521': 'rumour', '524966946467749888': 'non-rumour', '552792913910833152': 'rumour', '581325197661376512': 'non-rumour', '500402804910206976': 'non-rumour', '499696948296028160': 'non-rumour', '498283586425200640': 'non-rumour', '524953508978909185': 'non-rumour', '552813223360352256': 'non-rumour', '552848620375261184': 'rumour', '524959630158725120': 'non-rumour', '525024181218725888': 'rumour', '553541676229472256': 'rumour', '553135991083769856': 'non-rumour', '525030331775463424': 'non-rumour', '544461661055098881': 'non-rumour', '553480257747963905': 'non-rumour', '524998110268096513': 'non-rumour', '544462911729455104': 'non-rumour', '524926643325132800': 'rumour', '499567797064957952': 'non-rumour', '553566829273423872': 'non-rumour', '524957872296583168': 'non-rumour', '581303989406314496': 'rumour', '553587511839444994': 'rumour', '525027116287811584': 'rumour', '553527807771308034': 'non-rumour', '499439550029381632': 'non-rumour', '553483707122995201': 'non-rumour', '553135106609922049': 'non-rumour', '553137557601812480': 'non-rumour', '552792518870708224': 'non-rumour', '499657337985323009': 'non-rumour', '544317474498703360': 'non-rumour', '552822604206460928': 'non-rumour', '499576743967993856': 'non-rumour', '552851838928953344': 'non-rumour', '544519245087121408': 'rumour', '552827123963281408': 'non-rumour', '525027539845410816': 'rumour', '500296090671796224': 'non-rumour', '544270745744846848': 'rumour', '581256908251758592': 'rumour', '500285938824908801': 'rumour', '499377813758443520': 'non-rumour', '544457324211879937': 'non-rumour', '553236009081921536': 'non-rumour', '553470977972375553': 'rumour', '552805638024077312': 'non-rumour', '524958743000125440': 'rumour', '499588291335692288': 'non-rumour', '524922078638903296': 'rumour', '524943658748219392': 'rumour', '552808669646036992': 'non-rumour', '580346835295997952': 'non-rumour', '552837268160135169': 'non-rumour', '500290098647547904': 'non-rumour', '544494408515608576': 'non-rumour', '524956383318650880': 'rumour', '552837599304622080': 'non-rumour', '553145759961714688': 'non-rumour', '553115926531674112': 'non-rumour', '552789521646247936': 'non-rumour', '500288971197263873': 'rumour', '544301624814870530': 'rumour', '500382265076379649': 'non-rumour', '525005886272843776': 'rumour', '544372465078505472': 'rumour', '580339510883561472': 'non-rumour', '524984505376182272': 'non-rumour', '544472595140063232': 'non-rumour', '500308546815131648': 'non-rumour', '498280074941313024': 'non-rumour', '498269289812856832': 'non-rumour', '544287991959539713': 'non-rumour', '553175714128990208': 'rumour', '544377463581581314': 'rumour', '553542245966958592': 'non-rumour', '524936931059056640': 'non-rumour', '552979275776032768': 'non-rumour', '498254929942028288': 'rumour', '524926528342487041': 'non-rumour', '544295960449265664': 'non-rumour', '553101858013581313': 'rumour', '498316826988642304': 'non-rumour', '524943504192311296': 'non-rumour', '544458558432350208': 'non-rumour', '524949526487265280': 'rumour', '552821730100924417': 'non-rumour', '544508543152304128': 'rumour', '500362083221786625': 'rumour', '553124985900785666': 'non-rumour', '552812416485294080': 'non-rumour', '544351967829839873': 'non-rumour', '525055967076290560': 'non-rumour', '500287479015292928': 'non-rumour', '553572740125712385': 'rumour', '544505129810944001': 'non-rumour', '524972443308683264': 'rumour', '499689438092726273': 'non-rumour', '580327611760545792': 'non-rumour', '524974135030874114': 'non-rumour', '544337816298070017': 'non-rumour', '544320992361721856': 'rumour', '553544694765215745': 'rumour', '553576849943826432': 'rumour', '524968337747767298': 'rumour', '553583847850663936': 'non-rumour', '544325779199258626': 'rumour', '553501469388058626': 'non-rumour', '552846287314640897': 'non-rumour', '500307863894786048': 'non-rumour', '553584683892871168': 'non-rumour', '552839190921351168': 'non-rumour', '552831107985858560': 'non-rumour', '580331561398108160': 'rumour', '500280422295937024': 'rumour', '500274132756738048': 'non-rumour', '500197999205965824': 'non-rumour', '499411596255719424': 'non-rumour', '552802128688078848': 'rumour', '544512465099587584': 'non-rumour', '498280126254428160': 'non-rumour', '544516240572940289': 'rumour', '544475892685099008': 'non-rumour', '552841157785694208': 'non-rumour', '552839167575883776': 'non-rumour', '544515446100090881': 'non-rumour', '553215305917665280': 'non-rumour', '553177465883873280': 'non-rumour', '544294794139533313': 'rumour', '499665889198088192': 'non-rumour', '544286724831318017': 'non-rumour', '544280077778055168': 'rumour', '498482619232055296': 'non-rumour', '580323152103911424': 'non-rumour', '524957105296404480': 'non-rumour', '580320563891019776': 'rumour', '552836167029514240': 'non-rumour', '553234578086055937': 'non-rumour', '524936365427806208': 'rumour', '552845840247971842': 'non-rumour', '553186515359379457': 'non-rumour', '500376174095511552': 'non-rumour', '524948520605069312': 'non-rumour', '544449632131375104': 'non-rumour', '553162459885096962': 'non-rumour', '580327626419482624': 'rumour', '525025552634494976': 'rumour', '553136565296582657': 'rumour', '553566279953842176': 'rumour', '544393889113600000': 'non-rumour', '552845255796461569': 'non-rumour', '499402643869929474': 'non-rumour', '553186739326812161': 'non-rumour', '544513339544838147': 'rumour', '524979925343014912': 'non-rumour', '500281094239817728': 'rumour', '580322005280493568': 'rumour', '499635247311781889': 'non-rumour', '544277113683980288': 'rumour', '500295393301647360': 'non-rumour', '544334623023452160': 'non-rumour', '544381029537312769': 'non-rumour', '524947071770435585': 'non-rumour', '553587672137334785': 'rumour', '499602370917978112': 'non-rumour', '524979279701241856': 'non-rumour', '524988712367955970': 'rumour', '553107939926736896': 'rumour', '525044725926330368': 'rumour', '580324027715063808': 'rumour', '553480082996879360': 'rumour', '544468178961592321': 'non-rumour', '552800222553079809': 'non-rumour', '524959715957424128': 'non-rumour', '544516164333080577': 'rumour', '500283877567770624': 'rumour', '500276114011742208': 'non-rumour', '499431474731683841': 'rumour', '552838479844216833': 'non-rumour', '499412007573942272': 'non-rumour', '580335510280892416': 'non-rumour', '553151804046082048': 'non-rumour', '499607500187250689': 'non-rumour', '525028734991343617': 'rumour', '525019992337551360': 'non-rumour', '553533193899368448': 'non-rumour', '499511714438971392': 'non-rumour', '500293511632388097': 'non-rumour', '544436279442546688': 'non-rumour', '553196035133440000': 'non-rumour', '500361647144574976': 'non-rumour', '544309096351268864': 'non-rumour', '498248415223246848': 'non-rumour', '552845017208061953': 'non-rumour', '524981945131102211': 'non-rumour', '525034687245582337': 'non-rumour', '499701423135678464': 'non-rumour', '580370275121938432': 'rumour', '499673760484433921': 'non-rumour', '499690032601772032': 'non-rumour', '552985297307598848': 'non-rumour', '544287796719288321': 'rumour', '524925223226081282': 'rumour', '552797114863198208': 'non-rumour', '553170912129056770': 'non-rumour', '552798640679702528': 'non-rumour', '524936008870400000': 'non-rumour', '552846938677059586': 'non-rumour', '500263561298710528': 'non-rumour', '552846893588709376': 'non-rumour', '553481963864993792': 'non-rumour', '524943226948820992': 'non-rumour', '544320939245043712': 'rumour', '544476492281827328': 'non-rumour', '525072912957452289': 'rumour', '500359377585704961': 'rumour', '553219770053443585': 'non-rumour', '552821069036670976': 'non-rumour', '553198853487611907': 'non-rumour', '544484918567239681': 'non-rumour', '524968466559033345': 'non-rumour', '544517991086710784': 'rumour', '552833393734721537': 'non-rumour', '580320856561209344': 'rumour', '499345023721537536': 'non-rumour', '544509140911919104': 'rumour', '553183420499894272': 'non-rumour', '553588946039746560': 'rumour', '498278882244853760': 'non-rumour', '498292200955195392': 'non-rumour', '552813549291315200': 'non-rumour', '544319832486064128': 'rumour', '580330161725255681': 'rumour', '498292628455034882': 'non-rumour', '499361679197884416': 'non-rumour', '552823198484819969': 'non-rumour', '552820025409941506': 'non-rumour', '581325025569030145': 'non-rumour', '552827117663436801': 'non-rumour', '553104539898089472': 'non-rumour', '552818685526933505': 'non-rumour', '544292670336925696': 'rumour', '544373593841623040': 'rumour', '499706065769201666': 'non-rumour', '544307161589772290': 'rumour', '553578250186665984': 'non-rumour', '552834808121163776': 'non-rumour', '544291829290508289': 'rumour', '524943804051496961': 'non-rumour', '553159290987364352': 'non-rumour', '499345604297121792': 'non-rumour', '553186026811039745': 'non-rumour', '524950428598153216': 'rumour', '552834558706847745': 'non-rumour', '524945879455653889': 'rumour', '500429525558505473': 'non-rumour', '525023057526947840': 'rumour', '553493300242432000': 'non-rumour', '544288968624578560': 'non-rumour', '544336585052160001': 'non-rumour', '580328617713897472': 'rumour', '524923148576518144': 'non-rumour', '552810249263259649': 'non-rumour', '552819064301961216': 'non-rumour', '552851421155299328': 'non-rumour', '553128548526227456': 'non-rumour', '499602989133594624': 'non-rumour', '552832738542514176': 'non-rumour', '544511558173601792': 'rumour', '544401029211496449': 'non-rumour', '553543395017510912': 'non-rumour', '499412821671968769': 'rumour', '544326782334558208': 'non-rumour', '552817785949396992': 'non-rumour', '524979756371288064': 'non-rumour', '544413195884974080': 'non-rumour', '498308585227431936': 'non-rumour', '553219651594125312': 'non-rumour', '552789127792717826': 'rumour', '524953604717686784': 'non-rumour', '553588053348843520': 'non-rumour', '500362679039840257': 'rumour', '580327625626927104': 'rumour', '544445364322197504': 'non-rumour', '544280923923959808': 'rumour', '500364290545958912': 'rumour', '524928960615186432': 'non-rumour', '552810166744547328': 'non-rumour', '500381163866062848': 'rumour', '552807054268956673': 'non-rumour', '580329937573302272': 'non-rumour', '553225809444749313': 'non-rumour', '552844023996907520': 'non-rumour', '524940716733370374': 'rumour', '498313143530303488': 'rumour', '552824742097719296': 'non-rumour', '544500527720325120': 'rumour', '553571432475951105': 'rumour', '544497405237161984': 'rumour', '553477943616634880': 'rumour', '524929769469538305': 'rumour', '544502782267752449': 'non-rumour', '524997921042071553': 'non-rumour', '544294618758520832': 'non-rumour', '580337831253581824': 'non-rumour', '553220369721483266': 'non-rumour', '552828573896413185': 'non-rumour', '552840805678084096': 'non-rumour', '552982443511144448': 'rumour', '500324234821124096': 'rumour', '580352273001410560': 'rumour', '552786299875520512': 'non-rumour', '553536358950916096': 'rumour', '524951439417044992': 'non-rumour', '500318980007948288': 'non-rumour', '544337689676636161': 'non-rumour', '544312732443348995': 'non-rumour', '552815993165467649': 'non-rumour', '552823186966855680': 'non-rumour', '552805267990413312': 'non-rumour', '499386181310619649': 'non-rumour', '552850611524599808': 'non-rumour', '498345343629541376': 'non-rumour', '553143017922912256': 'non-rumour', '544366036498542592': 'rumour', '552818868604129280': 'non-rumour', '544511865729323009': 'rumour', '544504889746149377': 'rumour', '552822434639126528': 'rumour', '552828811344359425': 'non-rumour', '553219195056693249': 'non-rumour', '553584868685520897': 'non-rumour', '499526002469797888': 'non-rumour', '553143987377561600': 'non-rumour', '552808776492130304': 'non-rumour', '544298346345947136': 'rumour', '499402006146977792': 'non-rumour', '544458445764968448': 'non-rumour', '580338194731962368': 'non-rumour', '552825844755083265': 'non-rumour', '552807904597008385': 'non-rumour', '552804592988479488': 'non-rumour', '525025279803424768': 'rumour', '552784600502915072': 'non-rumour', '499696525808001024': 'non-rumour', '580320612155060224': 'rumour', '553218279557582849': 'non-rumour'}\n",
            "580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUkUJ05wdhNz"
      },
      "source": [
        "with open(\"dev.baseline_bert6.json\", \"w\") as outfile: \n",
        "    json.dump(dictt_bert, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3Dwrq86J2X_"
      },
      "source": [
        "print(val_data)\n",
        "print(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y58Z01saM6hC"
      },
      "source": [
        "print(dfs_dev)\n",
        "print(Corpus_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwuneIcI-RH"
      },
      "source": [
        "output = Corpus_dev[preds==1]\n",
        "print(len(output)) #117 tweets detected as rumour in val dataset(source tweets only)\n",
        "# print(output)\n",
        "\n",
        "list(output.sample(20).tweet)\n",
        "lst = list(output.tweet)\n",
        "print(len(lst))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5DW6grRmfT-"
      },
      "source": [
        "### 3.5. Train Our Model on the Entire Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkMK5VqJJvSO"
      },
      "source": [
        "# Concatenate the train set and the validation set\n",
        "full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
        "full_train_sampler = RandomSampler(full_train_data)\n",
        "full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
        "\n",
        "# Train the Bert Classifier on the entire training data\n",
        "set_seed(42)\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, full_train_dataloader, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q89oT0n3N0m6"
      },
      "source": [
        "## 4. Predictions on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqk_CPwjN_W0"
      },
      "source": [
        "### 4.1. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U3K1LbDZTOU"
      },
      "source": [
        "Let's revisit out test set shortly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzCpJBgWZYR_"
      },
      "source": [
        "Before making predictions on the test set, we need to redo processing and encoding steps done on the training data. Fortunately, we have written the `preprocessing_for_bert` function to do that for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYv9lSXsQCZ2"
      },
      "source": [
        "### 4.2. Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsSlCGCAajmD"
      },
      "source": [
        "There are about 300 non-negative tweets in our test set. Therefore, we will keep adjusting the decision threshold until we have about 300 non-negative tweets.\n",
        "\n",
        "The threshold we will use is 0.992, meaning that tweets with a predicted probability greater than 99.2% will be predicted positive. This value is very high compared to the default 0.5 threshold.\n",
        "\n",
        "After manually examining the test set, I find that the sentiment classification task here is even difficult for human. Therefore, a high threshold will give us safe predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GMqDdsScTQb"
      },
      "source": [
        "Now we will examine 20 random tweets from our predictions. 17 of them are correct, showing that the BERT Classifier acquires about 0.85 precision rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_OcVenocEH_"
      },
      "source": [
        "# E - Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMg9ZUvocF6U"
      },
      "source": [
        "By adding a simple one-hidden-layer neural network classifier on top of BERT and fine-tuning BERT, we can achieve near state-of-the-art performance, which is 10 points better than the baseline method although we only have 3,400 data points.\n",
        "\n",
        "In addition, although BERT is very large, complicated, and have millions of parameters, we only need to fine-tune it in only 2-4 epochs. That result can be achieved because BERT was trained on the huge amount and already encode a lot of information about our language. An impresive performance achieved in a short amount of time, with a small amount of data has shown why BERT is one of the most powerful NLP models available at the moment. "
      ]
    }
  ]
}